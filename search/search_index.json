{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Monitorias Este website foi desenvolvido para concentrar as informa\u00e7\u00f5es sobre as monitorias que dei ao longo da minha vida acad\u00eamica. Em particular durante a Gradua\u00e7\u00e3o na Escola de Matem\u00e1tica Aplicada, Funda\u00e7\u00e3o Getulio Vargas (EMAp/FGV). Eu espero que a informa\u00e7\u00e3o aqui contida seja de interesse! T\u00f3picos abordados \u00c1lgebra Linear (2019.2) Professor Eduardo Wagner Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias (2020.1) Professora Maria Izabel Camacho Infer\u00eancia \u00e0 Estat\u00edstica (2020.2) Professor Luiz Max de Carvalho Refer\u00eancias interessantes de matem\u00e1tica SageMath : software matem\u00e1tico que facilita c\u00e1lculos. \u00c9 uma possibilidade alternativa ao WolframAlpha , por\u00e9m com c\u00f3digo em Python. Manin : ferramenta para criar v\u00eddeos matem\u00e1ticos explicativos com anima\u00e7\u00e3o program\u00e1tica, uma biblioteca para Python.","title":"Home"},{"location":"#monitorias","text":"Este website foi desenvolvido para concentrar as informa\u00e7\u00f5es sobre as monitorias que dei ao longo da minha vida acad\u00eamica. Em particular durante a Gradua\u00e7\u00e3o na Escola de Matem\u00e1tica Aplicada, Funda\u00e7\u00e3o Getulio Vargas (EMAp/FGV). Eu espero que a informa\u00e7\u00e3o aqui contida seja de interesse!","title":"Monitorias"},{"location":"#topicos-abordados","text":"\u00c1lgebra Linear (2019.2) Professor Eduardo Wagner Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias (2020.1) Professora Maria Izabel Camacho Infer\u00eancia \u00e0 Estat\u00edstica (2020.2) Professor Luiz Max de Carvalho","title":"T\u00f3picos abordados"},{"location":"#referencias-interessantes-de-matematica","text":"SageMath : software matem\u00e1tico que facilita c\u00e1lculos. \u00c9 uma possibilidade alternativa ao WolframAlpha , por\u00e9m com c\u00f3digo em Python. Manin : ferramenta para criar v\u00eddeos matem\u00e1ticos explicativos com anima\u00e7\u00e3o program\u00e1tica, uma biblioteca para Python.","title":"Refer\u00eancias interessantes de matem\u00e1tica"},{"location":"alglin/info/","text":"Informa\u00e7\u00f5es Gerais Este t\u00f3pico ainda est\u00e1 em desenvolvimento. Monitoriais","title":"\u00c1lgebra Linear"},{"location":"alglin/info/#informacoes-gerais","text":"Este t\u00f3pico ainda est\u00e1 em desenvolvimento. Monitoriais","title":"Informa\u00e7\u00f5es Gerais"},{"location":"edo/info/","text":"Informa\u00e7\u00f5es Gerais Este t\u00f3pico ainda est\u00e1 em desenvolvimento. Monitoria","title":"Equa\u00e7\u00f5es Diferenciais"},{"location":"edo/info/#informacoes-gerais","text":"Este t\u00f3pico ainda est\u00e1 em desenvolvimento. Monitoria","title":"Informa\u00e7\u00f5es Gerais"},{"location":"infestatistica/SufficientStatistics/","text":"Estat\u00edsticas Suficientes A ideia por tra\u015b da estat\u00edstica \u00e9, como o nome diz, ser suficiente. Uma estat\u00edstica \u00e9 uma fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias, como, por exemplo, T = r(X_1, ..., X_n) . M\u00e9dia amostral, vari\u00e2ncia amostral, valor m\u00e1ximo, s\u00e3o todos exemplos. Imagine que temos um problema como o seguinte: Vamos imaginar que um estat\u00edstico d\u00e1 um trabalho para seu estagi\u00e1rio para organizar os dados de forma mais eficiente poss\u00edvel, enquanto ele pensa no modelo. O estagi\u00e1rio de forma muito ing\u00eanua cria uma lista em seu Jupyter Notebook e salva o notebook com os dados na sua lista. Depois ele salva num arquivo .txt e vai para casa tranquilo que o trabalho acabou mais cedo. Ser\u00e1 que era necess\u00e1rio ter salvo todos os dados? O estat\u00edstico no dia seuinte diz que n\u00e3o! E manda o estagi\u00e1rio estudar novamente estat\u00edstica. Ele disse para estudar Estat\u00edstica Suficientes . Defini\u00e7\u00e3o Unidimensional Seja X_1, ..., X_n uma amostra aleat\u00f3ria de distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta . Suponha que para todo valor que \\theta assume e para todo valor que T assume (vamos chamar de t = r(x_1, ..., x_n) , nesse caso j\u00e1 observamos o processo e calculamos t ), a distribui\u00e7\u00e3o conjunta condicional de X_1, ...., X_n dado T=t e \\theta , isto \u00e9, dado que voc\u00ea observou uma estat\u00edstica (a m\u00e9dia de temperaturas, por exemplo) depende apenas de t , mas n\u00e3o de \\theta . Isso significa que a distribui\u00e7\u00e3o \u00e9 constante para todos os valores de \\theta . Chamaremos essa estat\u00edstica T de suficiente para \\theta . Obs.: Para quem estudou fun\u00e7\u00f5es mensur\u00e1veis, podemos definir estat\u00edstica como fun\u00e7\u00e3o mensur\u00e1vel dos dados. Seja (\\mathbb{T}, \\mathbb{C}) um espa\u00e7o mensur\u00e1vel tal que \\mathbb{C} cont\u00e9m todos os conjuntos unit\u00e1rios. Se T : \\mathbb{X} \\to \\mathbb{T} \u00e9 mensu\u00e1vel, ent\u00e3o \u00e9 uma estat\u00edstica. Seja \\mathbb{P}_0 uma familia param\u00e9trica de distribui\u00e7\u00f5es em (\\mathbb{X}, \\mathbb{B}) . Seja (\\Omega, \\tau) um espa\u00e7o dos par\u00e2metros e \\Theta: \\mathbb{P}_0 \\to \\Omega um par\u00e2metro. Seja T uma estat\u00edstica. Ela \u00e9 suficiente para \\Theta se para toda priori \\mu_{\\Theta} , existem vers\u00f5es da posteriori \\mu_{\\Theta|X} e \\mu_{\\Theta|T} tal que \\forall B \\in \\tau, \\mu_{\\Theta|X}(B|x) = \\mu_{\\Theta|T}(B|T(x)) , quase certamente convergente para [\\mu_X] onde \\mu_X \u00e9 distribui\u00e7\u00e3o marginal de X . Crit\u00e9rio de Fatoriza\u00e7\u00e3o Teorema atribu\u00eddo a Neyman-Fisher. X_1,...X_n amostra aleat\u00f3ria com pdf ou pmf f(x|\\theta) , onde \\theta \u00e9 desconhecido. Uma estat\u00edstica T = r(X) para \\theta \u00e9 suficiente se, e somente se, a distribui\u00e7\u00e3o conjunta f_n(x|\\theta) pode ser fatorada para todo valor x \\in \\mathbb{R}^n da seguinte forma: f_n(x|\\theta) = u(x)v[r(x), \\theta] Onde u e v s\u00e3o n\u00e3o negativas, u n\u00e3o depende de \\theta e v s\u00f3 depende dos dados atrav\u00e9s da estat\u00edstica. Isto \u00e9, n\u00e3o adianta voc\u00ea encontrar qualquer fun\u00e7\u00e3o de x , tem que encontrar a estat\u00edstica T em v . Estat\u00edsticas Conjuntas Suficientes Suponha que para cada \\theta , vetor, e cada valor das estat\u00edsticas (T_1, ..., T_k) = (t_1, ..., t_k) a distribui\u00e7\u00e3o conjunta condicional dos dados dadas as estat\u00edsticas n\u00e3o depende de \\theta . Veja que nesse caso, a diferen\u00e7a \u00e9 que condiciono em k estat\u00edsticas, k \\geq 1 . Crit\u00e9rio de Fatoriza\u00e7\u00e3o Sejam r_1, ..., r_k fun\u00e7\u00f5es de n vari\u00e1veis. A estat\u00edsticas T_i = r_i(X) s\u00e3o estat\u00edsticas suficientes conjuntas para \\theta se, e somene se, a pdf conjunta f_n(x|\\theta) pode ser fatorado como f_n(x|\\theta) = u(x)v[r_1(x), ..., r_k(x),\\theta], para todos os valores x \\in \\mathbb{R}^n e \\theta \\in \\Omega Obs.: Podemos mostrar que qualquer fun\u00e7\u00e3o injetiva de uma estat\u00edstica suficiente \u00e9 uma estat\u00edstica suficiente. Estat\u00edstica Suficiente M\u00ednima Estat\u00edstica de Ordem Considere uma amostra aleat\u00f3ria e a ordene. Diremos que a nova amostra, ordenada, \u00e9 uma estat\u00edstica de ordem. Observe que ela funciona como uma matrix de \"shifts\" que opera trocando as linhas do vetor de lugar. Por isso ela \u00e9 uma fun\u00e7\u00e3o. Essa estat\u00edstica \u00e9 sufciente conjunta para \\theta . O interessante que podemos ver isso dado que o produt\u00f3rio n\u00e3o importa a ordem. Estat\u00edstica Suficiente M\u00ednima \u00c9 uma estat\u00edstica T suficiente e, al\u00e9m disso, \u00e9 fun\u00e7\u00e3o de todas as outras estat\u00edsticas suficientes. MLE e Estat\u00edstica Suficiente Seja T uma estat\u00edstica suficiente para \\theta . Ent\u00e3o o estimador de m\u00e1xima verossimilhan\u00e7a \\hat{\\theta} depende das observa\u00e7\u00f5es somente atrav\u00e9s da estat\u00edstica T . Al\u00e9m disso, se \\hat{\\theta} \u00e9 suficiente, ent\u00e3o \u00e9 m\u00ednimo. Estat\u00edsticas Suficientes e Estimador de Bayes T = r(X) estat\u00edstica suficiente para \\theta . Ent\u00e3o todo estimador de Bayes \\hat{\\theta} depende nas observa\u00e7\u00f5es X_1, ..., X_n apenas atrav\u00e9s da estat\u00edstica T . Al\u00e9m do mais, se for suficiente, ser\u00e1 suficiente m\u00ednimo. Defini\u00e7\u00f5es Adicionais Considere uma amostra aleat\u00f3ria X_1,...,X_n Estat\u00edstica Completa Seja t = T(X) estat\u00edstica. Se E[g(T(X))|\\theta] = 0, \\forall \\theta \\implies P[g(T(X)) = 0] = 1, ent\u00e3o ela \u00e9 dita completa. Estat\u00edstica Ancillary Suponha que queremos estimar \\theta e f_n(x|\\theta) seja a pdf conjunta. Seja A(X) uma estat\u00edstica. Se a sua distribui\u00e7\u00e3o n\u00e3o depende de \\theta , ent\u00e3o ser\u00e1 uma estat\u00edstica ancillary (auxiliar?) Por exemplo, se X_1, X_2 \\sim N(\\mu, \\sigma^2) e \\mu \u00e9 desconhecido, temos que X_1 - X_2 \\sim N(0, 2\\sigma^2) \u00e9 uma estat\u00edstica auxiliar. Melhorando um Estimador Suponha que temos uma amostra aleat\u00f3ria X = (X_1, ..., X_n) cuja pdf \u00e9 f(x|\\theta) e \\theta \\in \\Omega desconhecido, tal que queremos estimar h(\\theta) para alguma fun\u00e7\u00e3o h . Seja Z = g(X_1, ..., X_n) . E_{\\theta}(Z) = \\int_{-\\infty}^{\\infty}...\\int_{-\\infty}^{\\infty} g(x)f_n(x|\\theta)dx_1, ..., dx_n Para cada estimado \\delta(X) e para todo valor de \\theta , definimos o MSE (Erro M\u00e9dio Quadr\u00e1tico) R(\\theta, \\delta) = E_{\\theta}\\{[\\delta(X) - h(\\theta)]^2\\} Quando n\u00e3o atribu\u00edmos uma priori para \\theta , ent\u00e3o queremos encontrar um estimador para que o MSE seja pequeno para v\u00e1rios valores de \\theta . Seja T uma estat\u00edstica suficiente conhecida. Definimos \\delta_0(T) = E_{\\theta}\\{\\delta(X)|T\\} \\overset{1}{=} E\\{\\delta(X)|T\\} (1) Agora, por que podemos chamar \\delta_0 de estimador se depende de \\theta ? Como T \u00e9 uma estat\u00edstica suficiente, a distribui\u00e7\u00e3o condicionada em T e em \\theta da amostra X_1, ..., X_n n\u00e3o depende de \\theta !!! Em particular o valor esperado do estimador \\delta(T) . Logo, como esse valor esperado n\u00e3o depende de \\theta , podemos dizer sim que ele \u00e9 um estimador. Teorema Rao - Blackwell Teorema 7.9.1 do livro. Seja \\delta(X) um estimador e T uma estat\u00edstica suficiente para \\theta . O estimador \\delta_0(T) definido acima, para todo valor \\theta \\in \\Omega \u00e9: R(\\theta, \\delta_0) \\leq R(\\theta, \\delta), isto \u00e9, \u00e9 um estimador com menor erro quadr\u00e1tico m\u00e9dio (MSE). Em particular se R(\\theta, \\delta) < \\infty , a desigualdade se torna estrita, a menos que \\delta(X) seja um afun\u00e7\u00e3o de T , isto \u00e9, se \\delta(X) n\u00e3o for fun\u00e7\u00e3o de T , ent\u00e3o a desigualdade ser\u00e1 estrita. Por desigualdade estrita entenda < . Obs.: Chamamos o processo de melhorar um estimador com esse teorema de \"Rao-Blackwelliation\". Obs.2: Podemos generalizar um pouco mais. Para isso, pesquise sobre Conjuntos Convexos e sobre Fun\u00e7\u00f5es Convexas . Em um conjunto convexo, se a nossa fun\u00e7\u00e3o de perda n\u00e3o for o MSE, mas for uma fun\u00e7\u00e3o convexa, o teorema tamb\u00e9m valer\u00e1. Uma suposi\u00e7\u00e3o interessante que o Livro n\u00e3o imp\u00f5e \u00e9 que E[||\\delta(X)||) < \\infty . Inadmissibilidade Suponha que R(\\theta, \\delta) \u00e9 MSE. O estimador \\delta \u00e9 inadimiss\u00edvel se existe outro estimador \\delta_0 tal que R(\\theta, \\delta_0) \\leq R(\\theta, \\delta) para todo valor de \\theta e existe a desigualdade estrita em, pelo menos um valor de \\theta . Dizemos nesse caso que \\delta_0 domina o estimador \\delta . Um estimador \\delta_0 \u00e9 admiss\u00edvel se n\u00e3o existe outro estimador que o domine.","title":"Estat\u00edsticas Suficientes"},{"location":"infestatistica/SufficientStatistics/#estatisticas-suficientes","text":"A ideia por tra\u015b da estat\u00edstica \u00e9, como o nome diz, ser suficiente. Uma estat\u00edstica \u00e9 uma fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias, como, por exemplo, T = r(X_1, ..., X_n) . M\u00e9dia amostral, vari\u00e2ncia amostral, valor m\u00e1ximo, s\u00e3o todos exemplos. Imagine que temos um problema como o seguinte: Vamos imaginar que um estat\u00edstico d\u00e1 um trabalho para seu estagi\u00e1rio para organizar os dados de forma mais eficiente poss\u00edvel, enquanto ele pensa no modelo. O estagi\u00e1rio de forma muito ing\u00eanua cria uma lista em seu Jupyter Notebook e salva o notebook com os dados na sua lista. Depois ele salva num arquivo .txt e vai para casa tranquilo que o trabalho acabou mais cedo. Ser\u00e1 que era necess\u00e1rio ter salvo todos os dados? O estat\u00edstico no dia seuinte diz que n\u00e3o! E manda o estagi\u00e1rio estudar novamente estat\u00edstica. Ele disse para estudar Estat\u00edstica Suficientes .","title":"Estat\u00edsticas Suficientes"},{"location":"infestatistica/SufficientStatistics/#definicao-unidimensional","text":"Seja X_1, ..., X_n uma amostra aleat\u00f3ria de distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta . Suponha que para todo valor que \\theta assume e para todo valor que T assume (vamos chamar de t = r(x_1, ..., x_n) , nesse caso j\u00e1 observamos o processo e calculamos t ), a distribui\u00e7\u00e3o conjunta condicional de X_1, ...., X_n dado T=t e \\theta , isto \u00e9, dado que voc\u00ea observou uma estat\u00edstica (a m\u00e9dia de temperaturas, por exemplo) depende apenas de t , mas n\u00e3o de \\theta . Isso significa que a distribui\u00e7\u00e3o \u00e9 constante para todos os valores de \\theta . Chamaremos essa estat\u00edstica T de suficiente para \\theta . Obs.: Para quem estudou fun\u00e7\u00f5es mensur\u00e1veis, podemos definir estat\u00edstica como fun\u00e7\u00e3o mensur\u00e1vel dos dados. Seja (\\mathbb{T}, \\mathbb{C}) um espa\u00e7o mensur\u00e1vel tal que \\mathbb{C} cont\u00e9m todos os conjuntos unit\u00e1rios. Se T : \\mathbb{X} \\to \\mathbb{T} \u00e9 mensu\u00e1vel, ent\u00e3o \u00e9 uma estat\u00edstica. Seja \\mathbb{P}_0 uma familia param\u00e9trica de distribui\u00e7\u00f5es em (\\mathbb{X}, \\mathbb{B}) . Seja (\\Omega, \\tau) um espa\u00e7o dos par\u00e2metros e \\Theta: \\mathbb{P}_0 \\to \\Omega um par\u00e2metro. Seja T uma estat\u00edstica. Ela \u00e9 suficiente para \\Theta se para toda priori \\mu_{\\Theta} , existem vers\u00f5es da posteriori \\mu_{\\Theta|X} e \\mu_{\\Theta|T} tal que \\forall B \\in \\tau, \\mu_{\\Theta|X}(B|x) = \\mu_{\\Theta|T}(B|T(x)) , quase certamente convergente para [\\mu_X] onde \\mu_X \u00e9 distribui\u00e7\u00e3o marginal de X .","title":"Defini\u00e7\u00e3o Unidimensional"},{"location":"infestatistica/SufficientStatistics/#criterio-de-fatorizacao","text":"Teorema atribu\u00eddo a Neyman-Fisher. X_1,...X_n amostra aleat\u00f3ria com pdf ou pmf f(x|\\theta) , onde \\theta \u00e9 desconhecido. Uma estat\u00edstica T = r(X) para \\theta \u00e9 suficiente se, e somente se, a distribui\u00e7\u00e3o conjunta f_n(x|\\theta) pode ser fatorada para todo valor x \\in \\mathbb{R}^n da seguinte forma: f_n(x|\\theta) = u(x)v[r(x), \\theta] Onde u e v s\u00e3o n\u00e3o negativas, u n\u00e3o depende de \\theta e v s\u00f3 depende dos dados atrav\u00e9s da estat\u00edstica. Isto \u00e9, n\u00e3o adianta voc\u00ea encontrar qualquer fun\u00e7\u00e3o de x , tem que encontrar a estat\u00edstica T em v .","title":"Crit\u00e9rio de Fatoriza\u00e7\u00e3o"},{"location":"infestatistica/SufficientStatistics/#estatisticas-conjuntas-suficientes","text":"Suponha que para cada \\theta , vetor, e cada valor das estat\u00edsticas (T_1, ..., T_k) = (t_1, ..., t_k) a distribui\u00e7\u00e3o conjunta condicional dos dados dadas as estat\u00edsticas n\u00e3o depende de \\theta . Veja que nesse caso, a diferen\u00e7a \u00e9 que condiciono em k estat\u00edsticas, k \\geq 1 .","title":"Estat\u00edsticas Conjuntas Suficientes"},{"location":"infestatistica/SufficientStatistics/#criterio-de-fatorizacao_1","text":"Sejam r_1, ..., r_k fun\u00e7\u00f5es de n vari\u00e1veis. A estat\u00edsticas T_i = r_i(X) s\u00e3o estat\u00edsticas suficientes conjuntas para \\theta se, e somene se, a pdf conjunta f_n(x|\\theta) pode ser fatorado como f_n(x|\\theta) = u(x)v[r_1(x), ..., r_k(x),\\theta], para todos os valores x \\in \\mathbb{R}^n e \\theta \\in \\Omega Obs.: Podemos mostrar que qualquer fun\u00e7\u00e3o injetiva de uma estat\u00edstica suficiente \u00e9 uma estat\u00edstica suficiente.","title":"Crit\u00e9rio de Fatoriza\u00e7\u00e3o"},{"location":"infestatistica/SufficientStatistics/#estatistica-suficiente-minima","text":"","title":"Estat\u00edstica Suficiente M\u00ednima"},{"location":"infestatistica/SufficientStatistics/#estatistica-de-ordem","text":"Considere uma amostra aleat\u00f3ria e a ordene. Diremos que a nova amostra, ordenada, \u00e9 uma estat\u00edstica de ordem. Observe que ela funciona como uma matrix de \"shifts\" que opera trocando as linhas do vetor de lugar. Por isso ela \u00e9 uma fun\u00e7\u00e3o. Essa estat\u00edstica \u00e9 sufciente conjunta para \\theta . O interessante que podemos ver isso dado que o produt\u00f3rio n\u00e3o importa a ordem.","title":"Estat\u00edstica de Ordem"},{"location":"infestatistica/SufficientStatistics/#estatistica-suficiente-minima_1","text":"\u00c9 uma estat\u00edstica T suficiente e, al\u00e9m disso, \u00e9 fun\u00e7\u00e3o de todas as outras estat\u00edsticas suficientes.","title":"Estat\u00edstica Suficiente M\u00ednima"},{"location":"infestatistica/SufficientStatistics/#mle-e-estatistica-suficiente","text":"Seja T uma estat\u00edstica suficiente para \\theta . Ent\u00e3o o estimador de m\u00e1xima verossimilhan\u00e7a \\hat{\\theta} depende das observa\u00e7\u00f5es somente atrav\u00e9s da estat\u00edstica T . Al\u00e9m disso, se \\hat{\\theta} \u00e9 suficiente, ent\u00e3o \u00e9 m\u00ednimo.","title":"MLE e Estat\u00edstica Suficiente"},{"location":"infestatistica/SufficientStatistics/#estatisticas-suficientes-e-estimador-de-bayes","text":"T = r(X) estat\u00edstica suficiente para \\theta . Ent\u00e3o todo estimador de Bayes \\hat{\\theta} depende nas observa\u00e7\u00f5es X_1, ..., X_n apenas atrav\u00e9s da estat\u00edstica T . Al\u00e9m do mais, se for suficiente, ser\u00e1 suficiente m\u00ednimo.","title":"Estat\u00edsticas Suficientes e Estimador de Bayes"},{"location":"infestatistica/SufficientStatistics/#definicoes-adicionais","text":"Considere uma amostra aleat\u00f3ria X_1,...,X_n","title":"Defini\u00e7\u00f5es Adicionais"},{"location":"infestatistica/SufficientStatistics/#estatistica-completa","text":"Seja t = T(X) estat\u00edstica. Se E[g(T(X))|\\theta] = 0, \\forall \\theta \\implies P[g(T(X)) = 0] = 1, ent\u00e3o ela \u00e9 dita completa.","title":"Estat\u00edstica Completa"},{"location":"infestatistica/SufficientStatistics/#estatistica-ancillary","text":"Suponha que queremos estimar \\theta e f_n(x|\\theta) seja a pdf conjunta. Seja A(X) uma estat\u00edstica. Se a sua distribui\u00e7\u00e3o n\u00e3o depende de \\theta , ent\u00e3o ser\u00e1 uma estat\u00edstica ancillary (auxiliar?) Por exemplo, se X_1, X_2 \\sim N(\\mu, \\sigma^2) e \\mu \u00e9 desconhecido, temos que X_1 - X_2 \\sim N(0, 2\\sigma^2) \u00e9 uma estat\u00edstica auxiliar.","title":"Estat\u00edstica Ancillary"},{"location":"infestatistica/SufficientStatistics/#melhorando-um-estimador","text":"Suponha que temos uma amostra aleat\u00f3ria X = (X_1, ..., X_n) cuja pdf \u00e9 f(x|\\theta) e \\theta \\in \\Omega desconhecido, tal que queremos estimar h(\\theta) para alguma fun\u00e7\u00e3o h . Seja Z = g(X_1, ..., X_n) . E_{\\theta}(Z) = \\int_{-\\infty}^{\\infty}...\\int_{-\\infty}^{\\infty} g(x)f_n(x|\\theta)dx_1, ..., dx_n Para cada estimado \\delta(X) e para todo valor de \\theta , definimos o MSE (Erro M\u00e9dio Quadr\u00e1tico) R(\\theta, \\delta) = E_{\\theta}\\{[\\delta(X) - h(\\theta)]^2\\} Quando n\u00e3o atribu\u00edmos uma priori para \\theta , ent\u00e3o queremos encontrar um estimador para que o MSE seja pequeno para v\u00e1rios valores de \\theta . Seja T uma estat\u00edstica suficiente conhecida. Definimos \\delta_0(T) = E_{\\theta}\\{\\delta(X)|T\\} \\overset{1}{=} E\\{\\delta(X)|T\\} (1) Agora, por que podemos chamar \\delta_0 de estimador se depende de \\theta ? Como T \u00e9 uma estat\u00edstica suficiente, a distribui\u00e7\u00e3o condicionada em T e em \\theta da amostra X_1, ..., X_n n\u00e3o depende de \\theta !!! Em particular o valor esperado do estimador \\delta(T) . Logo, como esse valor esperado n\u00e3o depende de \\theta , podemos dizer sim que ele \u00e9 um estimador.","title":"Melhorando um Estimador"},{"location":"infestatistica/SufficientStatistics/#teorema-rao-blackwell","text":"Teorema 7.9.1 do livro. Seja \\delta(X) um estimador e T uma estat\u00edstica suficiente para \\theta . O estimador \\delta_0(T) definido acima, para todo valor \\theta \\in \\Omega \u00e9: R(\\theta, \\delta_0) \\leq R(\\theta, \\delta), isto \u00e9, \u00e9 um estimador com menor erro quadr\u00e1tico m\u00e9dio (MSE). Em particular se R(\\theta, \\delta) < \\infty , a desigualdade se torna estrita, a menos que \\delta(X) seja um afun\u00e7\u00e3o de T , isto \u00e9, se \\delta(X) n\u00e3o for fun\u00e7\u00e3o de T , ent\u00e3o a desigualdade ser\u00e1 estrita. Por desigualdade estrita entenda < . Obs.: Chamamos o processo de melhorar um estimador com esse teorema de \"Rao-Blackwelliation\". Obs.2: Podemos generalizar um pouco mais. Para isso, pesquise sobre Conjuntos Convexos e sobre Fun\u00e7\u00f5es Convexas . Em um conjunto convexo, se a nossa fun\u00e7\u00e3o de perda n\u00e3o for o MSE, mas for uma fun\u00e7\u00e3o convexa, o teorema tamb\u00e9m valer\u00e1. Uma suposi\u00e7\u00e3o interessante que o Livro n\u00e3o imp\u00f5e \u00e9 que E[||\\delta(X)||) < \\infty .","title":"Teorema Rao - Blackwell"},{"location":"infestatistica/SufficientStatistics/#inadmissibilidade","text":"Suponha que R(\\theta, \\delta) \u00e9 MSE. O estimador \\delta \u00e9 inadimiss\u00edvel se existe outro estimador \\delta_0 tal que R(\\theta, \\delta_0) \\leq R(\\theta, \\delta) para todo valor de \\theta e existe a desigualdade estrita em, pelo menos um valor de \\theta . Dizemos nesse caso que \\delta_0 domina o estimador \\delta . Um estimador \\delta_0 \u00e9 admiss\u00edvel se n\u00e3o existe outro estimador que o domine.","title":"Inadmissibilidade"},{"location":"infestatistica/info/","text":"Informa\u00e7\u00f5es Gerais As monitorias ocorrem todas as quinta-feiras, \u00e0s 16h, atrav\u00e9s da plataforma Zoom. Link para acesso Monitorias gravadas Notebooks : Voc\u00ea pode rodar esses notebooks tamb\u00e9m! T\u00f3picos e Exerc\u00edcios Os exerc\u00edcios resolvidos dispon\u00edveis est\u00e3o linkados ao t\u00edtulo da se\u00e7\u00e3o. A1 Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios 7.2 Priori e Posteriori 2,3,10 7.3 Fam\u00edlias Conjugadas 2,17,19,21 7.4 Estimador de Bayes 2,4,7,11,14 7.5 Estimador de M\u00e1xima Verossimilhan\u00e7a 1,4,9,10 7.6 Propriedades EMV 3,5,11,20,22,23 7.7 Estat\u00edstica Suficiente 4,7,13,16 7.8 Estat\u00edstica Suficiente Conjunta 3,8,12,16 7.9 Melhorando um Estimador 2,3,6,9,10 8.7 Estimadores n\u00e3o viesados 4,6,11,13 8.8 Informa\u00e7\u00e3o de Fisher 5,7,10 8.1 Estimadores de Distribui\u00e7\u00f5es Amostrais 1,2,3,9 8.2 Distribui\u00e7\u00e3o Chi-Quadrada 4,7,10,13 - Exerc\u00edcios de Revis\u00e3o - A2 Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios 8.3 Distribui\u00e7\u00e3o conjunta da m\u00e9dia e vari\u00e2ncia amostral 8 8.4 A distribui\u00e7\u00e3o t Derivar a distribui\u00e7\u00e3o 8.5 Intervalos de Confian\u00e7a 1,4,5,6 9.1 Problemas de Teste de Hip\u00f3teses 3,8,13,19,21 9.5 Teste t 4,5,8 9.6 Comparando as m\u00e9dias de Normais - 9.7 Teste F Derivar a distribui\u00e7\u00e3o e teste de raz\u00e3o de verossimilhan\u00e7as 11.1 M\u00e9todo de M\u00ednimos Quadrados 3 11.2 Regress\u00e3o 2,3,6,19 11.3 Infer\u00eancia Estat\u00edstica sobre Regress\u00e3o Linear Simples - Resumos Probability and Statistics (Morris H. DeGroot) Cap\u00edtulo 7 Cap\u00edtulo 8 - parte 1 Cap\u00edtulo 8 - parte 2 Cap\u00edtulo 9 - Defini\u00e7\u00f5es Cap\u00edtulo 9 - Testes Documentos Adicionais Theory of Statistical Estimation (Ronald Fisher) : problema da estima\u00e7\u00e3o \u00e9 abordado e as t\u00e9cnicas apresentadas por Ronald Fisher. Ele se debru\u00e7a sobre estat\u00edsticas eficientes e suficientes. Mathematical Foudations Statistics (Ronald Fisher) : refer\u00eancia em estat\u00edstica com principais conceitos da mat\u00e9ria. Digital TextBook Statlect","title":"Informa\u00e7\u00f5es Gerais"},{"location":"infestatistica/info/#informacoes-gerais","text":"As monitorias ocorrem todas as quinta-feiras, \u00e0s 16h, atrav\u00e9s da plataforma Zoom. Link para acesso Monitorias gravadas Notebooks : Voc\u00ea pode rodar esses notebooks tamb\u00e9m!","title":"Informa\u00e7\u00f5es Gerais"},{"location":"infestatistica/info/#topicos-e-exercicios","text":"Os exerc\u00edcios resolvidos dispon\u00edveis est\u00e3o linkados ao t\u00edtulo da se\u00e7\u00e3o.","title":"T\u00f3picos e Exerc\u00edcios"},{"location":"infestatistica/info/#a1","text":"Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios 7.2 Priori e Posteriori 2,3,10 7.3 Fam\u00edlias Conjugadas 2,17,19,21 7.4 Estimador de Bayes 2,4,7,11,14 7.5 Estimador de M\u00e1xima Verossimilhan\u00e7a 1,4,9,10 7.6 Propriedades EMV 3,5,11,20,22,23 7.7 Estat\u00edstica Suficiente 4,7,13,16 7.8 Estat\u00edstica Suficiente Conjunta 3,8,12,16 7.9 Melhorando um Estimador 2,3,6,9,10 8.7 Estimadores n\u00e3o viesados 4,6,11,13 8.8 Informa\u00e7\u00e3o de Fisher 5,7,10 8.1 Estimadores de Distribui\u00e7\u00f5es Amostrais 1,2,3,9 8.2 Distribui\u00e7\u00e3o Chi-Quadrada 4,7,10,13 - Exerc\u00edcios de Revis\u00e3o -","title":"A1"},{"location":"infestatistica/info/#a2","text":"Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios 8.3 Distribui\u00e7\u00e3o conjunta da m\u00e9dia e vari\u00e2ncia amostral 8 8.4 A distribui\u00e7\u00e3o t Derivar a distribui\u00e7\u00e3o 8.5 Intervalos de Confian\u00e7a 1,4,5,6 9.1 Problemas de Teste de Hip\u00f3teses 3,8,13,19,21 9.5 Teste t 4,5,8 9.6 Comparando as m\u00e9dias de Normais - 9.7 Teste F Derivar a distribui\u00e7\u00e3o e teste de raz\u00e3o de verossimilhan\u00e7as 11.1 M\u00e9todo de M\u00ednimos Quadrados 3 11.2 Regress\u00e3o 2,3,6,19 11.3 Infer\u00eancia Estat\u00edstica sobre Regress\u00e3o Linear Simples -","title":"A2"},{"location":"infestatistica/info/#resumos","text":"Probability and Statistics (Morris H. DeGroot) Cap\u00edtulo 7 Cap\u00edtulo 8 - parte 1 Cap\u00edtulo 8 - parte 2 Cap\u00edtulo 9 - Defini\u00e7\u00f5es Cap\u00edtulo 9 - Testes","title":"Resumos"},{"location":"infestatistica/info/#documentos-adicionais","text":"Theory of Statistical Estimation (Ronald Fisher) : problema da estima\u00e7\u00e3o \u00e9 abordado e as t\u00e9cnicas apresentadas por Ronald Fisher. Ele se debru\u00e7a sobre estat\u00edsticas eficientes e suficientes. Mathematical Foudations Statistics (Ronald Fisher) : refer\u00eancia em estat\u00edstica com principais conceitos da mat\u00e9ria. Digital TextBook Statlect","title":"Documentos Adicionais"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/","text":"An\u00e1lise Bayesiana de amostras da distribui\u00e7\u00e3o Normal Refer\u00eancias [1] Andrew Gelman. Bayesian Data Analysis. Parte I, Se\u00e7\u00e3o 3.3: essa refer\u00eancia usa a vari\u00e2ncia ao inv\u00e9s da precis\u00e3o. [2] Kevin P. Murphy. Conjugate Bayesian analysis of the Gaussian distribution: N\u00e3o muito simples, mas possui todas as contas. Precis\u00e3o da distribui\u00e7\u00e3o normal Definimos \\tau := 1/\\sigma^2 como a precis\u00e3o da distribui\u00e7\u00e3o normal. A fun\u00e7\u00e3o da densidade de probabilidade da distribui\u00e7\u00e3o normal f(x|\\mu,\\tau) \u00e9, -\\infty < x < \\infty : f(x|\\mu,\\tau) = \\left(\\frac{\\tau}{2\\pi}\\right)^{1/2}\\exp\\left[-\\frac{1}{2}\\tau(x-\\mu)^2\\right] Teorema Suponha que X_1,...,X_n \\overset{iid}{\\sim} N_2(\\mu, \\tau) , desconhecidos. Suponha que \\mu|\\tau \\sim N_2(\\mu_0, \\lambda_0 \\tau) \\tau \\sim \\text{Gamma}(\\alpha_0, \\beta_0) Ent\u00e3o a distribui\u00e7\u00e3o conjunta de \\mu e \\tau a posteriori \u00e9 dada por: \\mu|\\tau, X_1,...,X_n \\sim N_2(\\mu_1, \\lambda_1\\tau) \\tau|X_1,...,X_n \\sim \\text{Gamma}(\\alpha_1, \\beta_1), onde Par\u00e2metro Valor a posteriori do par\u00e2metro \\mu_1 \\frac{\\lambda_0 \\mu_0 + n\\bar{x}_n}{\\lambda_0 + n} \\lambda_1 \\lambda_0 + n \\alpha_1 \\alpha_0 + n/2 \\beta_1 \\beta_0 + s_n^2/2 + \\frac{n\\lambda_0(\\bar{x}_n - \\mu_0)^2}{2(\\lambda_0 + n)} Fam\u00edlia Normal-Gamma Sejam \\mu e \\tau vari\u00e1veis aleat\u00f3rias. Suponha que a distribui\u00e7\u00e3o condicional de \\mu dado \\tau \u00e9 normal com m\u00e9dia \\mu_0 e precis\u00e3o \\lambda_0 \\tau e que a distribui\u00e7\u00e3o marginal de \\tau seja gamma com par\u00e2metros \\alpha_0, \\beta_0 . Ent\u00e3o, falamos que a distribui\u00e7\u00e3o conjunta de \\mu e \\tau \u00e9 a distribui\u00e7\u00e3o normal-gamma com hiperpar\u00e2meteros \\mu_0, \\lambda_0, \\alpha_0 e \\beta_0 . A distribui\u00e7\u00e3o marginal da m\u00e9dia \\mu Suponha que a distribui\u00e7\u00e3o a priori \\mu e \\tau seja normal-gamma com hiperpar\u00e2metros \\mu_0, \\lambda_0, \\alpha_0 e \\beta_0 . Ent\u00e3o \\left(\\frac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{1/2}(\\mu - \\mu_0) tem a distribui\u00e7\u00e3o t com 2\\alpha_0 graus de liberdade. Nesse caso, se \\alpha_0 > 1/2 , E(\\mu) = \\mu_0 . Se \\alpha_0 > 1 , Var(\\mu) \\frac{\\beta_0}{\\lambda_0(\\alpha_0 - 1)} Obs.: As condi\u00e7\u00f5es sobre \\alpha_0 , vem da exist\u00eancia do k momento somente se o grau de liberdade da distribui\u00e7\u00e3o t \u00e9 maior do que k , isto \u00e9, 2\\alpha_0 > k . Compara\u00e7\u00e3o com Intervalos de Confian\u00e7a Podemos construir intervalos de confian\u00e7a para \\mu no mundo Bayesiano, pois ela \u00e9 uma vari\u00e1vel aleat\u00f3ria. Nesse caso, podemos fazer da seguinte maneira: \\begin{split} &P(-c < \\left(\\frac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{1/2}(\\mu - \\mu_0) < c) = \\gamma \\\\ &P(\\mu_0 - c\\left(\\frac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{1/2} < \\mu < \\mu_0 + c\\left(\\frac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{1/2}) &= \\gamma \\end{split} Ou seja, ganhamos um intervalo de confian\u00e7a de gra\u00e7a! Implementa\u00e7\u00e3o Quando temos dados, X_1, ..., X_n \\sim N(\\mu, \\sigma^2) , podemos fazer \\hat{\\mu} = \\bar{X}_n que \u00e9 uma pontual (s\u00f3 para um valor de \\mu ). Mas isso nem sempre \u00e9 o melhor, e \u00e0s vezes nem t\u00e3o prazeroso. Por isso precisamos trazer Bayes para nossa an\u00e1lise. import numpy as np from scipy.stats import norm , gamma , t import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns . set () Nesse exemplos, vamos estudar o exemplo Casas de Enfermagem em Novo M\u00e9xico , tema estudado por Howard L. Smith, Neill F. Piland e Nancy Fisher . Nesse trabalho explora os desafios da enfermagem na \u00e1rea rural, para que se mantenha a viabilidade. Vamos utilizar o banco de dados deles, que incluem as seguintes informa\u00e7\u00f5es: BED: n\u00fameros de camas na casa. MCDAYS: dias anuais de interna\u00e7\u00e3o m\u00e9dica (centenas) TDAYS: total anual de pacientes dias (centenas) PCREV: receita anual total de atendimento ao paciente (centenas) NSAL: sal\u00e1rio das enfermeiras anual (centenas) FEXP: despesas anuais com instala\u00e7\u00f5es (centenas) RURAL: 1, se rural, 0, se urbano Os dados se encontram nesse site . table = [] with open ( '../data/nursinghome.txt' , 'r' ) as f : line = f . readline () table . append ( line . split ()) line = f . readline () while line != '' : table . append ([ int ( i ) for i in line . split ()]) line = f . readline () nurse_df = pd . DataFrame ( data = table [ 1 :], columns = table [ 0 ], dtype = np . int ) Vamos considerar nesse exemplo a coluna MCDAYS, restrita \u00e0s casas urbanas, que denotaremos por X . Antes de observarmos os dados, vamos modelar o valor de X para cada casa como uma vari\u00e1vel aleat\u00f3ria normal com m\u00e9dia \\mu e precis\u00e3o \\tau . urban_nurse_df = nurse_df [ nurse_df . RURAL == 0 ] plt . bar ( x = range ( len ( urban_nurse_df )), height = urban_nurse_df . MCDAYS ) plt . title ( 'MCDAYS por casa urbana de enfermeiras' ) plt . show () Para calcular nossa priori, dever\u00edamos conversar com especialistas. Como n\u00e3o \u00e9 o caso, vamos usar as informa\u00e7\u00f5es de camas. Temos: print ( 'M\u00e9dia: {:.1f}, Desvio-Padr\u00e3o: {:.1f}' . format ( np . mean ( urban_nurse_df . BED ), np . std ( urban_nurse_df . BED ))) M\u00e9dia: 111.4, Desvio-Padr\u00e3o: 42.3 Podemos supor, a priori, que a taxa de ocupa\u00e7\u00e3o \u00e9 de 50%. Logo, em um ano, podemos obter que os dias anuais de interna\u00e7\u00e3o m\u00e9dica s\u00e3o: media = 0.5 * 365 * np . mean ( urban_nurse_df . BED ) / 100 # unidade em centenas std = 0.5 * 365 * np . std ( urban_nurse_df . BED ) / 100 Agora precisamos mapear esses valores para os hiperpar\u00e2metros a priori \\alpha_0, \\beta_0, \\lambda_0, \\mu_0 . Vamos dibidir a vari\u00e2ncia obtida acima (usando o n\u00famero de camas), como incerteza, e dividiremos igualmente essa incerteza sobre a m\u00e9dia e a precis\u00e3o, isto \u00e9, Var(\\mu) = std^2/2 E(\\tau) = 1/(std^2/2) Escolhemos \\alpha_0 = 2 (arbitr\u00e1rio, mas prefer\u00edvel a ser pequeno, porque esse par\u00e2metro tem a interpreta\u00e7\u00e3o de ser o conhecimento sobre o valor. Logo E[\\tau] = \\alpha_0/\\beta_0 \\implies \\beta_0 = \\alpha_0/E[\\tau] E[\\mu] = \\mu_0 Var(\\mu) = \\frac{\\beta_0}{\\lambda_0(\\alpha_0 - 1)} \\implies \\lambda_0 = \\frac{\\beta_0}{Var(\\mu)(\\alpha_0 - 1)} alpha0 = 2 beta0 = alpha * std ** 2 / 2 mu0 = media lambda0 = beta * 2 / ( std ** 2 * ( alpha - 1 )) Agora conseguimos expressar nosso conhecimento a priori para construir o primeiro intervalo de confian\u00e7a. Vamos usar a express\u00e3o constru\u00edda anteriormente. Primeiro, vamos ver, numericamente, a distribui\u00e7\u00e3o de \\mu def draw_samples ( alpha0 , beta0 , lambda0 , mu0 , seed = 10000 ): r = np . random . RandomState ( seed ) tau = r . gamma ( shape = alpha0 , scale = 1 / beta0 , size = 100000 ) mu = r . normal ( mu0 , scale = np . sqrt ( 1 / ( lambda0 * tau ))) return mu , tau mu_samples , tau_samples = draw_samples ( alpha0 , beta0 , lambda0 , mu0 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) sns . histplot ( mu_samples , ax = ax [ 0 ]) sns . histplot ( tau_samples , ax = ax [ 1 ]) ax [ 0 ] . set_title ( r '$\\mu$ a priori' ) ax [ 1 ] . set_title ( r '$\\tau$ a priori' ) ax [ 0 ] . set_xlim (( 0 , 500 )) plt . show () ci_mu = ( np . quantile ( mu_samples , 0.025 ), np . quantile ( mu_samples , 0.975 )) print ( ci_mu ) (95.85458277508882, 309.0974651995673) Agora, vamos observar os dados e calcular nossa posteriori! Vamos atualizar como demonstrado no Teorema. Par\u00e2metro Valor a posteriori do par\u00e2metro \\mu_1 \\frac{\\lambda_0 \\mu_0 + n\\bar{x}_n}{\\lambda_0 + n} \\lambda_1 \\lambda_0 + n \\alpha_1 \\alpha_0 + n/2 \\beta_1 \\beta_0 + s_n^2/2 + \\frac{n\\lambda_0(\\bar{x}_n - \\mu_0)^2}{2(\\lambda_0 + n)} n = len ( urban_nurse_df ) mom1 = np . mean ( urban_nurse_df . MCDAYS ) mom2 = np . var ( urban_nurse_df . MCDAYS ) * n mu1 = ( lambda0 * mu0 + n * mom1 ) / ( lambda0 + n ) lambda1 = lambda0 + n alpha1 = alpha0 + n / 2 beta1 = beta0 + mom2 / 2 + ( n * lambda0 * ( mom1 - mu0 ) ** 2 ) / ( 2 * ( lambda0 + n )) mu_samples_post , tau_samples_post = draw_samples ( alpha1 , beta1 , lambda1 , mu1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) sns . histplot ( mu_samples_post , ax = ax [ 0 ]) sns . histplot ( tau_samples_post , ax = ax [ 1 ]) ax [ 0 ] . set_title ( r '$\\mu$ a posteriori' ) ax [ 1 ] . set_title ( r '$\\tau$ a posteriori' ) ax [ 0 ] . set_xlim (( 100 , 300 )) plt . show () ci_mu = ( np . quantile ( mu_samples_post , 0.025 ), np . quantile ( mu_samples_post , 0.975 )) print ( ci_mu ) (152.61993663355443, 215.58303707725233) Observe como o intervalo de confian\u00e7a est\u00e1 bem menor! Ou seja, os dados aumentaram nossa certeza sobre o par\u00e2metro. Vamos comparar os valores. fig , ax = plt . subplots ( figsize = ( 10 , 5 )) sns . histplot ( mu_samples_post , ax = ax , kde = True , stat = 'density' , label = 'Posteriori' ) sns . histplot ( mu_samples , ax = ax , kde = True , stat = 'density' , label = 'Priori' , color = 'red' ) ax . set_title ( r '$\\mu$' ) ax . set_xlim (( - 200 , 400 )) ax . legend () plt . show () Por fim, vamos comparar o estimador de Bayes com o de M\u00e1xima Verossimilhan\u00e7a: mle = mom1 eb = np . mean ( mu_samples_post ) print ( 'MLE: {}' . format ( mle )) print ( 'Bayes Estimator: {}' . format ( eb )) MLE: 182.16666666666666 Bayes Estimator: 184.21760983937554","title":"An\u00e1lise Bayesiana da Normal"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#analise-bayesiana-de-amostras-da-distribuicao-normal","text":"","title":"An\u00e1lise Bayesiana de amostras da distribui\u00e7\u00e3o Normal"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#referencias","text":"[1] Andrew Gelman. Bayesian Data Analysis. Parte I, Se\u00e7\u00e3o 3.3: essa refer\u00eancia usa a vari\u00e2ncia ao inv\u00e9s da precis\u00e3o. [2] Kevin P. Murphy. Conjugate Bayesian analysis of the Gaussian distribution: N\u00e3o muito simples, mas possui todas as contas.","title":"Refer\u00eancias"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#precisao-da-distribuicao-normal","text":"Definimos \\tau := 1/\\sigma^2 como a precis\u00e3o da distribui\u00e7\u00e3o normal. A fun\u00e7\u00e3o da densidade de probabilidade da distribui\u00e7\u00e3o normal f(x|\\mu,\\tau) \u00e9, -\\infty < x < \\infty : f(x|\\mu,\\tau) = \\left(\\frac{\\tau}{2\\pi}\\right)^{1/2}\\exp\\left[-\\frac{1}{2}\\tau(x-\\mu)^2\\right]","title":"Precis\u00e3o da distribui\u00e7\u00e3o normal"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#teorema","text":"Suponha que X_1,...,X_n \\overset{iid}{\\sim} N_2(\\mu, \\tau) , desconhecidos. Suponha que \\mu|\\tau \\sim N_2(\\mu_0, \\lambda_0 \\tau) \\tau \\sim \\text{Gamma}(\\alpha_0, \\beta_0) Ent\u00e3o a distribui\u00e7\u00e3o conjunta de \\mu e \\tau a posteriori \u00e9 dada por: \\mu|\\tau, X_1,...,X_n \\sim N_2(\\mu_1, \\lambda_1\\tau) \\tau|X_1,...,X_n \\sim \\text{Gamma}(\\alpha_1, \\beta_1), onde Par\u00e2metro Valor a posteriori do par\u00e2metro \\mu_1 \\frac{\\lambda_0 \\mu_0 + n\\bar{x}_n}{\\lambda_0 + n} \\lambda_1 \\lambda_0 + n \\alpha_1 \\alpha_0 + n/2 \\beta_1 \\beta_0 + s_n^2/2 + \\frac{n\\lambda_0(\\bar{x}_n - \\mu_0)^2}{2(\\lambda_0 + n)}","title":"Teorema"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#familia-normal-gamma","text":"Sejam \\mu e \\tau vari\u00e1veis aleat\u00f3rias. Suponha que a distribui\u00e7\u00e3o condicional de \\mu dado \\tau \u00e9 normal com m\u00e9dia \\mu_0 e precis\u00e3o \\lambda_0 \\tau e que a distribui\u00e7\u00e3o marginal de \\tau seja gamma com par\u00e2metros \\alpha_0, \\beta_0 . Ent\u00e3o, falamos que a distribui\u00e7\u00e3o conjunta de \\mu e \\tau \u00e9 a distribui\u00e7\u00e3o normal-gamma com hiperpar\u00e2meteros \\mu_0, \\lambda_0, \\alpha_0 e \\beta_0 .","title":"Fam\u00edlia Normal-Gamma"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#a-distribuicao-marginal-da-media-mu","text":"Suponha que a distribui\u00e7\u00e3o a priori \\mu e \\tau seja normal-gamma com hiperpar\u00e2metros \\mu_0, \\lambda_0, \\alpha_0 e \\beta_0 . Ent\u00e3o \\left(\\frac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{1/2}(\\mu - \\mu_0) tem a distribui\u00e7\u00e3o t com 2\\alpha_0 graus de liberdade. Nesse caso, se \\alpha_0 > 1/2 , E(\\mu) = \\mu_0 . Se \\alpha_0 > 1 , Var(\\mu) \\frac{\\beta_0}{\\lambda_0(\\alpha_0 - 1)} Obs.: As condi\u00e7\u00f5es sobre \\alpha_0 , vem da exist\u00eancia do k momento somente se o grau de liberdade da distribui\u00e7\u00e3o t \u00e9 maior do que k , isto \u00e9, 2\\alpha_0 > k .","title":"A distribui\u00e7\u00e3o marginal da m\u00e9dia \\mu"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#comparacao-com-intervalos-de-confianca","text":"Podemos construir intervalos de confian\u00e7a para \\mu no mundo Bayesiano, pois ela \u00e9 uma vari\u00e1vel aleat\u00f3ria. Nesse caso, podemos fazer da seguinte maneira: \\begin{split} &P(-c < \\left(\\frac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{1/2}(\\mu - \\mu_0) < c) = \\gamma \\\\ &P(\\mu_0 - c\\left(\\frac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{1/2} < \\mu < \\mu_0 + c\\left(\\frac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{1/2}) &= \\gamma \\end{split} Ou seja, ganhamos um intervalo de confian\u00e7a de gra\u00e7a!","title":"Compara\u00e7\u00e3o com Intervalos de Confian\u00e7a"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#implementacao","text":"Quando temos dados, X_1, ..., X_n \\sim N(\\mu, \\sigma^2) , podemos fazer \\hat{\\mu} = \\bar{X}_n que \u00e9 uma pontual (s\u00f3 para um valor de \\mu ). Mas isso nem sempre \u00e9 o melhor, e \u00e0s vezes nem t\u00e3o prazeroso. Por isso precisamos trazer Bayes para nossa an\u00e1lise. import numpy as np from scipy.stats import norm , gamma , t import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns . set () Nesse exemplos, vamos estudar o exemplo Casas de Enfermagem em Novo M\u00e9xico , tema estudado por Howard L. Smith, Neill F. Piland e Nancy Fisher . Nesse trabalho explora os desafios da enfermagem na \u00e1rea rural, para que se mantenha a viabilidade. Vamos utilizar o banco de dados deles, que incluem as seguintes informa\u00e7\u00f5es: BED: n\u00fameros de camas na casa. MCDAYS: dias anuais de interna\u00e7\u00e3o m\u00e9dica (centenas) TDAYS: total anual de pacientes dias (centenas) PCREV: receita anual total de atendimento ao paciente (centenas) NSAL: sal\u00e1rio das enfermeiras anual (centenas) FEXP: despesas anuais com instala\u00e7\u00f5es (centenas) RURAL: 1, se rural, 0, se urbano Os dados se encontram nesse site . table = [] with open ( '../data/nursinghome.txt' , 'r' ) as f : line = f . readline () table . append ( line . split ()) line = f . readline () while line != '' : table . append ([ int ( i ) for i in line . split ()]) line = f . readline () nurse_df = pd . DataFrame ( data = table [ 1 :], columns = table [ 0 ], dtype = np . int ) Vamos considerar nesse exemplo a coluna MCDAYS, restrita \u00e0s casas urbanas, que denotaremos por X . Antes de observarmos os dados, vamos modelar o valor de X para cada casa como uma vari\u00e1vel aleat\u00f3ria normal com m\u00e9dia \\mu e precis\u00e3o \\tau . urban_nurse_df = nurse_df [ nurse_df . RURAL == 0 ] plt . bar ( x = range ( len ( urban_nurse_df )), height = urban_nurse_df . MCDAYS ) plt . title ( 'MCDAYS por casa urbana de enfermeiras' ) plt . show () Para calcular nossa priori, dever\u00edamos conversar com especialistas. Como n\u00e3o \u00e9 o caso, vamos usar as informa\u00e7\u00f5es de camas. Temos: print ( 'M\u00e9dia: {:.1f}, Desvio-Padr\u00e3o: {:.1f}' . format ( np . mean ( urban_nurse_df . BED ), np . std ( urban_nurse_df . BED ))) M\u00e9dia: 111.4, Desvio-Padr\u00e3o: 42.3 Podemos supor, a priori, que a taxa de ocupa\u00e7\u00e3o \u00e9 de 50%. Logo, em um ano, podemos obter que os dias anuais de interna\u00e7\u00e3o m\u00e9dica s\u00e3o: media = 0.5 * 365 * np . mean ( urban_nurse_df . BED ) / 100 # unidade em centenas std = 0.5 * 365 * np . std ( urban_nurse_df . BED ) / 100 Agora precisamos mapear esses valores para os hiperpar\u00e2metros a priori \\alpha_0, \\beta_0, \\lambda_0, \\mu_0 . Vamos dibidir a vari\u00e2ncia obtida acima (usando o n\u00famero de camas), como incerteza, e dividiremos igualmente essa incerteza sobre a m\u00e9dia e a precis\u00e3o, isto \u00e9, Var(\\mu) = std^2/2 E(\\tau) = 1/(std^2/2) Escolhemos \\alpha_0 = 2 (arbitr\u00e1rio, mas prefer\u00edvel a ser pequeno, porque esse par\u00e2metro tem a interpreta\u00e7\u00e3o de ser o conhecimento sobre o valor. Logo E[\\tau] = \\alpha_0/\\beta_0 \\implies \\beta_0 = \\alpha_0/E[\\tau] E[\\mu] = \\mu_0 Var(\\mu) = \\frac{\\beta_0}{\\lambda_0(\\alpha_0 - 1)} \\implies \\lambda_0 = \\frac{\\beta_0}{Var(\\mu)(\\alpha_0 - 1)} alpha0 = 2 beta0 = alpha * std ** 2 / 2 mu0 = media lambda0 = beta * 2 / ( std ** 2 * ( alpha - 1 )) Agora conseguimos expressar nosso conhecimento a priori para construir o primeiro intervalo de confian\u00e7a. Vamos usar a express\u00e3o constru\u00edda anteriormente. Primeiro, vamos ver, numericamente, a distribui\u00e7\u00e3o de \\mu def draw_samples ( alpha0 , beta0 , lambda0 , mu0 , seed = 10000 ): r = np . random . RandomState ( seed ) tau = r . gamma ( shape = alpha0 , scale = 1 / beta0 , size = 100000 ) mu = r . normal ( mu0 , scale = np . sqrt ( 1 / ( lambda0 * tau ))) return mu , tau mu_samples , tau_samples = draw_samples ( alpha0 , beta0 , lambda0 , mu0 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) sns . histplot ( mu_samples , ax = ax [ 0 ]) sns . histplot ( tau_samples , ax = ax [ 1 ]) ax [ 0 ] . set_title ( r '$\\mu$ a priori' ) ax [ 1 ] . set_title ( r '$\\tau$ a priori' ) ax [ 0 ] . set_xlim (( 0 , 500 )) plt . show () ci_mu = ( np . quantile ( mu_samples , 0.025 ), np . quantile ( mu_samples , 0.975 )) print ( ci_mu ) (95.85458277508882, 309.0974651995673) Agora, vamos observar os dados e calcular nossa posteriori! Vamos atualizar como demonstrado no Teorema. Par\u00e2metro Valor a posteriori do par\u00e2metro \\mu_1 \\frac{\\lambda_0 \\mu_0 + n\\bar{x}_n}{\\lambda_0 + n} \\lambda_1 \\lambda_0 + n \\alpha_1 \\alpha_0 + n/2 \\beta_1 \\beta_0 + s_n^2/2 + \\frac{n\\lambda_0(\\bar{x}_n - \\mu_0)^2}{2(\\lambda_0 + n)} n = len ( urban_nurse_df ) mom1 = np . mean ( urban_nurse_df . MCDAYS ) mom2 = np . var ( urban_nurse_df . MCDAYS ) * n mu1 = ( lambda0 * mu0 + n * mom1 ) / ( lambda0 + n ) lambda1 = lambda0 + n alpha1 = alpha0 + n / 2 beta1 = beta0 + mom2 / 2 + ( n * lambda0 * ( mom1 - mu0 ) ** 2 ) / ( 2 * ( lambda0 + n )) mu_samples_post , tau_samples_post = draw_samples ( alpha1 , beta1 , lambda1 , mu1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) sns . histplot ( mu_samples_post , ax = ax [ 0 ]) sns . histplot ( tau_samples_post , ax = ax [ 1 ]) ax [ 0 ] . set_title ( r '$\\mu$ a posteriori' ) ax [ 1 ] . set_title ( r '$\\tau$ a posteriori' ) ax [ 0 ] . set_xlim (( 100 , 300 )) plt . show () ci_mu = ( np . quantile ( mu_samples_post , 0.025 ), np . quantile ( mu_samples_post , 0.975 )) print ( ci_mu ) (152.61993663355443, 215.58303707725233) Observe como o intervalo de confian\u00e7a est\u00e1 bem menor! Ou seja, os dados aumentaram nossa certeza sobre o par\u00e2metro. Vamos comparar os valores. fig , ax = plt . subplots ( figsize = ( 10 , 5 )) sns . histplot ( mu_samples_post , ax = ax , kde = True , stat = 'density' , label = 'Posteriori' ) sns . histplot ( mu_samples , ax = ax , kde = True , stat = 'density' , label = 'Priori' , color = 'red' ) ax . set_title ( r '$\\mu$' ) ax . set_xlim (( - 200 , 400 )) ax . legend () plt . show () Por fim, vamos comparar o estimador de Bayes com o de M\u00e1xima Verossimilhan\u00e7a: mle = mom1 eb = np . mean ( mu_samples_post ) print ( 'MLE: {}' . format ( mle )) print ( 'Bayes Estimator: {}' . format ( eb )) MLE: 182.16666666666666 Bayes Estimator: 184.21760983937554","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/","text":"Intervalos de Confian\u00e7a Esse tema procura responder qu\u00e3o confian\u00e7a dever\u00edamos por em um estimador. \u00c9 claro que essa pergunta tem que ser um pouco melhor descrita matematicamente. A ideia \u00e9 frequentista e tem a ideia a seguinte forma: O intervalo [a,b] , uma realiza\u00e7\u00e3o de [A,B] , tem 95% de confian\u00e7a se em 95% do tempo, o par\u00e2metro procurado est\u00e1 entre a e b . Veja que a ideia \u00e9 basicamente frequentista, dado que a interpreta\u00e7\u00e3o est\u00e1 ligada \u00e0 frequ\u00eancia quando o n\u00famero de experimentos tende para infinito. (Cuidado: N\u00e3o vamos falar da probabilidade do par\u00e2metro estar em [a,b] , isso n\u00e3o faz sentido, pois \\theta n\u00e3o \u00e9 uma vari\u00e1vel aleat\u00f3ria, e sim um valor fixo). Defini\u00e7\u00e3o Seja X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Sejam A \\leq B duas estat\u00edsticas que possuem a propriedade, para todo \\theta , P(A < g(\\theta) < B) \\geq \\gamma Chamamos (A,B) de intevalo de confian\u00e7a para g(\\theta) com coeficiente \\gamma . O intervalo \u00e9 chamado de exato se ao inv\u00e9s da desigualdade, tivermos uma igualdade. Ap\u00f3s observarmos os valores de X_1, ..., X_n e computarmos A = a e B = b , o intervalo (a,b) \u00e9 chamado de valor observado do intervalo de confian\u00e7a. Intervalo de Confian\u00e7a para a m\u00e9dia de N(\\mu, \\sigma^2) Seja X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Para cada 0 < \\gamma < 1 , o intervalo (A,B) \u00e9 intervalo de confian\u00e7a exato para \\mu com coeficiente \\gamma , em que: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} onde T_{n-1} denota a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. O interessante \u00e9 que isso \u00e9 implica\u00e7\u00e3o direta da distribui\u00e7\u00e3o de U = \\frac{n^{1/2}(\\bar{X}_n - \\mu)}{\\sigma '} que inferimos no cap\u00edtulo 8.4, nesse caso, simplemente fizemos a transforma\u00e7\u00e3o: \\gamma = P(-c < U < c) = P(A < \\mu < B) e c \u00e9 escolhido de acordo com \\gamma . Implementa\u00e7\u00e3o Vamos rever a informa\u00e7\u00e3o sobre caf\u00e9 que usamos cap\u00edtulos antes para ver como isso acontece na pr\u00e1tica. Considere dados sobre pesos de beb\u00eas logo ao nascer. bwt: peso do beb\u00ea ao nascer. gestation: dura\u00e7\u00e3o em dias da gesta\u00e7\u00e3o. parity: primeiro filho ou n\u00e3o. age: idade da m\u00e3e. height: altura da m\u00e3e em polegadas. weight: peso da m\u00e3e em pounds. smoke: se a m\u00e3e \u00e9 fumante ou n\u00e3o. # Importando bibliotecas import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import t birth_df = pd . read_csv ( \"http://people.reed.edu/~jones/141/Bwt.dat\" ) birth_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bwt gestation parity age height weight smoke 0 120 284 0 27 62 100 0 1 113 282 0 33 64 135 0 2 128 279 0 28 64 115 1 3 108 282 0 23 67 125 1 4 136 286 0 25 62 93 0 sns . histplot ( data = birth_df . bwt , kde = True ) plt . title ( 'Histograma dos pesos dos beb\u00eas' ) plt . show () birth_df [ birth_df . smoke == 0 ] . bwt . hist ( density = True , label = 'N\u00e3o fumante' ) birth_df [ birth_df . smoke == 1 ] . bwt . hist ( density = True , label = 'Fumante' , alpha = 0.6 ) plt . xlabel ( 'Peso' ) plt . legend () plt . show () Sabemos que essa \u00e9 uma extra\u00e7\u00e3o de uma popula\u00e7\u00e3o maior. Para conseguirmos mais amostras, vamos usar um procedimento chamado bootstrap . A ideia desse procedimento \u00e9 criar um novas amostras a partir de uma amostra inicial, usando replace = True como diferencial. Vou fazer esse procedimento diversar vezes e ir calculando a m\u00e9dia amostral. Como a m\u00e9dia amostral \u00e9 uma vari\u00e1vel aleat\u00f3ria, vamos obter um histograma das realiza\u00e7\u00f5es. Vamos supor que o peso W_i da crian\u00e7a i vem de uma distribui\u00e7\u00e3o com par\u00e2metros \\mu e \\sigma^2 desconhecidos. Nesse caso, \\bar{W}_i vir\u00e1 de uma distribui\u00e7\u00e3o normal com par\u00e2metros \\mu e \\sigma^2/n . ite = 10000 n = 200 bootstrap_means = np . zeros ( ite ) for i in range ( ite ): bootstrap_sample = birth_df . sample ( n = n , replace = True , random_state = i ) bootstrap_means [ i ] = bootstrap_sample . bwt . mean () sns . histplot ( bootstrap_means , kde = True ) plt . title ( \"M\u00e9dias das amostras\" ) plt . xlabel ( 'Peso' ) plt . show () Vamos calcular o nosso intervalo de confian\u00e7a com \\gamma = 0.95 . Temos que: gamma = 0.95 A = lambda x : np . mean ( x ) - t . ppf ( q = ( 1 + gamma ) / 2 , df = len ( x ) - 1 ) * np . std ( x , ddof = 1 ) / len ( x ) ** ( 1 / 2 ) B = lambda x : np . mean ( x ) + t . ppf ( q = ( 1 + gamma ) / 2 , df = len ( x ) - 1 ) * np . std ( x , ddof = 1 ) / len ( x ) ** ( 1 / 2 ) ite = 100 n = 500 bootstrap_intervals = np . zeros (( ite , 2 )) for i in range ( ite ): bootstrap_sample = birth_df . sample ( n = n , replace = True , random_state = i ) bootstrap_intervals [ i , 0 ] = A ( bootstrap_sample . bwt ) bootstrap_intervals [ i , 1 ] = B ( bootstrap_sample . bwt ) out_values = np . where (( bootstrap_intervals [:, 0 ] > 119.5 ) | ( bootstrap_intervals [:, 1 ] < 119.5 )) plt . figure ( figsize = ( 6 , 10 )) plt . scatter ( bootstrap_intervals [:, 0 ], np . arange ( 0 , ite ), color = 'red' , label = 'a' ) plt . scatter ( bootstrap_intervals [:, 1 ], np . arange ( 0 , ite ), color = 'green' , label = 'b' ) plt . scatter ( bootstrap_intervals [ out_values [ 0 ], 0 ], out_values [ 0 ], color = 'black' , label = 'Fora' ) plt . scatter ( bootstrap_intervals [ out_values [ 0 ], 1 ], out_values [ 0 ], color = 'black' ) plt . vlines ( 119.5 , ymin = 0 , ymax = ite , linestyle = '--' , alpha = 0.6 , label = 'M\u00e9dia real' ) plt . legend () plt . show () Interpreta\u00e7\u00e3o Estamos fazer uma afirma\u00e7\u00e3o probabil\u00edstica sobre o intervalo (A,B) antes de observar os dados. Ap\u00f3s observarmos os dados, n\u00e3o podemos interpretar (a,b) como um intervalo em que temos 95% de confian\u00e7a de g(\\theta) estar no intervalo. Antes de observarmos as amostras, temos a confian\u00e7a de que 95% dos intervalos conter\u00e3o \\mu . Sem simetria Construimos anteriormente um intervalo sim\u00e9trico, onde a estat\u00edstica U acima mencionada estaria entre -c e c com probabilidade \\gamma . Mas podemos desenvolver intervalos n\u00e3o sim\u00e9tricos tamb\u00e9m. Uma forma que podemos fazer isso \u00e9 escolhendo \\gamma_1 e \\gamma_2 , tal que \\gamma_2 - \\gamma_1 = \\gamma . Assim: P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) = \\gamma Talv\u00e9m vc esteja se perguntando: porque escolher \\gamma_1, \\gamma_2 dessa forma? Bom: \\begin{split} \\gamma &= P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) \\\\ &= P\\left(U < T_{n-1}^{-1}(\\gamma_2)\\right) - P\\left(U \\leq T_{n-1}^{-1}(\\gamma_1)\\right) \\\\ &= \\gamma_2 - \\gamma_1 \\end{split} Intervalos de Confian\u00e7a Unilateral Defini\u00e7\u00e3o Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Sejam A e B duas estat\u00edsticas tais que: P(A < g(\\theta)) \\geq \\gamma P(B > g(\\theta)) \\geq \\gamma Ent\u00e3o (A, \\infty) e (-\\infty, B) s\u00e3o chamados de intervalos de confia\u00e7a unilaterais para g(\\theta) de coeficiente \\gamma ou percentil 100\\gamma . No caso de A 100\\gamma porcento abaixo e no caso de B a cima. Se vale a igualdade, dizemos que o intervalor \u00e9 exato. Intervalo unilateral para a m\u00e9dia de N(\\mu,\\sigma^2) Nas mesma condi\u00e7\u00f5es do teorema anterior, mas as estat\u00edsticas para baixo e para cima com coeficiente \\gamma para \\mu s\u00e3o: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}} Pivotal Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Seja V(\\vec{X},\\theta) uma vari\u00e1vel aleat\u00f3ria cuja distribui\u00e7\u00e3o \u00e9 a mesma para \\theta . Chamamos V de quantidade pivotal . Teorema Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Suponha que Exista V pivotal. A cdf G de V \u00e9 cont\u00ednua. Exista fun\u00e7\u00e3o r tal r(V(X,\\theta), X) = g(\\theta) , ou seja, \u00e9 uma esp\u00e9cie de \"inversa\". r(v,x) (3) \u00e9 uma fun\u00e7\u00e3o estritamente crescente em v para todo x . Ent\u00e3o A = r(G^{-1}(\\gamma_1), X) B = r(G^{-1}(\\gamma_2), X) s\u00e3o os pontos extremos do intervalo de confian\u00e7a exato para g(\\theta) de coeficiente \\gamma = \\gamma_2 - \\gamma_1 . Se r \u00e9 estritamente decrescente, invertemos A e B . Obs.: Ainda podemos usar o Teorema Central do Limite para obter intervalos de confian\u00e7a assint\u00f3ticos. Exemplo com Regress\u00e3o Linear O dataset que utilizei anteriormente n\u00e3o \u00e9 muito bom para esse exemplo, mas eu vou usar, de qualquer forma, para entendermos o processo e como pode nos ajudar o intervalo de confian\u00e7a. Em uma Regress\u00e3o Linear, queremos dizer aferir uma rela\u00e7\u00e3o linear entre duas vari\u00e1veis, isto \u00e9, queremos dizer que uma vari\u00e1vel pode ser obtida pela outra atrav\u00e9s de uma reta, mais um erro aleat\u00f3rio. Suponha que queremos estimar Y o peso da crian\u00e7a ao nascer, sabendo a informa\u00e7\u00e3o do tempo de gesta\u00e7\u00e3o X e que Y = aX + b + E, onde E \\sim N(0,\\sigma^2) . Nesse caso, estamos dizendo que Y|X \\sim N(aX + b, \\sigma^2) . Queremos estimar a e b de forma que tenhamos o melhor ajuste poss\u00edvel. Esse tema em espec\u00edfico n\u00e3o me interessa. Entretanto, podemos dizer que queremos estimar aX + b , a m\u00e9dia de uma normal, mas que muda para cada X = x observado. sns . lmplot ( x = 'gestation' , y = 'bwt' , data = birth_df , height = 5 , ci = 95 ) <seaborn.axisgrid.FacetGrid at 0x7fc517f55c18> O resultado n\u00e3o foi muito bom (na verdade eu j\u00e1 imaginava isso). Mas o interessante \u00e9 tentar refletir o que essas bandas significam? Por que os pontos n\u00e3o est\u00e3o nela? Esper\u00e1vamos que estiv\u00e9sse? E por que ela diminui a vari\u00e2ncia com o n\u00famero de pontos? Essas perguntas v\u00e3o ser devidamente respondidas no pr\u00f3ximo curso de Estat\u00edstica! Mas eu j\u00e1 vou adiantando que esse intervalo de confian\u00e7a \u00e9 para a m\u00e9dia estimada.","title":"Intervalos de Confian\u00e7a"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#intervalos-de-confianca","text":"Esse tema procura responder qu\u00e3o confian\u00e7a dever\u00edamos por em um estimador. \u00c9 claro que essa pergunta tem que ser um pouco melhor descrita matematicamente. A ideia \u00e9 frequentista e tem a ideia a seguinte forma: O intervalo [a,b] , uma realiza\u00e7\u00e3o de [A,B] , tem 95% de confian\u00e7a se em 95% do tempo, o par\u00e2metro procurado est\u00e1 entre a e b . Veja que a ideia \u00e9 basicamente frequentista, dado que a interpreta\u00e7\u00e3o est\u00e1 ligada \u00e0 frequ\u00eancia quando o n\u00famero de experimentos tende para infinito. (Cuidado: N\u00e3o vamos falar da probabilidade do par\u00e2metro estar em [a,b] , isso n\u00e3o faz sentido, pois \\theta n\u00e3o \u00e9 uma vari\u00e1vel aleat\u00f3ria, e sim um valor fixo).","title":"Intervalos de Confian\u00e7a"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#definicao","text":"Seja X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Sejam A \\leq B duas estat\u00edsticas que possuem a propriedade, para todo \\theta , P(A < g(\\theta) < B) \\geq \\gamma Chamamos (A,B) de intevalo de confian\u00e7a para g(\\theta) com coeficiente \\gamma . O intervalo \u00e9 chamado de exato se ao inv\u00e9s da desigualdade, tivermos uma igualdade. Ap\u00f3s observarmos os valores de X_1, ..., X_n e computarmos A = a e B = b , o intervalo (a,b) \u00e9 chamado de valor observado do intervalo de confian\u00e7a.","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#intervalo-de-confianca-para-a-media-de-nmu-sigma2","text":"Seja X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Para cada 0 < \\gamma < 1 , o intervalo (A,B) \u00e9 intervalo de confian\u00e7a exato para \\mu com coeficiente \\gamma , em que: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} onde T_{n-1} denota a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. O interessante \u00e9 que isso \u00e9 implica\u00e7\u00e3o direta da distribui\u00e7\u00e3o de U = \\frac{n^{1/2}(\\bar{X}_n - \\mu)}{\\sigma '} que inferimos no cap\u00edtulo 8.4, nesse caso, simplemente fizemos a transforma\u00e7\u00e3o: \\gamma = P(-c < U < c) = P(A < \\mu < B) e c \u00e9 escolhido de acordo com \\gamma .","title":"Intervalo de Confian\u00e7a para a m\u00e9dia de N(\\mu, \\sigma^2)"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#implementacao","text":"Vamos rever a informa\u00e7\u00e3o sobre caf\u00e9 que usamos cap\u00edtulos antes para ver como isso acontece na pr\u00e1tica. Considere dados sobre pesos de beb\u00eas logo ao nascer. bwt: peso do beb\u00ea ao nascer. gestation: dura\u00e7\u00e3o em dias da gesta\u00e7\u00e3o. parity: primeiro filho ou n\u00e3o. age: idade da m\u00e3e. height: altura da m\u00e3e em polegadas. weight: peso da m\u00e3e em pounds. smoke: se a m\u00e3e \u00e9 fumante ou n\u00e3o. # Importando bibliotecas import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import t birth_df = pd . read_csv ( \"http://people.reed.edu/~jones/141/Bwt.dat\" ) birth_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bwt gestation parity age height weight smoke 0 120 284 0 27 62 100 0 1 113 282 0 33 64 135 0 2 128 279 0 28 64 115 1 3 108 282 0 23 67 125 1 4 136 286 0 25 62 93 0 sns . histplot ( data = birth_df . bwt , kde = True ) plt . title ( 'Histograma dos pesos dos beb\u00eas' ) plt . show () birth_df [ birth_df . smoke == 0 ] . bwt . hist ( density = True , label = 'N\u00e3o fumante' ) birth_df [ birth_df . smoke == 1 ] . bwt . hist ( density = True , label = 'Fumante' , alpha = 0.6 ) plt . xlabel ( 'Peso' ) plt . legend () plt . show () Sabemos que essa \u00e9 uma extra\u00e7\u00e3o de uma popula\u00e7\u00e3o maior. Para conseguirmos mais amostras, vamos usar um procedimento chamado bootstrap . A ideia desse procedimento \u00e9 criar um novas amostras a partir de uma amostra inicial, usando replace = True como diferencial. Vou fazer esse procedimento diversar vezes e ir calculando a m\u00e9dia amostral. Como a m\u00e9dia amostral \u00e9 uma vari\u00e1vel aleat\u00f3ria, vamos obter um histograma das realiza\u00e7\u00f5es. Vamos supor que o peso W_i da crian\u00e7a i vem de uma distribui\u00e7\u00e3o com par\u00e2metros \\mu e \\sigma^2 desconhecidos. Nesse caso, \\bar{W}_i vir\u00e1 de uma distribui\u00e7\u00e3o normal com par\u00e2metros \\mu e \\sigma^2/n . ite = 10000 n = 200 bootstrap_means = np . zeros ( ite ) for i in range ( ite ): bootstrap_sample = birth_df . sample ( n = n , replace = True , random_state = i ) bootstrap_means [ i ] = bootstrap_sample . bwt . mean () sns . histplot ( bootstrap_means , kde = True ) plt . title ( \"M\u00e9dias das amostras\" ) plt . xlabel ( 'Peso' ) plt . show () Vamos calcular o nosso intervalo de confian\u00e7a com \\gamma = 0.95 . Temos que: gamma = 0.95 A = lambda x : np . mean ( x ) - t . ppf ( q = ( 1 + gamma ) / 2 , df = len ( x ) - 1 ) * np . std ( x , ddof = 1 ) / len ( x ) ** ( 1 / 2 ) B = lambda x : np . mean ( x ) + t . ppf ( q = ( 1 + gamma ) / 2 , df = len ( x ) - 1 ) * np . std ( x , ddof = 1 ) / len ( x ) ** ( 1 / 2 ) ite = 100 n = 500 bootstrap_intervals = np . zeros (( ite , 2 )) for i in range ( ite ): bootstrap_sample = birth_df . sample ( n = n , replace = True , random_state = i ) bootstrap_intervals [ i , 0 ] = A ( bootstrap_sample . bwt ) bootstrap_intervals [ i , 1 ] = B ( bootstrap_sample . bwt ) out_values = np . where (( bootstrap_intervals [:, 0 ] > 119.5 ) | ( bootstrap_intervals [:, 1 ] < 119.5 )) plt . figure ( figsize = ( 6 , 10 )) plt . scatter ( bootstrap_intervals [:, 0 ], np . arange ( 0 , ite ), color = 'red' , label = 'a' ) plt . scatter ( bootstrap_intervals [:, 1 ], np . arange ( 0 , ite ), color = 'green' , label = 'b' ) plt . scatter ( bootstrap_intervals [ out_values [ 0 ], 0 ], out_values [ 0 ], color = 'black' , label = 'Fora' ) plt . scatter ( bootstrap_intervals [ out_values [ 0 ], 1 ], out_values [ 0 ], color = 'black' ) plt . vlines ( 119.5 , ymin = 0 , ymax = ite , linestyle = '--' , alpha = 0.6 , label = 'M\u00e9dia real' ) plt . legend () plt . show ()","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#interpretacao","text":"Estamos fazer uma afirma\u00e7\u00e3o probabil\u00edstica sobre o intervalo (A,B) antes de observar os dados. Ap\u00f3s observarmos os dados, n\u00e3o podemos interpretar (a,b) como um intervalo em que temos 95% de confian\u00e7a de g(\\theta) estar no intervalo. Antes de observarmos as amostras, temos a confian\u00e7a de que 95% dos intervalos conter\u00e3o \\mu .","title":"Interpreta\u00e7\u00e3o"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#sem-simetria","text":"Construimos anteriormente um intervalo sim\u00e9trico, onde a estat\u00edstica U acima mencionada estaria entre -c e c com probabilidade \\gamma . Mas podemos desenvolver intervalos n\u00e3o sim\u00e9tricos tamb\u00e9m. Uma forma que podemos fazer isso \u00e9 escolhendo \\gamma_1 e \\gamma_2 , tal que \\gamma_2 - \\gamma_1 = \\gamma . Assim: P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) = \\gamma Talv\u00e9m vc esteja se perguntando: porque escolher \\gamma_1, \\gamma_2 dessa forma? Bom: \\begin{split} \\gamma &= P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) \\\\ &= P\\left(U < T_{n-1}^{-1}(\\gamma_2)\\right) - P\\left(U \\leq T_{n-1}^{-1}(\\gamma_1)\\right) \\\\ &= \\gamma_2 - \\gamma_1 \\end{split}","title":"Sem simetria"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#intervalos-de-confianca-unilateral","text":"","title":"Intervalos de Confian\u00e7a Unilateral"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#definicao_1","text":"Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Sejam A e B duas estat\u00edsticas tais que: P(A < g(\\theta)) \\geq \\gamma P(B > g(\\theta)) \\geq \\gamma Ent\u00e3o (A, \\infty) e (-\\infty, B) s\u00e3o chamados de intervalos de confia\u00e7a unilaterais para g(\\theta) de coeficiente \\gamma ou percentil 100\\gamma . No caso de A 100\\gamma porcento abaixo e no caso de B a cima. Se vale a igualdade, dizemos que o intervalor \u00e9 exato.","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#intervalo-unilateral-para-a-media-de-nmusigma2","text":"Nas mesma condi\u00e7\u00f5es do teorema anterior, mas as estat\u00edsticas para baixo e para cima com coeficiente \\gamma para \\mu s\u00e3o: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}}","title":"Intervalo unilateral para a m\u00e9dia de N(\\mu,\\sigma^2)"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#pivotal","text":"Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Seja V(\\vec{X},\\theta) uma vari\u00e1vel aleat\u00f3ria cuja distribui\u00e7\u00e3o \u00e9 a mesma para \\theta . Chamamos V de quantidade pivotal .","title":"Pivotal"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#teorema","text":"Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Suponha que Exista V pivotal. A cdf G de V \u00e9 cont\u00ednua. Exista fun\u00e7\u00e3o r tal r(V(X,\\theta), X) = g(\\theta) , ou seja, \u00e9 uma esp\u00e9cie de \"inversa\". r(v,x) (3) \u00e9 uma fun\u00e7\u00e3o estritamente crescente em v para todo x . Ent\u00e3o A = r(G^{-1}(\\gamma_1), X) B = r(G^{-1}(\\gamma_2), X) s\u00e3o os pontos extremos do intervalo de confian\u00e7a exato para g(\\theta) de coeficiente \\gamma = \\gamma_2 - \\gamma_1 . Se r \u00e9 estritamente decrescente, invertemos A e B . Obs.: Ainda podemos usar o Teorema Central do Limite para obter intervalos de confian\u00e7a assint\u00f3ticos.","title":"Teorema"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#exemplo-com-regressao-linear","text":"O dataset que utilizei anteriormente n\u00e3o \u00e9 muito bom para esse exemplo, mas eu vou usar, de qualquer forma, para entendermos o processo e como pode nos ajudar o intervalo de confian\u00e7a. Em uma Regress\u00e3o Linear, queremos dizer aferir uma rela\u00e7\u00e3o linear entre duas vari\u00e1veis, isto \u00e9, queremos dizer que uma vari\u00e1vel pode ser obtida pela outra atrav\u00e9s de uma reta, mais um erro aleat\u00f3rio. Suponha que queremos estimar Y o peso da crian\u00e7a ao nascer, sabendo a informa\u00e7\u00e3o do tempo de gesta\u00e7\u00e3o X e que Y = aX + b + E, onde E \\sim N(0,\\sigma^2) . Nesse caso, estamos dizendo que Y|X \\sim N(aX + b, \\sigma^2) . Queremos estimar a e b de forma que tenhamos o melhor ajuste poss\u00edvel. Esse tema em espec\u00edfico n\u00e3o me interessa. Entretanto, podemos dizer que queremos estimar aX + b , a m\u00e9dia de uma normal, mas que muda para cada X = x observado. sns . lmplot ( x = 'gestation' , y = 'bwt' , data = birth_df , height = 5 , ci = 95 ) <seaborn.axisgrid.FacetGrid at 0x7fc517f55c18> O resultado n\u00e3o foi muito bom (na verdade eu j\u00e1 imaginava isso). Mas o interessante \u00e9 tentar refletir o que essas bandas significam? Por que os pontos n\u00e3o est\u00e3o nela? Esper\u00e1vamos que estiv\u00e9sse? E por que ela diminui a vari\u00e2ncia com o n\u00famero de pontos? Essas perguntas v\u00e3o ser devidamente respondidas no pr\u00f3ximo curso de Estat\u00edstica! Mas eu j\u00e1 vou adiantando que esse intervalo de confian\u00e7a \u00e9 para a m\u00e9dia estimada.","title":"Exemplo com Regress\u00e3o Linear"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/","text":"Distribui\u00e7\u00f5es de Prioris Conjugadas Se a distribui\u00e7\u00e3o a priori \u00e9 membro de uma fam\u00edlia e a distribui\u00e7\u00e3o a posteriori tamb\u00e9m pertence a mesma a fam\u00edlia, essa fam\u00edlia de distribui\u00e7\u00f5es \u00e9 chamada de fam\u00edlia conjugada . A principal consequ\u00eancia de usar prioris de uma fam\u00edlia conjugada \u00e9 que as contas ficam muito mais simples. Principais Fam\u00edlias Conjugadas Teorema Suponha que X_1, ..., X_n \\overset{iid}{\\sim} \\text{Bernoulli}(\\theta) , 0 < \\theta < 1 desconhecido. Suponha que \\theta \\sim \\text{Beta}(\\alpha, \\beta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori de \\theta \u00e9 a distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n - \\sum_{i=1}^n x_i . Teorema Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Poisson}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n . Teorema Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Normal}(\\theta, \\sigma^2) , onde \\theta \u00e9 desconhecido e \\sigma \u00e9 conhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma normal com m\u00e9dia \\mu_0 e vari\u00e2ncia v_0^2 . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 normal com m\u00e9dia \\mu_1 e vari\u00e2ncia v_1^2 , onde: \\mu_1 = \\frac{\\sigma^2\\mu_0 + nv_0^2\\bar{x}_n}{\\sigma^2 + nv_0^2} v_1^2 = \\frac{\\sigma^2v_0^2}{\\sigma^2 + nv_0^2} Teorema Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + n e \\beta + \\sum_{i=1}^n x_i . import numpy as np from scipy import stats import matplotlib.pyplot as plt from matplotlib import animation , cm from IPython.display import HTML % matplotlib inline Suponha que \\theta seja a probabilidade de um item ser defeituoso em uma s\u00e9rie de items. Suponha que nossa priori em \\theta \u00e9 uma distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha e \\beta . S\u00e3o selecionados n items por vez para o teste. Sabemos que nossa posteriori ser\u00e1 uma Beta com os par\u00e2metros acima. Vejamos graficamente esse processo. theta_real = 0.1 n = 100 np . random . seed ( 10 ) XMIN = 0 XMAX = 1 YMIN = 0 YMAX = 5 alpha = [ 1 ] beta = [ 1 ] x = np . linspace ( 0.001 , 1 , 1000 ) # Definindo cores cmap = cm . autumn # Esta fun\u00e7\u00e3o permite plotar o backgroud def init (): line . set_data ([], []) return ( line ,) # Definindo o espa\u00e7o da imagem fig , ax = plt . subplots () # Definindo caracter\u00edsticas do background ax . set_xlim (( XMIN , XMAX )) ax . set_ylim (( YMIN , YMAX )) ax . set_title ( 'Evolu\u00e7\u00e3o da posteriori a cada itera\u00e7\u00e3o' ) ax . vlines ( theta_real , ymin = YMIN , ymax = YMAX , linestyle = '--' , color = 'grey' ) # Definindo plots vari\u00e1veis line , = ax . plot ([], [], lw = 2 ) line2 , _ = ax . plot ( XMIN , XMAX , YMIN , YMAX , linestyle = ':' ) def animate ( i , alpha , beta , x , n ): # Amostro da distribui\u00e7\u00e3o sample = np . random . binomial ( 1 , p = theta_real ) # Junto a lista que guarda os alphas e betas de cada itera\u00e7\u00e3o alpha . append ( alpha [ - 1 ] + sample ) beta . append ( beta [ - 1 ] + 1 - sample ) # Calculo a posteriori posteriori = stats . beta ( a = alpha [ - 1 ], b = beta [ - 1 ]) line . set_data ( x , posteriori . pdf ( x )) line . set_color ( cmap ( 1 - i / n )) line2 . set_data ([ posteriori . mean (), posteriori . mean ()], [ YMIN , YMAX ]) line2 . set_color ( cmap ( 1 - i / n )) return ( line , line2 ) anim = animation . FuncAnimation ( fig , animate , frames = n , init_func = init , interval = 100 , blit = True , fargs = ( alpha , beta , x , n ), repeat = False ) HTML ( anim . to_html5_video ()) Your browser does not support the video tag. Hiperpar\u00e2metros Seja \\Psi uma fam\u00edlia de distribui\u00e7\u00f5es poss\u00edveis sobre um espa\u00e7o de par\u00eamtros \\Omega . Suponha que independente da distribui\u00e7\u00e3o a priori dessa fam\u00edlia e n\u00e3o importando as observa\u00e7\u00f5es (quais s\u00e3o ou quantas s\u00e3o), a distribui\u00e7\u00e3o a posteriori seja da mesma fam\u00edlia. Chamamos \\Psi de fam\u00edlia conjunda de distribui\u00e7\u00f5es a priori. Os par\u00e2metros associados a essa fam\u00edlia s\u00e3o chamados de hiperpar\u00e2metros. Distribui\u00e7\u00f5es a Priori Impr\u00f3prias Seja \\xi uma fun\u00e7\u00e3o n\u00e3o negativa tal que \\Omega \u00e9 subconjunto de seu dom\u00ednio. Suponha que \\int \\xi(\\theta) d\\theta = \\infty . Se \\xi(\\theta) \u00e9 priori de \\theta , ela \u00e9 chamada de prioori impr\u00f3pria. Podemos gerar limites de distribui\u00e7\u00f5es, como, por exemplo, a distribui\u00e7\u00e3o uniforme [0,1] com intervalo sendo a reta, agora. Estimador de Bayes Estimador Seja X_1, ..., X_n dados observador cuja distribui\u00e7\u00e3o conjunta \u00e9 inndexada pelo par\u00e2metro \\theta . Um estimador do par\u00e2metro \\theta \u00e9 uma fun\u00e7\u00e3o real X_1, ..., X_n \\mapsto \\delta(X_1, ..., X_n) . Se X_i = x_i \u00e9 observado, \\delta(x_1,...,x_n) \u00e9 uma estimativa. Fun\u00e7\u00e3o de Perda \u00c9 uma fun\u00e7\u00e3o real L(\\theta, a) onde \\theta \\in \\Omega e a \\in \\mathbb{R} . Essa fun\u00e7\u00e3o procura indicar, para cada escoolha de \\theta , a perda do estat\u00edstico. Seja \\xi(\\theta) priori de \\theta . O valor esperado da perda \u00e9 dado por: E[L(\\theta, a)] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta) d\\theta, \\text{ a priori} E[L(\\theta, a)|x] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta|x) d\\theta, \\text{ a posteriori} Estimador de Bayes Seja L(\\theta, a) fun\u00e7\u00e3o de perda. Seja \\delta^*(x) o valor de a tal que E[L(\\theta, a)|x] \u00e9 minimizado. Ent\u00e3o \\delta^* \u00e9 o estimador de Bayes de \\theta . E[L(\\theta, \\delta^*(x))|x] = \\min_{a \\in \\mathbb{R}}E[L(\\theta, a)|x] Fun\u00e7\u00f5es de Perda: Exemplos Erro quadr\u00e1tico: L(\\theta, a) = (\\theta - a)^2 Queremos minimizar E[(\\theta - a)^2|x] \\delta^*(X) = E(\\theta| X) P\u00e1gina 260 (DeGroot) Erro absoluto: L(\\theta, a) = |\\theta - a| Queremos minimizar E[|\\theta - a||x] \\delta^*(X) = \\text{mediana (quartil 0.5)} P\u00e1gina 245 (DeGroot) Estimador Consistente Uma sequ\u00eancia de estimadores que converge em probabilidade para um valor desconhecido de um par\u00e2metro a ser estimafo \u00e9 chamado de sequ\u00eancia consistente de estimadores. Essa consist\u00eancia fala que em grandes amostras, o estimador estar\u00e1 pr\u00f3ximo o suficiente do valor desconhecido de \\theta . O estimador de Bayes, sob algumas condi\u00e7\u00f5es, forma uma sequ\u00eancia de estimadores consistentes. Limita\u00e7\u00f5es De acordo com a teoria Bayesiana , esse estimador \u00e9 o \u00fanico coerente que pode ser constru\u00eddo. \u00c9 importante que tenha-se definido uma fun\u00e7\u00e3o de perda e uma distribui\u00e7\u00e3o a priori para os par\u00e2metros. Quando \\theta \u00e9 um vetor, precisamos definit uma priori multivariada, mesmo que n\u00e3o queiramos estimar todos os par\u00e2metros. Exemplo 7.4.7 Quetelet reportou medidas do peito de 5732 homens militares. Os dados foram retirados desse site . import requests from bs4 import BeautifulSoup import pandas as pd Obtendo os dados direto do site Eu uso essas tr\u00eas bibliotecas, onde as duas primeiras s\u00e3o usadas para retirar informa\u00e7\u00e3o do site desejado. Veja que n\u00e3o coloco verifica\u00e7\u00e3o, pois o site tem esse problema. Depois eu coloco numa estrutura chamada DataFrame que \u00e9 basicamente uma tabela onde tem cada item nas linhas e cada caracter\u00edstica nas colunas. website = requests . get ( 'https://www.stat.cmu.edu/StatDat/Datafiles/MilitiamenChests.html' , verify = False ) soup = BeautifulSoup ( website . content ) data = soup . pre . text . strip () . split ( ' \\n ' ) chest_data = { 'Chest' : [], 'Count' : []} for item in data [ 1 :]: co , ch = item . split ( ' \\t ' ) chest_data [ 'Chest' ] . append ( int ( ch )) chest_data [ 'Count' ] . append ( int ( co )) chest_df = pd . DataFrame ( chest_data ) chest_df . head () /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Chest Count 0 33 3 1 34 18 2 35 81 3 36 185 4 37 420 plt . bar ( chest_df [ 'Chest' ], chest_df [ 'Count' ]) mean = sum ( chest_df [ 'Chest' ] * chest_df [ 'Count' ]) / chest_df [ 'Count' ] . sum () plt . vlines ( mean , ymin = 0 , ymax = 1200 , color = 'black' , linestyle = '--' , label = 'M\u00e9dia={:.2f}' . format ( mean )) plt . title ( 'Histograma do tamanho do ppeito de militares escoseses' ) plt . xlabel ( 'Medida de peitorais' ) plt . legend () plt . show () Vamos modelar as medidas do peitoral, como uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal com m\u00e9dia \\theta e vari\u00e2ncia \\sigma^2 , conhecido. Temos que a m\u00e9dia \u00e9 39.83 das amostras. Se \\theta \\sim N(\\mu_0, v_0^2) \u00e9 uma priori para \\theta , podemos calcular o estimador de Bayes a posteriori. Sabemos que a posteriori ser\u00e1 uma normal (conjugada) com m\u00e9dia e vari\u00e2ncia: \\mu_1 = \\frac{\\sigma^2 + 5732\\cdot v_0^2 \\cdot 39.83}{\\sigma^2 + 5732\\cdot v_0^2} v_1^2 = \\frac{\\sigma^2 v_0^2}{\\sigma^2 + 5732\\cdot v_0^2} O estimador de Bayes, segundo a perda quadr\u00e1tica, \u00e9 a m\u00e9dia a posteriori, portanto \\delta(x) = \\mu_1 Priori para \\theta \\mu_0 = 39.83 v_0^2 = 4","title":"Distribui\u00e7\u00f5es Conjugada e Estimador de Bayes"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#distribuicoes-de-prioris-conjugadas","text":"Se a distribui\u00e7\u00e3o a priori \u00e9 membro de uma fam\u00edlia e a distribui\u00e7\u00e3o a posteriori tamb\u00e9m pertence a mesma a fam\u00edlia, essa fam\u00edlia de distribui\u00e7\u00f5es \u00e9 chamada de fam\u00edlia conjugada . A principal consequ\u00eancia de usar prioris de uma fam\u00edlia conjugada \u00e9 que as contas ficam muito mais simples.","title":"Distribui\u00e7\u00f5es de Prioris Conjugadas"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#principais-familias-conjugadas","text":"","title":"Principais Fam\u00edlias Conjugadas"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#teorema","text":"Suponha que X_1, ..., X_n \\overset{iid}{\\sim} \\text{Bernoulli}(\\theta) , 0 < \\theta < 1 desconhecido. Suponha que \\theta \\sim \\text{Beta}(\\alpha, \\beta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori de \\theta \u00e9 a distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n - \\sum_{i=1}^n x_i .","title":"Teorema"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#teorema_1","text":"Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Poisson}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n .","title":"Teorema"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#teorema_2","text":"Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Normal}(\\theta, \\sigma^2) , onde \\theta \u00e9 desconhecido e \\sigma \u00e9 conhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma normal com m\u00e9dia \\mu_0 e vari\u00e2ncia v_0^2 . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 normal com m\u00e9dia \\mu_1 e vari\u00e2ncia v_1^2 , onde: \\mu_1 = \\frac{\\sigma^2\\mu_0 + nv_0^2\\bar{x}_n}{\\sigma^2 + nv_0^2} v_1^2 = \\frac{\\sigma^2v_0^2}{\\sigma^2 + nv_0^2}","title":"Teorema"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#teorema_3","text":"Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + n e \\beta + \\sum_{i=1}^n x_i . import numpy as np from scipy import stats import matplotlib.pyplot as plt from matplotlib import animation , cm from IPython.display import HTML % matplotlib inline Suponha que \\theta seja a probabilidade de um item ser defeituoso em uma s\u00e9rie de items. Suponha que nossa priori em \\theta \u00e9 uma distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha e \\beta . S\u00e3o selecionados n items por vez para o teste. Sabemos que nossa posteriori ser\u00e1 uma Beta com os par\u00e2metros acima. Vejamos graficamente esse processo. theta_real = 0.1 n = 100 np . random . seed ( 10 ) XMIN = 0 XMAX = 1 YMIN = 0 YMAX = 5 alpha = [ 1 ] beta = [ 1 ] x = np . linspace ( 0.001 , 1 , 1000 ) # Definindo cores cmap = cm . autumn # Esta fun\u00e7\u00e3o permite plotar o backgroud def init (): line . set_data ([], []) return ( line ,) # Definindo o espa\u00e7o da imagem fig , ax = plt . subplots () # Definindo caracter\u00edsticas do background ax . set_xlim (( XMIN , XMAX )) ax . set_ylim (( YMIN , YMAX )) ax . set_title ( 'Evolu\u00e7\u00e3o da posteriori a cada itera\u00e7\u00e3o' ) ax . vlines ( theta_real , ymin = YMIN , ymax = YMAX , linestyle = '--' , color = 'grey' ) # Definindo plots vari\u00e1veis line , = ax . plot ([], [], lw = 2 ) line2 , _ = ax . plot ( XMIN , XMAX , YMIN , YMAX , linestyle = ':' ) def animate ( i , alpha , beta , x , n ): # Amostro da distribui\u00e7\u00e3o sample = np . random . binomial ( 1 , p = theta_real ) # Junto a lista que guarda os alphas e betas de cada itera\u00e7\u00e3o alpha . append ( alpha [ - 1 ] + sample ) beta . append ( beta [ - 1 ] + 1 - sample ) # Calculo a posteriori posteriori = stats . beta ( a = alpha [ - 1 ], b = beta [ - 1 ]) line . set_data ( x , posteriori . pdf ( x )) line . set_color ( cmap ( 1 - i / n )) line2 . set_data ([ posteriori . mean (), posteriori . mean ()], [ YMIN , YMAX ]) line2 . set_color ( cmap ( 1 - i / n )) return ( line , line2 ) anim = animation . FuncAnimation ( fig , animate , frames = n , init_func = init , interval = 100 , blit = True , fargs = ( alpha , beta , x , n ), repeat = False ) HTML ( anim . to_html5_video ()) Your browser does not support the video tag.","title":"Teorema"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#hiperparametros","text":"Seja \\Psi uma fam\u00edlia de distribui\u00e7\u00f5es poss\u00edveis sobre um espa\u00e7o de par\u00eamtros \\Omega . Suponha que independente da distribui\u00e7\u00e3o a priori dessa fam\u00edlia e n\u00e3o importando as observa\u00e7\u00f5es (quais s\u00e3o ou quantas s\u00e3o), a distribui\u00e7\u00e3o a posteriori seja da mesma fam\u00edlia. Chamamos \\Psi de fam\u00edlia conjunda de distribui\u00e7\u00f5es a priori. Os par\u00e2metros associados a essa fam\u00edlia s\u00e3o chamados de hiperpar\u00e2metros.","title":"Hiperpar\u00e2metros"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#distribuicoes-a-priori-improprias","text":"Seja \\xi uma fun\u00e7\u00e3o n\u00e3o negativa tal que \\Omega \u00e9 subconjunto de seu dom\u00ednio. Suponha que \\int \\xi(\\theta) d\\theta = \\infty . Se \\xi(\\theta) \u00e9 priori de \\theta , ela \u00e9 chamada de prioori impr\u00f3pria. Podemos gerar limites de distribui\u00e7\u00f5es, como, por exemplo, a distribui\u00e7\u00e3o uniforme [0,1] com intervalo sendo a reta, agora.","title":"Distribui\u00e7\u00f5es a Priori Impr\u00f3prias"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#estimador-de-bayes","text":"","title":"Estimador de Bayes"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#estimador","text":"Seja X_1, ..., X_n dados observador cuja distribui\u00e7\u00e3o conjunta \u00e9 inndexada pelo par\u00e2metro \\theta . Um estimador do par\u00e2metro \\theta \u00e9 uma fun\u00e7\u00e3o real X_1, ..., X_n \\mapsto \\delta(X_1, ..., X_n) . Se X_i = x_i \u00e9 observado, \\delta(x_1,...,x_n) \u00e9 uma estimativa.","title":"Estimador"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#funcao-de-perda","text":"\u00c9 uma fun\u00e7\u00e3o real L(\\theta, a) onde \\theta \\in \\Omega e a \\in \\mathbb{R} . Essa fun\u00e7\u00e3o procura indicar, para cada escoolha de \\theta , a perda do estat\u00edstico. Seja \\xi(\\theta) priori de \\theta . O valor esperado da perda \u00e9 dado por: E[L(\\theta, a)] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta) d\\theta, \\text{ a priori} E[L(\\theta, a)|x] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta|x) d\\theta, \\text{ a posteriori}","title":"Fun\u00e7\u00e3o de Perda"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#estimador-de-bayes_1","text":"Seja L(\\theta, a) fun\u00e7\u00e3o de perda. Seja \\delta^*(x) o valor de a tal que E[L(\\theta, a)|x] \u00e9 minimizado. Ent\u00e3o \\delta^* \u00e9 o estimador de Bayes de \\theta . E[L(\\theta, \\delta^*(x))|x] = \\min_{a \\in \\mathbb{R}}E[L(\\theta, a)|x]","title":"Estimador de Bayes"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#funcoes-de-perda-exemplos","text":"Erro quadr\u00e1tico: L(\\theta, a) = (\\theta - a)^2 Queremos minimizar E[(\\theta - a)^2|x] \\delta^*(X) = E(\\theta| X) P\u00e1gina 260 (DeGroot) Erro absoluto: L(\\theta, a) = |\\theta - a| Queremos minimizar E[|\\theta - a||x] \\delta^*(X) = \\text{mediana (quartil 0.5)} P\u00e1gina 245 (DeGroot)","title":"Fun\u00e7\u00f5es de Perda: Exemplos"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#estimador-consistente","text":"Uma sequ\u00eancia de estimadores que converge em probabilidade para um valor desconhecido de um par\u00e2metro a ser estimafo \u00e9 chamado de sequ\u00eancia consistente de estimadores. Essa consist\u00eancia fala que em grandes amostras, o estimador estar\u00e1 pr\u00f3ximo o suficiente do valor desconhecido de \\theta . O estimador de Bayes, sob algumas condi\u00e7\u00f5es, forma uma sequ\u00eancia de estimadores consistentes.","title":"Estimador Consistente"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#limitacoes","text":"De acordo com a teoria Bayesiana , esse estimador \u00e9 o \u00fanico coerente que pode ser constru\u00eddo. \u00c9 importante que tenha-se definido uma fun\u00e7\u00e3o de perda e uma distribui\u00e7\u00e3o a priori para os par\u00e2metros. Quando \\theta \u00e9 um vetor, precisamos definit uma priori multivariada, mesmo que n\u00e3o queiramos estimar todos os par\u00e2metros.","title":"Limita\u00e7\u00f5es"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#exemplo-747","text":"Quetelet reportou medidas do peito de 5732 homens militares. Os dados foram retirados desse site . import requests from bs4 import BeautifulSoup import pandas as pd","title":"Exemplo 7.4.7"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#obtendo-os-dados-direto-do-site","text":"Eu uso essas tr\u00eas bibliotecas, onde as duas primeiras s\u00e3o usadas para retirar informa\u00e7\u00e3o do site desejado. Veja que n\u00e3o coloco verifica\u00e7\u00e3o, pois o site tem esse problema. Depois eu coloco numa estrutura chamada DataFrame que \u00e9 basicamente uma tabela onde tem cada item nas linhas e cada caracter\u00edstica nas colunas. website = requests . get ( 'https://www.stat.cmu.edu/StatDat/Datafiles/MilitiamenChests.html' , verify = False ) soup = BeautifulSoup ( website . content ) data = soup . pre . text . strip () . split ( ' \\n ' ) chest_data = { 'Chest' : [], 'Count' : []} for item in data [ 1 :]: co , ch = item . split ( ' \\t ' ) chest_data [ 'Chest' ] . append ( int ( ch )) chest_data [ 'Count' ] . append ( int ( co )) chest_df = pd . DataFrame ( chest_data ) chest_df . head () /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Chest Count 0 33 3 1 34 18 2 35 81 3 36 185 4 37 420 plt . bar ( chest_df [ 'Chest' ], chest_df [ 'Count' ]) mean = sum ( chest_df [ 'Chest' ] * chest_df [ 'Count' ]) / chest_df [ 'Count' ] . sum () plt . vlines ( mean , ymin = 0 , ymax = 1200 , color = 'black' , linestyle = '--' , label = 'M\u00e9dia={:.2f}' . format ( mean )) plt . title ( 'Histograma do tamanho do ppeito de militares escoseses' ) plt . xlabel ( 'Medida de peitorais' ) plt . legend () plt . show () Vamos modelar as medidas do peitoral, como uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal com m\u00e9dia \\theta e vari\u00e2ncia \\sigma^2 , conhecido. Temos que a m\u00e9dia \u00e9 39.83 das amostras. Se \\theta \\sim N(\\mu_0, v_0^2) \u00e9 uma priori para \\theta , podemos calcular o estimador de Bayes a posteriori. Sabemos que a posteriori ser\u00e1 uma normal (conjugada) com m\u00e9dia e vari\u00e2ncia: \\mu_1 = \\frac{\\sigma^2 + 5732\\cdot v_0^2 \\cdot 39.83}{\\sigma^2 + 5732\\cdot v_0^2} v_1^2 = \\frac{\\sigma^2 v_0^2}{\\sigma^2 + 5732\\cdot v_0^2} O estimador de Bayes, segundo a perda quadr\u00e1tica, \u00e9 a m\u00e9dia a posteriori, portanto \\delta(x) = \\mu_1","title":"Obtendo os dados direto do site"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#priori-para-theta","text":"\\mu_0 = 39.83 v_0^2 = 4","title":"Priori para \\theta"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/","text":"Simples Exemplos em Teste de Hip\u00f3teses Refer\u00eancia: Dr. Tirthajyoti Sarkar , Fremont, 94536 Vamos lembrar que nosso objetivo \u00e9 testar hip\u00f3teses cient\u00edficas com uma formula\u00e7\u00e3o estat\u00edstica, a fim de fazer alguma infer\u00eancia sobre os par\u00e2metros de algum modelo. Os testes que ser\u00e3o discutidos nesse notebook s\u00e3o: Propor\u00e7\u00e3o de uma popula\u00e7\u00e3o. Deferen\u00e7a entre propor\u00e7\u00e3o de popula\u00e7\u00f5es M\u00e9dia de uma popula\u00e7\u00e3o. Diferen\u00e7a enre m\u00e9dias de popula\u00e7\u00f5es. import statsmodels.api as sm import numpy as np import matplotlib.pyplot as plt import pandas as pd Propor\u00e7\u00e3o de uma Popula\u00e7\u00e3o Quest\u00e3o : Em anos anteriores, 52% dos pais acreditavam que a falta de sono era causada por eletr\u00f4nicos e as m\u00e9dias sociais em seus filhos e filhas adolescentes. E agora, como essa propor\u00e7\u00e3o se encontra? Popula\u00e7\u00e3o : Pais com filhos e filhas adolescentes de 13 a 18 anos. Par\u00e2metro de interesse: p H_0: p \\le 0.52 e H_1: p > 0.52 . Dados: Pesquisa entre 1018 pessoas: 56% acreditam agora. Teste Z para propor\u00e7\u00f5es Vamos usar um teste chamado teste Z, considerando que $X_1 , ..., X_n \\sim Bernoulli(p) Z = \\frac{\\bar{X}_n - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} Onde \\pi_0 \u00e9 o valor limiar, no caso 0.52. Observe que quando p = \\pi_0 , a estat\u00edstica Z \u00e9 aproximadamente normal padr\u00e3o quando n cresce. O que esse teste mensura? No denominador, temos o desvio padr\u00e3o de quando p = \\pi_0 . Portanto, estamos medindo a dist\u00e2ncia entre a m\u00e9dia amostral e o limiar em unidades de desvio padr\u00e3o. Nosso procedimento de teste ser\u00e1, portanto, se Z \\ge c , rejeitamos H_0 , mas com n\u00edvel de signific\u00e2ncia \\alpha_0 , isto \u00e9: P(Z \\ge c|p \\le \\pi_0) \\le \\alpha_0 Podemos conferir que \\begin{split} P(Z \\ge c) &= P\\left(\\frac{\\bar{X}_n - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} \\ge c\\right) \\\\ &= P\\left(\\frac{\\bar{X}_n - p}{\\sqrt{(p(1 - p)/n)}}\\frac{\\sqrt{(p(1 - p)/n)}}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} + \\frac{p - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} \\ge c\\right) \\\\ &= P\\left(\\frac{\\bar{X}_n - p}{\\sqrt{(p(1 - p)/n)}} \\ge c\\frac{\\sqrt{\\pi_0(1 - \\pi_0)}}{\\sqrt{p(1 - p)}} - \\frac{p - \\pi_0}{\\sqrt{(p(1 - p)/n)}}\\right) \\\\ &\\approx 1 - \\Phi\\left(c\\frac{\\sqrt{\\pi_0(1 - \\pi_0)}}{\\sqrt{p(1 - p)}} - \\frac{p - \\pi_0}{\\sqrt{(p(1 - p)/n)}}\\right), \\text{ pelo Teo. Central do Limite} \\end{split} Assim podemos encontrar \\alpha_0 a partir do m\u00e1ximo que essa quantidade assume (Mas com uma obseeva\u00e7\u00e3o bem detalhada, quando n \u00e9 grande, p = 0.52 \u00e9 o maximizador) Mas como isso funciona na pr\u00e1tica, propriamente dito. Tenho que escolher um valor limiar para o p-valor, isto \u00e9, se p-valor for menor do que esse limiar, eu rejeito a hip\u00f3tese. Vou ficar esse limiar em 0.05, mas isso \u00e9 arbitr\u00e1rio, apesar da literatura costumar us\u00e1-lo. n = 1018 pnull = . 52 phat = . 56 ztest , pvalue = sm . stats . proportions_ztest ( count = phat * n , #n\u00famero de sucessos nobs = n , #n\u00famero de observa\u00e7\u00f5es value = pnull , #pi_0 alternative = 'larger' ) #hip\u00f3tese alternativa print ( 'O valor da estat\u00edstica de teste foi {} e o p-valor {}' . format ( ztest , pvalue )) O valor da estat\u00edstica de teste foi 0.6392739759907055 e o p-valor 0.26132235751888716 Conclus\u00e3o do teste de hip\u00f3teses Como o p-valor foi menor do que nosso limiar, n\u00f3s temos evid\u00eancia para rejeitar a hip\u00f3tese nula que dizia que a prpor\u00e7\u00e3o teria permanecido ou at\u00e9 diminu\u00eddo. Isso n\u00e3o significa que aceitamos a hip\u00f3tese alternativa, apenas que temos evid\u00eancia para acreditar que a propor\u00e7\u00e3o seja maior do que 0.52. n = 1018 pnull = . 52 phat = . 53 ztest , pvalue = sm . stats . proportions_ztest ( count = phat * n , #n\u00famero de sucessos nobs = n , #n\u00famero de observa\u00e7\u00f5es value = pnull , #pi_0 alternative = 'larger' ) #hip\u00f3tese alternativa print ( 'O valor da estat\u00edstica de teste foi {} e o p-valor {}' . format ( ztest , pvalue )) O valor da estat\u00edstica de teste foi 0.6392739759907055 e o p-valor 0.26132235751888716 Diferen\u00e7a entre propo\u00e7\u00f5es de duas popula\u00e7\u00f5es Quest\u00e3o : Existe diferen\u00e7a significativa entre pais ingleses e pais alem\u00e3es que reportaram que seus filhos e filhas tiveram aulas de nata\u00e7\u00e3o? Popula\u00e7\u00e3o : Pais com filhos e filhas ingleses e alem\u00e3es. Par\u00e2metro de interesse: p_{ingleses} = p_i e p_{alemaes} = p_a . H_0: p_i - p_a = 0 e H_1: p_i - p_a \\neq 0 . Dados: 247 pais ingleses responderam e dentre eles 36.8% reportaram que sim. Pais alem\u00e3es foram 308, 38.9% que disseram sim. Teste T para propor\u00e7\u00f5es Na verdade, poder\u00edamos usar o teste Z, com a mesma ideia, s\u00f3 que nesse caso, ter\u00edamos que tomar um pouco de cuidado com o denominador, dado que agora existem duas m\u00e9dias, ent\u00e3o nosso estimador para o desvio padr\u00e3o deve levar em conta esses dois fatores e n deve ser suficientemente grande para que n\u00e3o tenhamos problema. Para evitar isso, vamos usar o T teste. A estat\u00edstica de teste \u00e9 a seguinte, tratando como X e Y as duas amostras consideradas. t = \\frac{\\bar{X}_n - \\bar{Y}_m}{SE} Nesse caso SE \u00e9 o erro entre a diferen\u00e7a entre a m\u00e9dias: isso tem um pequeno problema quando m \\neq n . Ent\u00e3o fazer as contas no papel n\u00e3o \u00e9 trivial. Confira aqui as defini\u00e7\u00f5es de SE precisas. Como nossa inten\u00e7\u00e3o \u00e9 apenas usar esse teste, vamos mostrar como isso pode ser pr\u00e1tico. n = 247 pi = . 368 m = 308 pa = . 389 # Gerando as popula\u00e7\u00f5es england = np . random . binomial ( n = 1 , p = pi , size = n ) germany = np . random . binomial ( n = 1 , p = pa , size = m ) _ , p_value , _ = sm . stats . ttest_ind ( england , germany ) print ( 'O p-valor foi {}' . format ( p_value )) O p-valor foi 0.2615435082780627 Conclus\u00e3o sobre o teste de hip\u00f3teses Dado que o p-valor \u00e9 maior do que nosso limiar, n\u00e3o podemos rejeitar a hip\u00f3tese nula. Nesse caso, a diferen\u00e7a das propor\u00e7\u00f5es nas popu\u00e7a\u00e7\u00f5es n\u00e3o foi nada mais do que meramente uma aleatoriedade. Mas o que acontece se essas propor\u00e7\u00f5es se mantiveram para mais pessoas? n = 5000 pi = . 37 m = 5000 pa = . 389 england = np . random . binomial ( n = 1 , p = pi , size = n ) germany = np . random . binomial ( n = 1 , p = pa , size = m ) _ , p_value , _ = sm . stats . ttest_ind ( england , germany ) print ( 'O p-valor foi {}' . format ( p_value )) O p-valor foi 0.027721099791980015 Diferen\u00e7a entre m\u00e9dias de popula\u00e7\u00f5es Quest\u00e3o : Considerando os adultos nos dados da NHAMES , homens tem maior m\u00e9dia de \u00cdndice de Massa Corp\u00f3rea do que mulheres? Popula\u00e7\u00e3o : Adultos na base NHAMES. Par\u00e2metro de interesse: \\mu_{homens} = \\mu_h e \\mu_{mulheres} = \\mu_m . H_0: \\mu_1 = \\mu_2 e H_1: \\mu_1 \\neq \\mu_2 . Dados: 2976 mulheres adultas \\hat{\\mu}_m = 29.94 \\hat{\\sigma}_m = 7.75 2759 homens adultos \\hat{\\mu}_h = 28.78 \\hat{\\sigma}_h = 6.25 url = \"https://raw.githubusercontent.com/kshedden/statswpy/master/NHANES/merged/nhanes_2015_2016.csv\" da = pd . read_csv ( url ) da . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SEQN ALQ101 ALQ110 ALQ130 SMQ020 RIAGENDR RIDAGEYR RIDRETH1 DMDCITZN DMDEDUC2 ... BPXSY2 BPXDI2 BMXWT BMXHT BMXBMI BMXLEG BMXARML BMXARMC BMXWAIST HIQ210 0 83732 1.0 NaN 1.0 1 1 62 3 1.0 5.0 ... 124.0 64.0 94.8 184.5 27.8 43.3 43.6 35.9 101.1 2.0 1 83733 1.0 NaN 6.0 1 1 53 3 2.0 3.0 ... 140.0 88.0 90.4 171.4 30.8 38.0 40.0 33.2 107.9 NaN 2 83734 1.0 NaN NaN 1 1 78 3 1.0 3.0 ... 132.0 44.0 83.4 170.1 28.8 35.6 37.0 31.0 116.5 2.0 3 83735 2.0 1.0 1.0 2 2 56 3 1.0 5.0 ... 134.0 68.0 109.8 160.9 42.4 38.5 37.7 38.3 110.1 2.0 4 83736 2.0 1.0 1.0 2 2 42 4 1.0 4.0 ... 114.0 54.0 55.2 164.9 20.3 37.4 36.0 27.2 80.4 2.0 5 rows \u00d7 28 columns females = da [ da [ \"RIAGENDR\" ] == 2 ] male = da [ da [ \"RIAGENDR\" ] == 1 ] n_m = len ( females ) mu_m = females [ \"BMXBMI\" ] . mean () sd_m = females [ \"BMXBMI\" ] . std () ( n_m , mu_m , sd_m ) (2976, 29.939945652173996, 7.75331880954568) n_h = len ( male ) mu_h = male [ \"BMXBMI\" ] . mean () sd_h = male [ \"BMXBMI\" ] . std () ( n_h , mu_h , sd_h ) (2759, 28.778072111846985, 6.252567616801485) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . set_title ( \"Histogram BMI para mulheres\" , fontsize = 16 ) ax [ 0 ] . hist ( females [ \"BMXBMI\" ] . dropna (), edgecolor = 'k' , color = 'darkred' , bins = 25 ) ax [ 1 ] . set_title ( \"Histogram BMI para homens\" , fontsize = 16 ) ax [ 1 ] . hist ( male [ \"BMXBMI\" ] . dropna (), edgecolor = 'k' , color = 'green' , bins = 25 ) plt . show () Vamos usar o t-test descrito em Welch . Temos que us\u00e1-lo porque n\u00e3o conhecemos a vari\u00e2ncia. Se conhecessemos, poder\u00edamos usar o teste normal mesmo e se soub\u00e9ssemos que s\u00e3o iguais, mas desconhecessemos, poder\u00edamos usar o ttest ques estudamos no cap\u00edtulo 9.5 . sm . stats . ttest_ind ( x1 = females [ \"BMXBMI\" ] . dropna (), x2 = male [ \"BMXBMI\" ] . dropna (), alternative = 'two-sided' , value = 0 ) # diferen\u00e7a na hip\u00f3tese nula (6.175593353138302, 7.050275578095374e-10, 5660.0) Conclus\u00e3o no teste de hip\u00f3teses Como o p-valor \u00e9 bem pequeno, n\u00f3s podemos rejeitar a hip\u00f3tese nula, o que significa que existe diferen\u00e7a estat\u00edstica entre as m\u00e9dias. Isso n\u00e3o responde se a o \u00edndice \u00e9 mais alto para homens, mas podemos fazer o teste unilateral e perceber que de fato isso de fato acontece segundo os dados. sm . stats . ttest_ind ( x1 = females [ \"BMXBMI\" ] . dropna (), x2 = male [ \"BMXBMI\" ] . dropna (), alternative = 'larger' , value = 0 ) # diferen\u00e7a na hip\u00f3tese nula (6.175593353138302, 3.525137789047687e-10, 5660.0)","title":"Exemplos B\u00e1sicos"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#simples-exemplos-em-teste-de-hipoteses","text":"","title":"Simples Exemplos em Teste de Hip\u00f3teses"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#referencia-dr-tirthajyoti-sarkar-fremont-94536","text":"Vamos lembrar que nosso objetivo \u00e9 testar hip\u00f3teses cient\u00edficas com uma formula\u00e7\u00e3o estat\u00edstica, a fim de fazer alguma infer\u00eancia sobre os par\u00e2metros de algum modelo. Os testes que ser\u00e3o discutidos nesse notebook s\u00e3o: Propor\u00e7\u00e3o de uma popula\u00e7\u00e3o. Deferen\u00e7a entre propor\u00e7\u00e3o de popula\u00e7\u00f5es M\u00e9dia de uma popula\u00e7\u00e3o. Diferen\u00e7a enre m\u00e9dias de popula\u00e7\u00f5es. import statsmodels.api as sm import numpy as np import matplotlib.pyplot as plt import pandas as pd","title":"Refer\u00eancia:  Dr. Tirthajyoti Sarkar, Fremont, 94536"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#proporcao-de-uma-populacao","text":"Quest\u00e3o : Em anos anteriores, 52% dos pais acreditavam que a falta de sono era causada por eletr\u00f4nicos e as m\u00e9dias sociais em seus filhos e filhas adolescentes. E agora, como essa propor\u00e7\u00e3o se encontra? Popula\u00e7\u00e3o : Pais com filhos e filhas adolescentes de 13 a 18 anos. Par\u00e2metro de interesse: p H_0: p \\le 0.52 e H_1: p > 0.52 . Dados: Pesquisa entre 1018 pessoas: 56% acreditam agora.","title":"Propor\u00e7\u00e3o de uma Popula\u00e7\u00e3o"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#teste-z-para-proporcoes","text":"Vamos usar um teste chamado teste Z, considerando que $X_1 , ..., X_n \\sim Bernoulli(p) Z = \\frac{\\bar{X}_n - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} Onde \\pi_0 \u00e9 o valor limiar, no caso 0.52. Observe que quando p = \\pi_0 , a estat\u00edstica Z \u00e9 aproximadamente normal padr\u00e3o quando n cresce. O que esse teste mensura? No denominador, temos o desvio padr\u00e3o de quando p = \\pi_0 . Portanto, estamos medindo a dist\u00e2ncia entre a m\u00e9dia amostral e o limiar em unidades de desvio padr\u00e3o. Nosso procedimento de teste ser\u00e1, portanto, se Z \\ge c , rejeitamos H_0 , mas com n\u00edvel de signific\u00e2ncia \\alpha_0 , isto \u00e9: P(Z \\ge c|p \\le \\pi_0) \\le \\alpha_0 Podemos conferir que \\begin{split} P(Z \\ge c) &= P\\left(\\frac{\\bar{X}_n - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} \\ge c\\right) \\\\ &= P\\left(\\frac{\\bar{X}_n - p}{\\sqrt{(p(1 - p)/n)}}\\frac{\\sqrt{(p(1 - p)/n)}}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} + \\frac{p - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} \\ge c\\right) \\\\ &= P\\left(\\frac{\\bar{X}_n - p}{\\sqrt{(p(1 - p)/n)}} \\ge c\\frac{\\sqrt{\\pi_0(1 - \\pi_0)}}{\\sqrt{p(1 - p)}} - \\frac{p - \\pi_0}{\\sqrt{(p(1 - p)/n)}}\\right) \\\\ &\\approx 1 - \\Phi\\left(c\\frac{\\sqrt{\\pi_0(1 - \\pi_0)}}{\\sqrt{p(1 - p)}} - \\frac{p - \\pi_0}{\\sqrt{(p(1 - p)/n)}}\\right), \\text{ pelo Teo. Central do Limite} \\end{split} Assim podemos encontrar \\alpha_0 a partir do m\u00e1ximo que essa quantidade assume (Mas com uma obseeva\u00e7\u00e3o bem detalhada, quando n \u00e9 grande, p = 0.52 \u00e9 o maximizador) Mas como isso funciona na pr\u00e1tica, propriamente dito. Tenho que escolher um valor limiar para o p-valor, isto \u00e9, se p-valor for menor do que esse limiar, eu rejeito a hip\u00f3tese. Vou ficar esse limiar em 0.05, mas isso \u00e9 arbitr\u00e1rio, apesar da literatura costumar us\u00e1-lo. n = 1018 pnull = . 52 phat = . 56 ztest , pvalue = sm . stats . proportions_ztest ( count = phat * n , #n\u00famero de sucessos nobs = n , #n\u00famero de observa\u00e7\u00f5es value = pnull , #pi_0 alternative = 'larger' ) #hip\u00f3tese alternativa print ( 'O valor da estat\u00edstica de teste foi {} e o p-valor {}' . format ( ztest , pvalue )) O valor da estat\u00edstica de teste foi 0.6392739759907055 e o p-valor 0.26132235751888716","title":"Teste Z para propor\u00e7\u00f5es"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#conclusao-do-teste-de-hipoteses","text":"Como o p-valor foi menor do que nosso limiar, n\u00f3s temos evid\u00eancia para rejeitar a hip\u00f3tese nula que dizia que a prpor\u00e7\u00e3o teria permanecido ou at\u00e9 diminu\u00eddo. Isso n\u00e3o significa que aceitamos a hip\u00f3tese alternativa, apenas que temos evid\u00eancia para acreditar que a propor\u00e7\u00e3o seja maior do que 0.52. n = 1018 pnull = . 52 phat = . 53 ztest , pvalue = sm . stats . proportions_ztest ( count = phat * n , #n\u00famero de sucessos nobs = n , #n\u00famero de observa\u00e7\u00f5es value = pnull , #pi_0 alternative = 'larger' ) #hip\u00f3tese alternativa print ( 'O valor da estat\u00edstica de teste foi {} e o p-valor {}' . format ( ztest , pvalue )) O valor da estat\u00edstica de teste foi 0.6392739759907055 e o p-valor 0.26132235751888716","title":"Conclus\u00e3o do teste de hip\u00f3teses"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#diferenca-entre-propocoes-de-duas-populacoes","text":"Quest\u00e3o : Existe diferen\u00e7a significativa entre pais ingleses e pais alem\u00e3es que reportaram que seus filhos e filhas tiveram aulas de nata\u00e7\u00e3o? Popula\u00e7\u00e3o : Pais com filhos e filhas ingleses e alem\u00e3es. Par\u00e2metro de interesse: p_{ingleses} = p_i e p_{alemaes} = p_a . H_0: p_i - p_a = 0 e H_1: p_i - p_a \\neq 0 . Dados: 247 pais ingleses responderam e dentre eles 36.8% reportaram que sim. Pais alem\u00e3es foram 308, 38.9% que disseram sim.","title":"Diferen\u00e7a entre propo\u00e7\u00f5es de duas popula\u00e7\u00f5es"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#teste-t-para-proporcoes","text":"Na verdade, poder\u00edamos usar o teste Z, com a mesma ideia, s\u00f3 que nesse caso, ter\u00edamos que tomar um pouco de cuidado com o denominador, dado que agora existem duas m\u00e9dias, ent\u00e3o nosso estimador para o desvio padr\u00e3o deve levar em conta esses dois fatores e n deve ser suficientemente grande para que n\u00e3o tenhamos problema. Para evitar isso, vamos usar o T teste. A estat\u00edstica de teste \u00e9 a seguinte, tratando como X e Y as duas amostras consideradas. t = \\frac{\\bar{X}_n - \\bar{Y}_m}{SE} Nesse caso SE \u00e9 o erro entre a diferen\u00e7a entre a m\u00e9dias: isso tem um pequeno problema quando m \\neq n . Ent\u00e3o fazer as contas no papel n\u00e3o \u00e9 trivial. Confira aqui as defini\u00e7\u00f5es de SE precisas. Como nossa inten\u00e7\u00e3o \u00e9 apenas usar esse teste, vamos mostrar como isso pode ser pr\u00e1tico. n = 247 pi = . 368 m = 308 pa = . 389 # Gerando as popula\u00e7\u00f5es england = np . random . binomial ( n = 1 , p = pi , size = n ) germany = np . random . binomial ( n = 1 , p = pa , size = m ) _ , p_value , _ = sm . stats . ttest_ind ( england , germany ) print ( 'O p-valor foi {}' . format ( p_value )) O p-valor foi 0.2615435082780627","title":"Teste T para propor\u00e7\u00f5es"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#conclusao-sobre-o-teste-de-hipoteses","text":"Dado que o p-valor \u00e9 maior do que nosso limiar, n\u00e3o podemos rejeitar a hip\u00f3tese nula. Nesse caso, a diferen\u00e7a das propor\u00e7\u00f5es nas popu\u00e7a\u00e7\u00f5es n\u00e3o foi nada mais do que meramente uma aleatoriedade.","title":"Conclus\u00e3o sobre o teste de hip\u00f3teses"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#mas-o-que-acontece-se-essas-proporcoes-se-mantiveram-para-mais-pessoas","text":"n = 5000 pi = . 37 m = 5000 pa = . 389 england = np . random . binomial ( n = 1 , p = pi , size = n ) germany = np . random . binomial ( n = 1 , p = pa , size = m ) _ , p_value , _ = sm . stats . ttest_ind ( england , germany ) print ( 'O p-valor foi {}' . format ( p_value )) O p-valor foi 0.027721099791980015","title":"Mas o que acontece se essas propor\u00e7\u00f5es se mantiveram para mais pessoas?"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#diferenca-entre-medias-de-populacoes","text":"Quest\u00e3o : Considerando os adultos nos dados da NHAMES , homens tem maior m\u00e9dia de \u00cdndice de Massa Corp\u00f3rea do que mulheres? Popula\u00e7\u00e3o : Adultos na base NHAMES. Par\u00e2metro de interesse: \\mu_{homens} = \\mu_h e \\mu_{mulheres} = \\mu_m . H_0: \\mu_1 = \\mu_2 e H_1: \\mu_1 \\neq \\mu_2 . Dados: 2976 mulheres adultas \\hat{\\mu}_m = 29.94 \\hat{\\sigma}_m = 7.75 2759 homens adultos \\hat{\\mu}_h = 28.78 \\hat{\\sigma}_h = 6.25 url = \"https://raw.githubusercontent.com/kshedden/statswpy/master/NHANES/merged/nhanes_2015_2016.csv\" da = pd . read_csv ( url ) da . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SEQN ALQ101 ALQ110 ALQ130 SMQ020 RIAGENDR RIDAGEYR RIDRETH1 DMDCITZN DMDEDUC2 ... BPXSY2 BPXDI2 BMXWT BMXHT BMXBMI BMXLEG BMXARML BMXARMC BMXWAIST HIQ210 0 83732 1.0 NaN 1.0 1 1 62 3 1.0 5.0 ... 124.0 64.0 94.8 184.5 27.8 43.3 43.6 35.9 101.1 2.0 1 83733 1.0 NaN 6.0 1 1 53 3 2.0 3.0 ... 140.0 88.0 90.4 171.4 30.8 38.0 40.0 33.2 107.9 NaN 2 83734 1.0 NaN NaN 1 1 78 3 1.0 3.0 ... 132.0 44.0 83.4 170.1 28.8 35.6 37.0 31.0 116.5 2.0 3 83735 2.0 1.0 1.0 2 2 56 3 1.0 5.0 ... 134.0 68.0 109.8 160.9 42.4 38.5 37.7 38.3 110.1 2.0 4 83736 2.0 1.0 1.0 2 2 42 4 1.0 4.0 ... 114.0 54.0 55.2 164.9 20.3 37.4 36.0 27.2 80.4 2.0 5 rows \u00d7 28 columns females = da [ da [ \"RIAGENDR\" ] == 2 ] male = da [ da [ \"RIAGENDR\" ] == 1 ] n_m = len ( females ) mu_m = females [ \"BMXBMI\" ] . mean () sd_m = females [ \"BMXBMI\" ] . std () ( n_m , mu_m , sd_m ) (2976, 29.939945652173996, 7.75331880954568) n_h = len ( male ) mu_h = male [ \"BMXBMI\" ] . mean () sd_h = male [ \"BMXBMI\" ] . std () ( n_h , mu_h , sd_h ) (2759, 28.778072111846985, 6.252567616801485) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . set_title ( \"Histogram BMI para mulheres\" , fontsize = 16 ) ax [ 0 ] . hist ( females [ \"BMXBMI\" ] . dropna (), edgecolor = 'k' , color = 'darkred' , bins = 25 ) ax [ 1 ] . set_title ( \"Histogram BMI para homens\" , fontsize = 16 ) ax [ 1 ] . hist ( male [ \"BMXBMI\" ] . dropna (), edgecolor = 'k' , color = 'green' , bins = 25 ) plt . show ()","title":"Diferen\u00e7a entre m\u00e9dias de popula\u00e7\u00f5es"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#vamos-usar-o-t-test-descrito-em-welch","text":"Temos que us\u00e1-lo porque n\u00e3o conhecemos a vari\u00e2ncia. Se conhecessemos, poder\u00edamos usar o teste normal mesmo e se soub\u00e9ssemos que s\u00e3o iguais, mas desconhecessemos, poder\u00edamos usar o ttest ques estudamos no cap\u00edtulo 9.5 . sm . stats . ttest_ind ( x1 = females [ \"BMXBMI\" ] . dropna (), x2 = male [ \"BMXBMI\" ] . dropna (), alternative = 'two-sided' , value = 0 ) # diferen\u00e7a na hip\u00f3tese nula (6.175593353138302, 7.050275578095374e-10, 5660.0)","title":"Vamos usar o t-test descrito em Welch."},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#conclusao-no-teste-de-hipoteses","text":"Como o p-valor \u00e9 bem pequeno, n\u00f3s podemos rejeitar a hip\u00f3tese nula, o que significa que existe diferen\u00e7a estat\u00edstica entre as m\u00e9dias. Isso n\u00e3o responde se a o \u00edndice \u00e9 mais alto para homens, mas podemos fazer o teste unilateral e perceber que de fato isso de fato acontece segundo os dados. sm . stats . ttest_ind ( x1 = females [ \"BMXBMI\" ] . dropna (), x2 = male [ \"BMXBMI\" ] . dropna (), alternative = 'larger' , value = 0 ) # diferen\u00e7a na hip\u00f3tese nula (6.175593353138302, 3.525137789047687e-10, 5660.0)","title":"Conclus\u00e3o no teste de hip\u00f3teses"},{"location":"infestatistica/FisherInformation/FisherInformation/","text":"Informa\u00e7\u00e3o de Fisher Seja X uma amostra aleat\u00f3ria cuja distribui\u00e7\u00e3o depende de \\theta e tem valores em (a,b) \\subset \\mathbb{R} . Seja f_n(x|\\theta) a pdf conjunta de X . Assuma que S = {x | f(x|\\theta) > 0} \u00e9 o mesmo para todo \\theta . E \\lambda_n(x|\\theta) = \\log f_n(x|\\theta) \u00e9 duas vezes diferenci\u00e1vel em \\theta . A informa\u00e7\u00e3o \u00e9: I_n(\\theta) = E_{\\theta}\\{[\\lambda_n '(X|\\theta)]^2\\} Agora assuma que duas derivadas de \\int_S f_n(x|\\theta)dx com respeito a \\theta podemos inverter a ordem de integra\u00e7\u00e3o e diferencia\u00e7\u00e3o . Ent\u00e3o: I_n(\\theta) = - E_{\\theta}[\\lambda_n ''(X|\\theta)] Teorema I_n(\\theta) = nI(\\theta) Obs.: Estamos tratando da informa\u00e7\u00e3o de Fisher para o caso unidimensional. Para o caso em que temos \\Omega \\subset \\mathbb{R}^k , a informa\u00e7\u00e3o de Fisher ser\u00e1 uma matriz de tamanho k \\times k onde I_{n,i,j} = Cov_{\\theta}\\left[\\frac{\\partial}{\\partial \\theta_i}\\lambda_n'(X|\\theta), \\frac{\\partial}{\\partial \\theta_j}\\lambda_n'(X|\\theta)\\right] import numpy as np from scipy.stats import norm from scipy.misc import derivative from scipy.optimize import curve_fit import matplotlib.pyplot as plt from seaborn import violinplot import inspect Exemplo Construtivo Vamos pensar num caso bem simples: amostra aleat\u00f3ria X_1, ..., X_n \\sim \\text{Normal}(\\mu, \\sigma^2) , onde o par\u00e2metro \\sigma^2 \u00e9 conhecido e \\mu n\u00e3o. De forma direta, poder\u00edamos perguntar qual a Informa\u00e7\u00e3o de Fisher (ou Informa\u00e7\u00e3o Diferencial) da amostra aleat\u00f3ria sobre o par\u00e2metro desconhecido \\mu . Vamos encontrar a distribui\u00e7\u00e3o conjunta: f(x|\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2}\\right] \\begin{split} f_n(x|\\mu) &= \\prod_{i=1}^n f(x_i|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i^2 - 2x_i\\mu + \\mu^2)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n x_i^2 - 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\end{split} Vamos encontrar a verossimilhan\u00e7a: \u00e9 a distribui\u00e7\u00e3o conjunta como fun\u00e7\u00e3o do par\u00e2metro! f_n(x|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] Vamos comparar para \\sigma = 1 e \\sigma = 5 loglikelihood = lambda mu , sigma , x : np . sum ( np . log ([ norm ( loc = mu , scale = sigma ) . pdf ( xi ) for xi in x ]), axis = 0 ) sigmas = [ 1 , 3 , 5 , 10 ] mu_true = 5 mu_range = np . linspace ( 0 , 10 , 1000 ) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Log-verossimilhan\u00e7as da Distribui\u00e7\u00e3o Normal' ) def generate_curves ( sigma , ax , n = 20 , n_times = 50 ): for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) logvalues = loglikelihood ( mu_range , sigma , x ) ax . plot ( mu_range , logvalues , color = 'blue' , alpha = 0.2 ) ax . vlines ( mu_true , ymin = ax . get_ylim ()[ 0 ], ymax = ax . get_ylim ()[ 1 ], linestyle = '--' ) ax . set_title ( r '$\\sigma =$ {}' . format ( sigma )) ax . set_xlabel ( r '$\\mu$' ) generate_curves ( sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_curves ( sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_curves ( sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_curves ( sigmas [ 3 ], ax [ 1 ][ 1 ]) Vamos ver como se comporta derivada. Esse \u00e9 o score: \\lambda '_n(y|\\mu) = \\frac{1}{\\sigma^2}\\left(n\\bar{x}_n - \\mu\\right) score = lambda mu , sigma , x : derivative ( loglikelihood , mu , dx = 1e-5 , args = ( sigma , x )) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Scores da Distribui\u00e7\u00e3o Normal' ) def generate_curves ( sigma , ax , n = 20 , n_times = 50 ): for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) scorevalues = score ( mu_range , sigma , x ) ax . plot ( mu_range , scorevalues , color = 'blue' , alpha = 0.2 ) ax . vlines ( mu_true , ymin = ax . get_ylim ()[ 0 ], ymax = ax . get_ylim ()[ 1 ], linestyle = '--' ) ax . set_title ( r '$\\sigma =$ {}' . format ( sigma )) ax . set_xlabel ( r '$\\mu$' ) ax . set_ylim (( - 10 , 10 )) generate_curves ( sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_curves ( sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_curves ( sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_curves ( sigmas [ 3 ], ax [ 1 ][ 1 ]) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Histogramas dos Scores para mu' ) def generate_histograms ( mu , sigma , ax , n = 15 , n_times = 100 ): scorevalues = [] for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) scorevalues . append ( score ( mu , sigma , x )) violinplot ( scorevalues , ax = ax ) ax . set_title ( r '$\\sigma =$ {}' . format ( sigma )) ax . set_xlabel ( 'score' ) generate_histograms ( 5 , sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_histograms ( 5 , sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_histograms ( 5 , sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_histograms ( 5 , sigmas [ 3 ], ax [ 1 ][ 1 ]) A informa\u00e7\u00e3o de Fisher \u00e9 a Vari\u00e2ncia da fun\u00e7\u00e3o score em X , isto \u00e9: \\begin{split} I_n(\\mu) &= Var(\\lambda '_n(x|p)) = E[(\\lambda '_n(x|p))^2] - E[\\lambda '_n(x|p)]^2\\\\ &= \\frac{1}{\\sigma^4}Var\\left[n\\bar{x}_n - \\mu\\right] \\\\ &= \\frac{n^2}{\\sigma^4}Var(\\bar{x}_n) \\\\ &= \\frac{n^2\\sigma^2}{n\\sigma^4} \\\\ &= \\frac{n}{\\sigma^2} \\end{split} Desigualdade de Cram\u00e9r-Rao Seja X uma amostra aleat\u00f3ria com pdf f(x| \\theta) . Suponha as hip\u00f3teses acima acerca dessa distribui\u00e7\u00e3o. Seja T = r(X) com vari\u00e2ncia finita e m(\\theta) = E_{\\theta}(T) \u00e9 diferenci\u00e1vel. Assim: Var_{\\theta}(T) \\geq \\frac{[m'(\\theta)]^2}{nI(\\theta)} A igualdade vale se, e somente se, existem fun\u00e7\u00f5es u(\\theta) e v(\\theta) que podem depender em \\theta mas n\u00e3o de X tal que: T = u(\\theta)\\lambda_n'(X|\\theta) + v(\\theta) Se T for n\u00e3o enviesado m(\\theta) = \\theta \\implies m'(\\theta) = 1 Exemplo Num\u00e9rico do limite de Cram\u00e9r-Rao Refer\u00eancia Considere um sinal (como uma m\u00fasica) com tr\u00eas par\u00e2metros, amplitude, frequ\u00eancia e fase inicia. Saberemos o n\u00famero de amostras que sera 100Hz com n\u00edvel de ru\u00eddo de 0.1 s = lambda t , a , f , ph : a * np . sin ( 2 * np . pi * f * t + ph ) # fun\u00e7\u00e3o que representa o sinal p0 = [ 2 , 8 , 0 ] # Amplitude, frequ\u00eancia e fase inicial para testar noise = 0.1 T = np . linspace ( 0 , 1 , 100 ) #100 valores entre 0 e 1 igualmente espa\u00e7ados plt . plot ( T , s ( T , * p0 ), '.-k' ) plt . xlabel ( 'Tempo (s)' ) plt . title ( 'Sinal' ) plt . show () Vamos usar inspect para nos ajudar a pegar labels das fun\u00e7\u00f5es, isto \u00e9, os par\u00e2metros necess\u00e1rios das fun\u00e7\u00f5es. Essa biblioteca fornece v\u00e1rias fun\u00e7\u00f5es de ajuda desse tipo. D\u00ea uma olhada. parameters = str ( inspect . signature ( s )) . strip ( '()' ) . replace ( ' ' , '' ) . split ( ',' )[ 1 :] p0dict = dict ( zip ( parameters , p0 )) p0dict {'a': 2, 'f': 8, 'ph': 0} No caso geral, calcular a Matriz de Informa\u00e7\u00e3o de Fisher n\u00e3o \u00e9 trivial. Por isso, vamos calcular para o caso em que as medi\u00e7\u00f5es s\u00e3o de uma amostra com distribui\u00e7\u00e3o multivariada normal, isto \u00e9, \u00e9 uma distribui\u00e7\u00e3o normal, s\u00f3 que em mais dimens\u00f5es, em particular, 441 dimens\u00f5es (n\u00famero de pontos no tempo) Se calcularmos a informa\u00e7\u00e3o de Fisher, podemos ver que: \\mathcal{I}_{mn} = \\frac{1}{\\sigma^2} \\frac{\\partial \\mu^\\mathrm{T}}{\\partial \\theta_m} \\frac{\\partial \\mu}{\\partial \\theta_n} = \\frac{1}{\\sigma^2} \\sum_k \\frac{\\partial \\mu_k}{\\partial \\theta_m} \\frac{\\partial \\mu_k}{\\partial \\theta_n} onde \\theta = [a,f,ph]^T , \\mu = \\mu(\\theta) \u00e9 o vetor m\u00e9dia da normal multivariada e \\sigma^2 \u00e9 a vari\u00e2ncia de cada marginal da normal. N\u00e3o se assuste. Na multivariada, temos uma matriz para indicar as vari\u00e2ncias (ela se chama Matriz de Covari\u00e2ncias, na verdade). O que estou dizendo \u00e9 que ela \u00e9 \\sigma^2 vezes a identidade. \u00c9 bom conhecer essa distribui\u00e7\u00e3o! Por enquando acredite em mim! Ou no Wikipedia . Vou chamar D_{ik} = \\frac{\\partial \\mu_k}{\\partial \\theta_i} # Usamos ** para desempacotar elementos de um dicion\u00e1rio. string = \"a: {a} f: {f} ph: {ph}\" . format ( ** p0dict ) print ( string ) a: 2 f: 8 ph: 0 D = np . zeros (( len ( p0 ), len ( T ))) # para cada par\u00e2metro for i , parameter in enumerate ( parameters ): # para cada ponto no tempo for k , t in enumerate ( T ): func = lambda x : s ( t , ** dict ( p0dict , ** { parameter : x })) # Calculamos a derivada com respeito a x, que nesse caso \u00e9 o valor do parametro D [ i , k ] = derivative ( func , p0dict [ parameter ], dx = 1e-4 ) Veja que o tamanho de D \u00e9 o seguinte: D . shape (3, 100) plt . plot ( T , s ( T , * p0 ), '--k' , lw = 2 , label = 'Sinal' ) for Di , parameter in zip ( D , parameters ): # Estamos acessando Di = linha_i(D) plt . plot ( T , Di , '.-' , label = parameter ) plt . legend () plt . xlabel ( 'Tempo (s)' ) plt . show () O que D_{ik} indica? \u00c9 a derivada da k-\u00e9sima m\u00e9dia com respeito ao i-\u00e9simo par\u00e2metro. Logo indica o quanto o quando a amostra k afeta o par\u00e2metro i . Veja que quando temos picos no seno, teremos pico na amplitude,. Tamb\u00e9m vemos que a fase inicial n\u00e3o tem essa relev\u00e2ncia. Vemos tamb\u00e9m que o sinal se torna mais e mais sens\u00edvel \u00e0 frequ\u00eancia. Assim, podemos calular a informa\u00e7\u00e3o de fisher, usando einsum I = 1 / noise ** 2 * np . einsum ( 'mk,nk' , D , D ) print ( I ) [[ 4.95000000e+03 -5.64643569e+02 -3.43706036e-09] [-5.64643569e+02 2.68635205e+05 6.34601694e+04] [-3.43706036e-09 6.34601694e+04 2.01999999e+04]] Podemos calcular o limite de Cram\u00e9r-Rao para qualquer estimador n\u00e3o enviesado. Nesse caso, veja aqui para mais detalhes. Mas n\u00e3o se incomode com os detalhes, se preferir. iI = np . linalg . inv ( I ) print ( 'Cram\u00e9r-Rao Limite Inferior' ) for parameter , variance in zip ( parameters , iI . diagonal ()): print ( '{}: {:.2g}' . format ( parameter , np . sqrt ( variance ))) Cram\u00e9r-Rao Limite Inferior a: 0.014 f: 0.0038 ph: 0.014 Estimador Eficiente T \u00e9 um estimador eficiente de sua esperan\u00e7a m(\\theta) se, para todo \\theta , vale a igualdade em Cram\u00e9r-Rao. Mas nem sempre vale a igualdade, inclusive conhecemos uma consdi\u00e7\u00e3o necess\u00e1ria e suficiente para isso, que est\u00e1 logo acima. Estimadores n\u00e3o enviesados com vari\u00e2ncia m\u00ednima Suponha que T seja um estimador eficiente de sua esperan\u00e7a m(\\theta) e T_1 outro estimador n\u00e3o enviesado. Ent\u00e3o para todo valor \\theta \\in \\Omega , Var_{\\theta}(T) ser\u00e1 igual ao limite inferior de Cram\u00e9r-Rao e Var_{\\theta}(T_1) ser\u00e1 pelo menos maior ou igual. Portanto Var_{\\theta}(T) \\leq Var_{\\theta}(T_1), \\forall \\theta . Isto \u00e9, um estimado eficiente de m(\\theta) ter\u00e1 menor vari\u00e2ncia. Distribui\u00e7\u00e3o assint\u00f3tica de um estimador eficiente Assuma as hip\u00f3teses do teorema de Cram\u00e9r-Rao. Seja T um estimador eficiente para a sua m\u00e9dia m(\\theta) e m'(\\theta) \\neq 0 . Ent\u00e3o: \\frac{[nI(\\theta)]^{1/2}}{m'(\\theta)}[T - m(\\theta)] \\overset{d}{\\to} N(0,1) Distribui\u00e7\u00e3o assint\u00f3tica do MLE Suponha que obtemos \\hat{\\theta}_n resolvendo a equa\u00e7\u00e3o \\lambda_n'(x|\\theta) = 0 , isto \u00e9, maximizando a log-verossimilhan\u00e7a (MLE). E suponha que \\lambda_n'' e \\lambda_n''' existem e satisfazem certas condi\u00e7\u00f5es de regularidade. Ent\u00e3o [nI(\\theta)]^{1/2}(\\hat{\\theta}_n - \\theta) \\overset{d}{\\to} N(0,1) Como o MLE \u00e9 n\u00e3o enviesado, ent\u00e3o se ele for Eficiente, j\u00e1 sabemos que esse teorema \u00e9 verdade pelo anterior. (se ele \u00e9 n\u00e3o enviesado) Bayesiano Suponha que adotamos uma priori para \\theta com uma pdf diferenci\u00e1vel no intervalo. Sobre condi\u00e7\u00f5es de regularidade similares \u00e0quelas que garantem normalidade assint\u00f3tica para \\hat{\\theta}_n , pode-se mostrar que que a distribui\u00e7\u00e3o a posteriori de \\theta vai se aproximadamente uma normal com m\u00e9dia \\hat{\\theta}_n e vari\u00e2ncia 1/[nI(\\hat{\\theta}_n)] , onde \\hat{\\theta}_n \u00e9 o MLE.","title":"Informa\u00e7\u00e3o de Fisher"},{"location":"infestatistica/FisherInformation/FisherInformation/#informacao-de-fisher","text":"Seja X uma amostra aleat\u00f3ria cuja distribui\u00e7\u00e3o depende de \\theta e tem valores em (a,b) \\subset \\mathbb{R} . Seja f_n(x|\\theta) a pdf conjunta de X . Assuma que S = {x | f(x|\\theta) > 0} \u00e9 o mesmo para todo \\theta . E \\lambda_n(x|\\theta) = \\log f_n(x|\\theta) \u00e9 duas vezes diferenci\u00e1vel em \\theta . A informa\u00e7\u00e3o \u00e9: I_n(\\theta) = E_{\\theta}\\{[\\lambda_n '(X|\\theta)]^2\\} Agora assuma que duas derivadas de \\int_S f_n(x|\\theta)dx com respeito a \\theta podemos inverter a ordem de integra\u00e7\u00e3o e diferencia\u00e7\u00e3o . Ent\u00e3o: I_n(\\theta) = - E_{\\theta}[\\lambda_n ''(X|\\theta)]","title":"Informa\u00e7\u00e3o de Fisher"},{"location":"infestatistica/FisherInformation/FisherInformation/#teorema","text":"I_n(\\theta) = nI(\\theta) Obs.: Estamos tratando da informa\u00e7\u00e3o de Fisher para o caso unidimensional. Para o caso em que temos \\Omega \\subset \\mathbb{R}^k , a informa\u00e7\u00e3o de Fisher ser\u00e1 uma matriz de tamanho k \\times k onde I_{n,i,j} = Cov_{\\theta}\\left[\\frac{\\partial}{\\partial \\theta_i}\\lambda_n'(X|\\theta), \\frac{\\partial}{\\partial \\theta_j}\\lambda_n'(X|\\theta)\\right] import numpy as np from scipy.stats import norm from scipy.misc import derivative from scipy.optimize import curve_fit import matplotlib.pyplot as plt from seaborn import violinplot import inspect","title":"Teorema"},{"location":"infestatistica/FisherInformation/FisherInformation/#exemplo-construtivo","text":"Vamos pensar num caso bem simples: amostra aleat\u00f3ria X_1, ..., X_n \\sim \\text{Normal}(\\mu, \\sigma^2) , onde o par\u00e2metro \\sigma^2 \u00e9 conhecido e \\mu n\u00e3o. De forma direta, poder\u00edamos perguntar qual a Informa\u00e7\u00e3o de Fisher (ou Informa\u00e7\u00e3o Diferencial) da amostra aleat\u00f3ria sobre o par\u00e2metro desconhecido \\mu . Vamos encontrar a distribui\u00e7\u00e3o conjunta: f(x|\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2}\\right] \\begin{split} f_n(x|\\mu) &= \\prod_{i=1}^n f(x_i|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i^2 - 2x_i\\mu + \\mu^2)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n x_i^2 - 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\end{split} Vamos encontrar a verossimilhan\u00e7a: \u00e9 a distribui\u00e7\u00e3o conjunta como fun\u00e7\u00e3o do par\u00e2metro! f_n(x|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] Vamos comparar para \\sigma = 1 e \\sigma = 5 loglikelihood = lambda mu , sigma , x : np . sum ( np . log ([ norm ( loc = mu , scale = sigma ) . pdf ( xi ) for xi in x ]), axis = 0 ) sigmas = [ 1 , 3 , 5 , 10 ] mu_true = 5 mu_range = np . linspace ( 0 , 10 , 1000 ) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Log-verossimilhan\u00e7as da Distribui\u00e7\u00e3o Normal' ) def generate_curves ( sigma , ax , n = 20 , n_times = 50 ): for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) logvalues = loglikelihood ( mu_range , sigma , x ) ax . plot ( mu_range , logvalues , color = 'blue' , alpha = 0.2 ) ax . vlines ( mu_true , ymin = ax . get_ylim ()[ 0 ], ymax = ax . get_ylim ()[ 1 ], linestyle = '--' ) ax . set_title ( r '$\\sigma =$ {}' . format ( sigma )) ax . set_xlabel ( r '$\\mu$' ) generate_curves ( sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_curves ( sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_curves ( sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_curves ( sigmas [ 3 ], ax [ 1 ][ 1 ]) Vamos ver como se comporta derivada. Esse \u00e9 o score: \\lambda '_n(y|\\mu) = \\frac{1}{\\sigma^2}\\left(n\\bar{x}_n - \\mu\\right) score = lambda mu , sigma , x : derivative ( loglikelihood , mu , dx = 1e-5 , args = ( sigma , x )) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Scores da Distribui\u00e7\u00e3o Normal' ) def generate_curves ( sigma , ax , n = 20 , n_times = 50 ): for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) scorevalues = score ( mu_range , sigma , x ) ax . plot ( mu_range , scorevalues , color = 'blue' , alpha = 0.2 ) ax . vlines ( mu_true , ymin = ax . get_ylim ()[ 0 ], ymax = ax . get_ylim ()[ 1 ], linestyle = '--' ) ax . set_title ( r '$\\sigma =$ {}' . format ( sigma )) ax . set_xlabel ( r '$\\mu$' ) ax . set_ylim (( - 10 , 10 )) generate_curves ( sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_curves ( sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_curves ( sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_curves ( sigmas [ 3 ], ax [ 1 ][ 1 ]) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Histogramas dos Scores para mu' ) def generate_histograms ( mu , sigma , ax , n = 15 , n_times = 100 ): scorevalues = [] for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) scorevalues . append ( score ( mu , sigma , x )) violinplot ( scorevalues , ax = ax ) ax . set_title ( r '$\\sigma =$ {}' . format ( sigma )) ax . set_xlabel ( 'score' ) generate_histograms ( 5 , sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_histograms ( 5 , sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_histograms ( 5 , sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_histograms ( 5 , sigmas [ 3 ], ax [ 1 ][ 1 ]) A informa\u00e7\u00e3o de Fisher \u00e9 a Vari\u00e2ncia da fun\u00e7\u00e3o score em X , isto \u00e9: \\begin{split} I_n(\\mu) &= Var(\\lambda '_n(x|p)) = E[(\\lambda '_n(x|p))^2] - E[\\lambda '_n(x|p)]^2\\\\ &= \\frac{1}{\\sigma^4}Var\\left[n\\bar{x}_n - \\mu\\right] \\\\ &= \\frac{n^2}{\\sigma^4}Var(\\bar{x}_n) \\\\ &= \\frac{n^2\\sigma^2}{n\\sigma^4} \\\\ &= \\frac{n}{\\sigma^2} \\end{split}","title":"Exemplo Construtivo"},{"location":"infestatistica/FisherInformation/FisherInformation/#desigualdade-de-cramer-rao","text":"Seja X uma amostra aleat\u00f3ria com pdf f(x| \\theta) . Suponha as hip\u00f3teses acima acerca dessa distribui\u00e7\u00e3o. Seja T = r(X) com vari\u00e2ncia finita e m(\\theta) = E_{\\theta}(T) \u00e9 diferenci\u00e1vel. Assim: Var_{\\theta}(T) \\geq \\frac{[m'(\\theta)]^2}{nI(\\theta)} A igualdade vale se, e somente se, existem fun\u00e7\u00f5es u(\\theta) e v(\\theta) que podem depender em \\theta mas n\u00e3o de X tal que: T = u(\\theta)\\lambda_n'(X|\\theta) + v(\\theta) Se T for n\u00e3o enviesado m(\\theta) = \\theta \\implies m'(\\theta) = 1","title":"Desigualdade de Cram\u00e9r-Rao"},{"location":"infestatistica/FisherInformation/FisherInformation/#exemplo-numerico-do-limite-de-cramer-rao","text":"Refer\u00eancia Considere um sinal (como uma m\u00fasica) com tr\u00eas par\u00e2metros, amplitude, frequ\u00eancia e fase inicia. Saberemos o n\u00famero de amostras que sera 100Hz com n\u00edvel de ru\u00eddo de 0.1 s = lambda t , a , f , ph : a * np . sin ( 2 * np . pi * f * t + ph ) # fun\u00e7\u00e3o que representa o sinal p0 = [ 2 , 8 , 0 ] # Amplitude, frequ\u00eancia e fase inicial para testar noise = 0.1 T = np . linspace ( 0 , 1 , 100 ) #100 valores entre 0 e 1 igualmente espa\u00e7ados plt . plot ( T , s ( T , * p0 ), '.-k' ) plt . xlabel ( 'Tempo (s)' ) plt . title ( 'Sinal' ) plt . show () Vamos usar inspect para nos ajudar a pegar labels das fun\u00e7\u00f5es, isto \u00e9, os par\u00e2metros necess\u00e1rios das fun\u00e7\u00f5es. Essa biblioteca fornece v\u00e1rias fun\u00e7\u00f5es de ajuda desse tipo. D\u00ea uma olhada. parameters = str ( inspect . signature ( s )) . strip ( '()' ) . replace ( ' ' , '' ) . split ( ',' )[ 1 :] p0dict = dict ( zip ( parameters , p0 )) p0dict {'a': 2, 'f': 8, 'ph': 0} No caso geral, calcular a Matriz de Informa\u00e7\u00e3o de Fisher n\u00e3o \u00e9 trivial. Por isso, vamos calcular para o caso em que as medi\u00e7\u00f5es s\u00e3o de uma amostra com distribui\u00e7\u00e3o multivariada normal, isto \u00e9, \u00e9 uma distribui\u00e7\u00e3o normal, s\u00f3 que em mais dimens\u00f5es, em particular, 441 dimens\u00f5es (n\u00famero de pontos no tempo) Se calcularmos a informa\u00e7\u00e3o de Fisher, podemos ver que: \\mathcal{I}_{mn} = \\frac{1}{\\sigma^2} \\frac{\\partial \\mu^\\mathrm{T}}{\\partial \\theta_m} \\frac{\\partial \\mu}{\\partial \\theta_n} = \\frac{1}{\\sigma^2} \\sum_k \\frac{\\partial \\mu_k}{\\partial \\theta_m} \\frac{\\partial \\mu_k}{\\partial \\theta_n} onde \\theta = [a,f,ph]^T , \\mu = \\mu(\\theta) \u00e9 o vetor m\u00e9dia da normal multivariada e \\sigma^2 \u00e9 a vari\u00e2ncia de cada marginal da normal. N\u00e3o se assuste. Na multivariada, temos uma matriz para indicar as vari\u00e2ncias (ela se chama Matriz de Covari\u00e2ncias, na verdade). O que estou dizendo \u00e9 que ela \u00e9 \\sigma^2 vezes a identidade. \u00c9 bom conhecer essa distribui\u00e7\u00e3o! Por enquando acredite em mim! Ou no Wikipedia . Vou chamar D_{ik} = \\frac{\\partial \\mu_k}{\\partial \\theta_i} # Usamos ** para desempacotar elementos de um dicion\u00e1rio. string = \"a: {a} f: {f} ph: {ph}\" . format ( ** p0dict ) print ( string ) a: 2 f: 8 ph: 0 D = np . zeros (( len ( p0 ), len ( T ))) # para cada par\u00e2metro for i , parameter in enumerate ( parameters ): # para cada ponto no tempo for k , t in enumerate ( T ): func = lambda x : s ( t , ** dict ( p0dict , ** { parameter : x })) # Calculamos a derivada com respeito a x, que nesse caso \u00e9 o valor do parametro D [ i , k ] = derivative ( func , p0dict [ parameter ], dx = 1e-4 ) Veja que o tamanho de D \u00e9 o seguinte: D . shape (3, 100) plt . plot ( T , s ( T , * p0 ), '--k' , lw = 2 , label = 'Sinal' ) for Di , parameter in zip ( D , parameters ): # Estamos acessando Di = linha_i(D) plt . plot ( T , Di , '.-' , label = parameter ) plt . legend () plt . xlabel ( 'Tempo (s)' ) plt . show () O que D_{ik} indica? \u00c9 a derivada da k-\u00e9sima m\u00e9dia com respeito ao i-\u00e9simo par\u00e2metro. Logo indica o quanto o quando a amostra k afeta o par\u00e2metro i . Veja que quando temos picos no seno, teremos pico na amplitude,. Tamb\u00e9m vemos que a fase inicial n\u00e3o tem essa relev\u00e2ncia. Vemos tamb\u00e9m que o sinal se torna mais e mais sens\u00edvel \u00e0 frequ\u00eancia. Assim, podemos calular a informa\u00e7\u00e3o de fisher, usando einsum I = 1 / noise ** 2 * np . einsum ( 'mk,nk' , D , D ) print ( I ) [[ 4.95000000e+03 -5.64643569e+02 -3.43706036e-09] [-5.64643569e+02 2.68635205e+05 6.34601694e+04] [-3.43706036e-09 6.34601694e+04 2.01999999e+04]] Podemos calcular o limite de Cram\u00e9r-Rao para qualquer estimador n\u00e3o enviesado. Nesse caso, veja aqui para mais detalhes. Mas n\u00e3o se incomode com os detalhes, se preferir. iI = np . linalg . inv ( I ) print ( 'Cram\u00e9r-Rao Limite Inferior' ) for parameter , variance in zip ( parameters , iI . diagonal ()): print ( '{}: {:.2g}' . format ( parameter , np . sqrt ( variance ))) Cram\u00e9r-Rao Limite Inferior a: 0.014 f: 0.0038 ph: 0.014","title":"Exemplo Num\u00e9rico do limite de Cram\u00e9r-Rao"},{"location":"infestatistica/FisherInformation/FisherInformation/#estimador-eficiente","text":"T \u00e9 um estimador eficiente de sua esperan\u00e7a m(\\theta) se, para todo \\theta , vale a igualdade em Cram\u00e9r-Rao. Mas nem sempre vale a igualdade, inclusive conhecemos uma consdi\u00e7\u00e3o necess\u00e1ria e suficiente para isso, que est\u00e1 logo acima.","title":"Estimador Eficiente"},{"location":"infestatistica/FisherInformation/FisherInformation/#estimadores-nao-enviesados-com-variancia-minima","text":"Suponha que T seja um estimador eficiente de sua esperan\u00e7a m(\\theta) e T_1 outro estimador n\u00e3o enviesado. Ent\u00e3o para todo valor \\theta \\in \\Omega , Var_{\\theta}(T) ser\u00e1 igual ao limite inferior de Cram\u00e9r-Rao e Var_{\\theta}(T_1) ser\u00e1 pelo menos maior ou igual. Portanto Var_{\\theta}(T) \\leq Var_{\\theta}(T_1), \\forall \\theta . Isto \u00e9, um estimado eficiente de m(\\theta) ter\u00e1 menor vari\u00e2ncia.","title":"Estimadores n\u00e3o enviesados com vari\u00e2ncia m\u00ednima"},{"location":"infestatistica/FisherInformation/FisherInformation/#distribuicao-assintotica-de-um-estimador-eficiente","text":"Assuma as hip\u00f3teses do teorema de Cram\u00e9r-Rao. Seja T um estimador eficiente para a sua m\u00e9dia m(\\theta) e m'(\\theta) \\neq 0 . Ent\u00e3o: \\frac{[nI(\\theta)]^{1/2}}{m'(\\theta)}[T - m(\\theta)] \\overset{d}{\\to} N(0,1)","title":"Distribui\u00e7\u00e3o assint\u00f3tica de um estimador eficiente"},{"location":"infestatistica/FisherInformation/FisherInformation/#distribuicao-assintotica-do-mle","text":"Suponha que obtemos \\hat{\\theta}_n resolvendo a equa\u00e7\u00e3o \\lambda_n'(x|\\theta) = 0 , isto \u00e9, maximizando a log-verossimilhan\u00e7a (MLE). E suponha que \\lambda_n'' e \\lambda_n''' existem e satisfazem certas condi\u00e7\u00f5es de regularidade. Ent\u00e3o [nI(\\theta)]^{1/2}(\\hat{\\theta}_n - \\theta) \\overset{d}{\\to} N(0,1) Como o MLE \u00e9 n\u00e3o enviesado, ent\u00e3o se ele for Eficiente, j\u00e1 sabemos que esse teorema \u00e9 verdade pelo anterior. (se ele \u00e9 n\u00e3o enviesado)","title":"Distribui\u00e7\u00e3o assint\u00f3tica do MLE"},{"location":"infestatistica/FisherInformation/FisherInformation/#bayesiano","text":"Suponha que adotamos uma priori para \\theta com uma pdf diferenci\u00e1vel no intervalo. Sobre condi\u00e7\u00f5es de regularidade similares \u00e0quelas que garantem normalidade assint\u00f3tica para \\hat{\\theta}_n , pode-se mostrar que que a distribui\u00e7\u00e3o a posteriori de \\theta vai se aproximadamente uma normal com m\u00e9dia \\hat{\\theta}_n e vari\u00e2ncia 1/[nI(\\hat{\\theta}_n)] , onde \\hat{\\theta}_n \u00e9 o MLE.","title":"Bayesiano"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/","text":"Grandes Amostras Desigualdade de Markov P(X \\geq t) \\leq \\frac{E[X^n]}{t^n}, ~dado~que~P(X \\geq 0) = 1, t > 0 Desigualdade de Chebyshev Seja X uma vari\u00e1vel aleat\u00f3ria em que o segundo momento \u00e9 finito. Ent\u00e3o, \\forall t > 0 . P(|X - E[X]| \\geq t) \\leq \\frac{Var[X]}{t^2} Propriedades Importantes X_1, ..., X_n amostra aleat\u00f3ria (por defini\u00e7\u00e3o mesma distribui\u00e7\u00e3o e independentes), com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 . Ent\u00e3o E[\\bar{X_n}] = \\mu e Var[\\bar{X_n}] = \\sigma^2/n . Importando bibliotecas import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set () Exemplo 6.2.2 Um engenheiro ambiental acredita que existam dois contaminantes na \u00e1gua: ars\u00eanico e chumbo. Suponha que ambas s\u00e3o vari\u00e1veis aleat\u00f3rias independentes X e Y , medidas na mesma unidade. O engenheiro est\u00e1 interessado em saber a propor\u00e7\u00e3o de contamina\u00e7\u00e3o por chumbo em m\u00e9dia, isto \u00e9, E[R] = E[Y/(X + Y)] . Como nem sempre conhecemos a distribui\u00e7\u00e3o de R, podemos aproximar o valor esperado atrav\u00e9s de uma m\u00e9dia amostral de R , atrav\u00e9s de observa\u00e7\u00f5es (X_1, Y_1), ..., (X_n, Y_n) . Usando a desigualdade de Chebyshev, (tente ver que Var[R] \\leq 1 ). P(|\\bar{R_n} - E[R]| \\geq \\epsilon) \\leq \\frac{1}{n\\epsilon^2} # Usando apenas Chebyshev: epsilon = 0.0005 prob = 0.95 # probabilidade m\u00ednima de que a diferen\u00e7a entre a # m\u00e9dia amostral e o valor esperado seja menor do que epsilon def get_number_simulations ( epsilon , prob ): assert prob >= 0 assert prob <= 1 # Queremos que P <= 1 - prob-> 1/(n*eps**2) <= 1 - prob min_n = 1 / (( 1 - prob ) * ( epsilon ** 2 )) min_n = np . ceil ( min_n ) print ( '----------------------------------------------------------------------' ) print ( 'In order to have the sample mean at least {} close with probability {}: ' . format ( epsilon , prob )) print ( 'The minimum number of simulations are: {}' . format ( int ( min_n ))) print ( '----------------------------------------------------------------------' ) return min_n def get_epsilon ( prob , n ): assert prob >= 0 assert prob <= 1 # Queremos encontrar epsilon para que 1/(n*eps**2) = 1 - prob eps = np . sqrt ( 1 / ( n * ( 1 - prob ))) return eps _ = get_number_simulations ( epsilon , prob ) ---------------------------------------------------------------------- In order to have the sample mean at least 0.0005 close with probability 0.95: The minimum number of simulations are: 80000000 ---------------------------------------------------------------------- # Testando com distribui\u00e7\u00e3o uniforme (X e Y tem distribui\u00e7\u00f5es uniformes) # Nesse caso, podemos provar que E[R] = 0.5 probs = [ 0.6 , 0.75 , 0.9 , 0.95 , 0.99 ] n_range = np . array ([ j * 10 ** i for i in [ 2 , 3 , 4 , 5 , 6 , 7 ] for j in [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]]) E_R = [] for n in n_range : X = np . random . uniform ( 0 , 1 , size = int ( n )) #np = numpy Y = np . random . uniform ( 0 , 1 , size = int ( n )) R = Y / ( X + Y ) E_R . append ( np . mean ( R )) # E[R] = 0.5 chebyshev_interval = np . empty ( shape = ( len ( probs ), len ( n_range ))) for i , prob in enumerate ( probs ): chebyshev_interval [ i , :] = get_epsilon ( prob , n_range ) # Plotando fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) for i in [ 0 , 1 ]: ax [ i ] . plot ( n_range , E_R , color = 'darkred' ) ax [ i ] . hlines ( 0.5 , xmin = min ( n_range ), xmax = max ( n_range ), linestyle = '--' , alpha = 0.4 , color = 'black' ) ax [ i ] . set_xscale ( 'log' ) colors = [ 'black' , 'red' , 'green' , 'blue' , 'pink' ] for i in range ( len ( probs )): ax [ 1 ] . fill_between ( x = n_range , y1 = 0.5 + chebyshev_interval [ i ,:], y2 = 0.5 - chebyshev_interval [ i ,:], color = colors [ i ], alpha = 0.3 + 0.5 * ( len ( probs ) - i ) / ( len ( probs )), label = probs [ i ]) ax [ 1 ] . legend () ax [ 0 ] . set_title ( 'Different mean samples' , fontsize = 15 ) ax [ 1 ] . set_title ( 'Chebyshev bound with prob' , fontsize = 15 ) plt . show () Lei dos Grandes N\u00fameros Converg\u00eancia em Probabilidade \\forall \\epsilon > 0, \\lim_{n\\to\\infty} P[|Z_n - b| < \\epsilon] = 1 \\iff Z_n \\overset{p}{\\to} b Converg\u00eancia quase certa (Implica a anterior) P[\\lim_{n_\\to\\infty} Z_n = b] = 1 Vers\u00e3o Fraca X_1, \\dots, X_n amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o com m\u00e9dia \\mu e vari\u00e2ncia vinita. Se \\bar{X}_n \u00e9 a m\u00e9dia amostral. Ent\u00e3o \\bar{X}_n \\overset{p}{\\to} \\mu . Vers\u00e3o Forte P[\\lim_{n\\to\\infty} \\bar{X}_n = \\mu] = 1 Histogramas S\u00e3o usados para aproximar uma fun\u00e7\u00e3o de densidade de probabilidade de forma discreta. Seja X_1, X_2, \\dots vari\u00e1veis aleat\u00f3rias iid. Seja c_1 < c_2 constantes. Seja Y_i uma indicadora para c_1\\leq X_i < c_2 . Ent\u00e3o \\bar{Y}_n (propor\u00e7\u00e3o de valores X_1, ..., X_n no intervalo [c_1, c_2) e \\bar{Y}_n \\overset{p}{\\to} P[c_1 \\leq X_1 < c_2] . # Exemplo 6.2.4 lamda = 0.5 # N\u00e3o posso usar lambda beta = 1 / lamda # Numpy usa esse par\u00e2metro t = np . arange ( 0.0001 , 15 , 0.01 ) X_true = lamda * np . exp ( - lamda * t ) fig , ax = plt . subplots ( figsize = ( 14 , 5 )) for n in [ 1 , 10 , 100 , 1000 , 10000 ]: X = np . random . exponential ( scale = beta , size = n ) sns . distplot ( X , ax = ax , kde = False , norm_hist = True , label = 'n=' + str ( n )) # area = 1 sns . lineplot ( t , X_true , ax = ax , lw = 3 ) ax . set_title ( 'Histograma da Distribui\u00e7\u00e3o Exponencial' ) ax . legend () plt . show () Teorema Central do Limite Se as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n formam uma amostra aleat\u00f3ria de tamanho n para uma dada distribui\u00e7\u00e3o de m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 finita, ent\u00e3o para cada n\u00famero x , lim_{n\\to\\infty} P[\\frac{\\bar{X}_n - \\mu}{\\sigma/n^{1/2}} \\leq x] = \\Phi(x), onde \\Phi \u00e9 a fun\u00e7\u00e3o de densidade acumulada da distribui\u00e7\u00e3o normal!!! coffee_df = pd . read_csv ( '../data/CoffeeAndCode.csv' ) display ( coffee_df . head ()) display ( coffee_df . shape ) display ( coffee_df . describe ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay CoffeeTime CodingWithoutCoffee CoffeeType CoffeeSolveBugs Gender Country AgeRange 0 8 2 Before coding Yes Caff\u00e8 latte Sometimes Female Lebanon 18 to 29 1 3 2 Before coding Yes Americano Yes Female Lebanon 30 to 39 2 5 3 While coding No Nescafe Yes Female Lebanon 18 to 29 3 8 2 Before coding No Nescafe Yes Male Lebanon NaN 4 10 3 While coding Sometimes Turkish No Male Lebanon 18 to 29 (100, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay count 100.000000 100.000000 mean 6.410000 2.890000 std 2.644205 1.613673 min 1.000000 1.000000 25% 4.000000 2.000000 50% 7.000000 2.500000 75% 8.000000 4.000000 max 10.000000 8.000000 # Plotting fig , ax = plt . subplots ( figsize = ( 7 , 5 )) sns . distplot ( coffee_df . CoffeeCupsPerDay , ax = ax ) ax . vlines ( coffee_df . CoffeeCupsPerDay . mean (), ymin = 0 , ymax = 1 , linestyle = '--' , color = 'black' , label = 'black' ) ax . annotate ( 'M\u00e9dia:' + str ( coffee_df . CoffeeCupsPerDay . mean ()), ( coffee_df . CoffeeCupsPerDay . mean () + 0.5 , 0.4 )) ax . set_title ( 'Histrograma de copos de caf\u00e9 bebidos por programadores' ) ax . set_ylabel ( 'Frequ\u00eancia' ) ax . set_ylim (( 0 , 0.45 )) plt . show () # Generating sample means samples = [ 10 , 50 , 150 , 300 , 500 , 1000 ] n_experiments = 500 experiments_coffe_cups = np . empty (( n_experiments , len ( samples ))) for j , sample_size in enumerate ( samples ): sample = coffee_df . CoffeeCupsPerDay . sample ( n = sample_size * n_experiments , replace = True ) matrix = np . array ( sample ) . reshape (( n_experiments , sample_size )) experiments_coffe_cups [:, j ] = matrix . mean ( axis = 1 ) experiments_coffe_cups_df = pd . DataFrame ( experiments_coffe_cups , columns = samples ) fig , ax = plt . subplots ( 2 , 3 , figsize = ( 20 , 10 )) for index , column in enumerate ( experiments_coffe_cups_df . columns ): i = int ( index / 3 ) j = index % 3 sns . distplot ( experiments_coffe_cups_df [ column ], ax = ax [ i ][ j ]) ax [ i ][ j ] . set_title ( 'M\u00e9dia amostral com {} amostras' . format ( column )) ax [ i ][ j ] . set_ylabel ( 'Frequ\u00eancia' ) fig . suptitle ( 'Histogramas com diferentes n\u00fameros de amostras' ) plt . show () M\u00e9todo Delta Seja Y_1, Y_2, \\dots uma sequ\u00eancia de v.a. e F uma fun\u00e7\u00e3o de densidade acumulada cont\u00ednua. Sejam \\theta \\in \\mathbb{R} e \\{a_n\\}_{n\\in\\mathbb{N}} que tende ao \\infty . Suponha que a_n(Y_n - \\theta) converge para F . Seja \\alpha uma fun\u00e7\u00e3o com derivada cont\u00ednua, tal que \\alpha '(\\theta) \\neq 0 . Ent\u00e3o a_n[\\alpha(Y_n) - \\alpha(\\theta)]/\\alpha '(\\theta) converge para a distribui\u00e7\u00e3o F . Teorema de Slutsky \\begin{align*} {X}^{(n)}& \\overset{d}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{d}{\\to} X,\\\\ {X}^{(n)}& \\overset{p}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{p}{\\to} X,\\\\ {X}^{(n)}& \\overset{as}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{as}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{as}{\\to} X. \\end{align*} Corol\u00e1rio Se f \u00e9 uma fun\u00e7\u00e3o cont\u00ednua: {X}^{(n)}\\overset{d}{\\to} X \\quad \\text{ e }\\quad {Y}^{(n)}\\overset{p}{\\to} c\\quad \\text{implica}\\quad f ({X}^{(n)},{Y}^{(n)}) \\overset{d}{\\to} f(X,c). Aproxima\u00e7\u00e3o de Taylor e M\u00e9todo Delta Refer\u00eancia de Probabilidade","title":"Grandes Amostras"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#grandes-amostras","text":"","title":"Grandes Amostras"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#desigualdade-de-markov","text":"P(X \\geq t) \\leq \\frac{E[X^n]}{t^n}, ~dado~que~P(X \\geq 0) = 1, t > 0","title":"Desigualdade de Markov"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#desigualdade-de-chebyshev","text":"Seja X uma vari\u00e1vel aleat\u00f3ria em que o segundo momento \u00e9 finito. Ent\u00e3o, \\forall t > 0 . P(|X - E[X]| \\geq t) \\leq \\frac{Var[X]}{t^2}","title":"Desigualdade de Chebyshev"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#propriedades-importantes","text":"X_1, ..., X_n amostra aleat\u00f3ria (por defini\u00e7\u00e3o mesma distribui\u00e7\u00e3o e independentes), com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 . Ent\u00e3o E[\\bar{X_n}] = \\mu e Var[\\bar{X_n}] = \\sigma^2/n .","title":"Propriedades Importantes"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#importando-bibliotecas","text":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set ()","title":"Importando bibliotecas"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#exemplo-622","text":"Um engenheiro ambiental acredita que existam dois contaminantes na \u00e1gua: ars\u00eanico e chumbo. Suponha que ambas s\u00e3o vari\u00e1veis aleat\u00f3rias independentes X e Y , medidas na mesma unidade. O engenheiro est\u00e1 interessado em saber a propor\u00e7\u00e3o de contamina\u00e7\u00e3o por chumbo em m\u00e9dia, isto \u00e9, E[R] = E[Y/(X + Y)] . Como nem sempre conhecemos a distribui\u00e7\u00e3o de R, podemos aproximar o valor esperado atrav\u00e9s de uma m\u00e9dia amostral de R , atrav\u00e9s de observa\u00e7\u00f5es (X_1, Y_1), ..., (X_n, Y_n) . Usando a desigualdade de Chebyshev, (tente ver que Var[R] \\leq 1 ). P(|\\bar{R_n} - E[R]| \\geq \\epsilon) \\leq \\frac{1}{n\\epsilon^2} # Usando apenas Chebyshev: epsilon = 0.0005 prob = 0.95 # probabilidade m\u00ednima de que a diferen\u00e7a entre a # m\u00e9dia amostral e o valor esperado seja menor do que epsilon def get_number_simulations ( epsilon , prob ): assert prob >= 0 assert prob <= 1 # Queremos que P <= 1 - prob-> 1/(n*eps**2) <= 1 - prob min_n = 1 / (( 1 - prob ) * ( epsilon ** 2 )) min_n = np . ceil ( min_n ) print ( '----------------------------------------------------------------------' ) print ( 'In order to have the sample mean at least {} close with probability {}: ' . format ( epsilon , prob )) print ( 'The minimum number of simulations are: {}' . format ( int ( min_n ))) print ( '----------------------------------------------------------------------' ) return min_n def get_epsilon ( prob , n ): assert prob >= 0 assert prob <= 1 # Queremos encontrar epsilon para que 1/(n*eps**2) = 1 - prob eps = np . sqrt ( 1 / ( n * ( 1 - prob ))) return eps _ = get_number_simulations ( epsilon , prob ) ---------------------------------------------------------------------- In order to have the sample mean at least 0.0005 close with probability 0.95: The minimum number of simulations are: 80000000 ---------------------------------------------------------------------- # Testando com distribui\u00e7\u00e3o uniforme (X e Y tem distribui\u00e7\u00f5es uniformes) # Nesse caso, podemos provar que E[R] = 0.5 probs = [ 0.6 , 0.75 , 0.9 , 0.95 , 0.99 ] n_range = np . array ([ j * 10 ** i for i in [ 2 , 3 , 4 , 5 , 6 , 7 ] for j in [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]]) E_R = [] for n in n_range : X = np . random . uniform ( 0 , 1 , size = int ( n )) #np = numpy Y = np . random . uniform ( 0 , 1 , size = int ( n )) R = Y / ( X + Y ) E_R . append ( np . mean ( R )) # E[R] = 0.5 chebyshev_interval = np . empty ( shape = ( len ( probs ), len ( n_range ))) for i , prob in enumerate ( probs ): chebyshev_interval [ i , :] = get_epsilon ( prob , n_range ) # Plotando fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) for i in [ 0 , 1 ]: ax [ i ] . plot ( n_range , E_R , color = 'darkred' ) ax [ i ] . hlines ( 0.5 , xmin = min ( n_range ), xmax = max ( n_range ), linestyle = '--' , alpha = 0.4 , color = 'black' ) ax [ i ] . set_xscale ( 'log' ) colors = [ 'black' , 'red' , 'green' , 'blue' , 'pink' ] for i in range ( len ( probs )): ax [ 1 ] . fill_between ( x = n_range , y1 = 0.5 + chebyshev_interval [ i ,:], y2 = 0.5 - chebyshev_interval [ i ,:], color = colors [ i ], alpha = 0.3 + 0.5 * ( len ( probs ) - i ) / ( len ( probs )), label = probs [ i ]) ax [ 1 ] . legend () ax [ 0 ] . set_title ( 'Different mean samples' , fontsize = 15 ) ax [ 1 ] . set_title ( 'Chebyshev bound with prob' , fontsize = 15 ) plt . show ()","title":"Exemplo 6.2.2"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#lei-dos-grandes-numeros","text":"","title":"Lei dos Grandes N\u00fameros"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#convergencia-em-probabilidade","text":"\\forall \\epsilon > 0, \\lim_{n\\to\\infty} P[|Z_n - b| < \\epsilon] = 1 \\iff Z_n \\overset{p}{\\to} b","title":"Converg\u00eancia em Probabilidade"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#convergencia-quase-certa-implica-a-anterior","text":"P[\\lim_{n_\\to\\infty} Z_n = b] = 1","title":"Converg\u00eancia quase certa (Implica a anterior)"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#versao-fraca","text":"X_1, \\dots, X_n amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o com m\u00e9dia \\mu e vari\u00e2ncia vinita. Se \\bar{X}_n \u00e9 a m\u00e9dia amostral. Ent\u00e3o \\bar{X}_n \\overset{p}{\\to} \\mu .","title":"Vers\u00e3o Fraca"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#versao-forte","text":"P[\\lim_{n\\to\\infty} \\bar{X}_n = \\mu] = 1","title":"Vers\u00e3o Forte"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#histogramas","text":"S\u00e3o usados para aproximar uma fun\u00e7\u00e3o de densidade de probabilidade de forma discreta. Seja X_1, X_2, \\dots vari\u00e1veis aleat\u00f3rias iid. Seja c_1 < c_2 constantes. Seja Y_i uma indicadora para c_1\\leq X_i < c_2 . Ent\u00e3o \\bar{Y}_n (propor\u00e7\u00e3o de valores X_1, ..., X_n no intervalo [c_1, c_2) e \\bar{Y}_n \\overset{p}{\\to} P[c_1 \\leq X_1 < c_2] . # Exemplo 6.2.4 lamda = 0.5 # N\u00e3o posso usar lambda beta = 1 / lamda # Numpy usa esse par\u00e2metro t = np . arange ( 0.0001 , 15 , 0.01 ) X_true = lamda * np . exp ( - lamda * t ) fig , ax = plt . subplots ( figsize = ( 14 , 5 )) for n in [ 1 , 10 , 100 , 1000 , 10000 ]: X = np . random . exponential ( scale = beta , size = n ) sns . distplot ( X , ax = ax , kde = False , norm_hist = True , label = 'n=' + str ( n )) # area = 1 sns . lineplot ( t , X_true , ax = ax , lw = 3 ) ax . set_title ( 'Histograma da Distribui\u00e7\u00e3o Exponencial' ) ax . legend () plt . show ()","title":"Histogramas"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#teorema-central-do-limite","text":"Se as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n formam uma amostra aleat\u00f3ria de tamanho n para uma dada distribui\u00e7\u00e3o de m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 finita, ent\u00e3o para cada n\u00famero x , lim_{n\\to\\infty} P[\\frac{\\bar{X}_n - \\mu}{\\sigma/n^{1/2}} \\leq x] = \\Phi(x), onde \\Phi \u00e9 a fun\u00e7\u00e3o de densidade acumulada da distribui\u00e7\u00e3o normal!!! coffee_df = pd . read_csv ( '../data/CoffeeAndCode.csv' ) display ( coffee_df . head ()) display ( coffee_df . shape ) display ( coffee_df . describe ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay CoffeeTime CodingWithoutCoffee CoffeeType CoffeeSolveBugs Gender Country AgeRange 0 8 2 Before coding Yes Caff\u00e8 latte Sometimes Female Lebanon 18 to 29 1 3 2 Before coding Yes Americano Yes Female Lebanon 30 to 39 2 5 3 While coding No Nescafe Yes Female Lebanon 18 to 29 3 8 2 Before coding No Nescafe Yes Male Lebanon NaN 4 10 3 While coding Sometimes Turkish No Male Lebanon 18 to 29 (100, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay count 100.000000 100.000000 mean 6.410000 2.890000 std 2.644205 1.613673 min 1.000000 1.000000 25% 4.000000 2.000000 50% 7.000000 2.500000 75% 8.000000 4.000000 max 10.000000 8.000000 # Plotting fig , ax = plt . subplots ( figsize = ( 7 , 5 )) sns . distplot ( coffee_df . CoffeeCupsPerDay , ax = ax ) ax . vlines ( coffee_df . CoffeeCupsPerDay . mean (), ymin = 0 , ymax = 1 , linestyle = '--' , color = 'black' , label = 'black' ) ax . annotate ( 'M\u00e9dia:' + str ( coffee_df . CoffeeCupsPerDay . mean ()), ( coffee_df . CoffeeCupsPerDay . mean () + 0.5 , 0.4 )) ax . set_title ( 'Histrograma de copos de caf\u00e9 bebidos por programadores' ) ax . set_ylabel ( 'Frequ\u00eancia' ) ax . set_ylim (( 0 , 0.45 )) plt . show () # Generating sample means samples = [ 10 , 50 , 150 , 300 , 500 , 1000 ] n_experiments = 500 experiments_coffe_cups = np . empty (( n_experiments , len ( samples ))) for j , sample_size in enumerate ( samples ): sample = coffee_df . CoffeeCupsPerDay . sample ( n = sample_size * n_experiments , replace = True ) matrix = np . array ( sample ) . reshape (( n_experiments , sample_size )) experiments_coffe_cups [:, j ] = matrix . mean ( axis = 1 ) experiments_coffe_cups_df = pd . DataFrame ( experiments_coffe_cups , columns = samples ) fig , ax = plt . subplots ( 2 , 3 , figsize = ( 20 , 10 )) for index , column in enumerate ( experiments_coffe_cups_df . columns ): i = int ( index / 3 ) j = index % 3 sns . distplot ( experiments_coffe_cups_df [ column ], ax = ax [ i ][ j ]) ax [ i ][ j ] . set_title ( 'M\u00e9dia amostral com {} amostras' . format ( column )) ax [ i ][ j ] . set_ylabel ( 'Frequ\u00eancia' ) fig . suptitle ( 'Histogramas com diferentes n\u00fameros de amostras' ) plt . show ()","title":"Teorema Central do Limite"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#metodo-delta","text":"Seja Y_1, Y_2, \\dots uma sequ\u00eancia de v.a. e F uma fun\u00e7\u00e3o de densidade acumulada cont\u00ednua. Sejam \\theta \\in \\mathbb{R} e \\{a_n\\}_{n\\in\\mathbb{N}} que tende ao \\infty . Suponha que a_n(Y_n - \\theta) converge para F . Seja \\alpha uma fun\u00e7\u00e3o com derivada cont\u00ednua, tal que \\alpha '(\\theta) \\neq 0 . Ent\u00e3o a_n[\\alpha(Y_n) - \\alpha(\\theta)]/\\alpha '(\\theta) converge para a distribui\u00e7\u00e3o F .","title":"M\u00e9todo Delta"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#teorema-de-slutsky","text":"\\begin{align*} {X}^{(n)}& \\overset{d}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{d}{\\to} X,\\\\ {X}^{(n)}& \\overset{p}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{p}{\\to} X,\\\\ {X}^{(n)}& \\overset{as}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{as}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{as}{\\to} X. \\end{align*}","title":"Teorema de Slutsky"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#corolario","text":"Se f \u00e9 uma fun\u00e7\u00e3o cont\u00ednua: {X}^{(n)}\\overset{d}{\\to} X \\quad \\text{ e }\\quad {Y}^{(n)}\\overset{p}{\\to} c\\quad \\text{implica}\\quad f ({X}^{(n)},{Y}^{(n)}) \\overset{d}{\\to} f(X,c). Aproxima\u00e7\u00e3o de Taylor e M\u00e9todo Delta Refer\u00eancia de Probabilidade","title":"Corol\u00e1rio"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/","text":"Estimador de M\u00e1xima Verossimilhan\u00e7a Introdu\u00e7\u00e3o \"Tradicionalmente a infer\u00eancia estat\u00edstica sobre a m\u00e9dia de uma popula\u00e7\u00e3o se apoia no Teorema Central do Limite para construir Intervalos de Confian\u00e7a ou testar hip\u00f3teses sobre o valor do par\u00e2metro. Esta abordagem da estat\u00edstica tradicional pode ser extendida para infer\u00eancias a respeito de qualquer par\u00e2metro, n\u00e3o s\u00f3 a m\u00e9dia. Da mesma forma que no caso da m\u00e9dia populacional se usa a distribui\u00e7\u00e3o t-Student ou a distribui\u00e7\u00e3o Normal Padr\u00e3o , no caso de outros par\u00e2metros se utiliza outras distribui\u00e7\u00f5es amostrais. Essas distribui\u00e7\u00f5es s\u00e3o chamadas amostrais porque representam o comportamento das estimativas baseado na repeti\u00e7\u00e3o incont\u00e1vel do processo de amostragem . Na pr\u00e1tica cient\u00edfica, no entanto, sempre se realiza uma \u00fanica amostragem , o que resulta em uma \u00fanica amostra. Assim, o conceito de distribui\u00e7\u00e3o amostral \u00e9 at\u00e9 certo ponto artificial, pois em pesquisa cient\u00edfica n\u00e3o raciocinamos em termos de repeti\u00e7\u00f5es incont\u00e1veis de experimentos ou processos de observa\u00e7\u00e3o . O resultado disto \u00e9 que o conceito de teste estat\u00edstico de hip\u00f3tese e de intervalo de confian\u00e7a s\u00e3o frequentemente mal compreendidos. O desenvolvimento da infer\u00eancia estat\u00edstica a partir do conceito de verossimilhan\u00e7a tem sido utilizado como uma alternativa \u00e0 abordagem estat\u00edstica frequentista e, segundo alguns autores (como por exemplo Royall, 1997), \u00e9 mais coerente com a pr\u00e1tica cient\u00edfica .\" (Batista, 2009) Site de Refer\u00eancia Fun\u00e7\u00e3o Verossimilhan\u00e7a Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x) Estimador de M\u00e1xima Verossimilhan\u00e7a (MLE) Para cada observa\u00e7\u00e3o x , seja \\delta(x) um valor de \\theta \\in \\Omega tal que a fun\u00e7\u00e3o de verossimilhna\u00e7a seja m\u00e1xima . Defina \\hat{\\theta} = \\delta(X) o estimador. \u00c9 importante observar que o m\u00e1ximo dessa fun\u00e7\u00e3o pode n\u00e3o estar em um ponto de \\Omega . Nesse caso, MLE n\u00e3o existe. Ele pode n\u00e3o estar unicamente definido, tamb\u00e9m. Limita\u00e7\u00f5es N\u00e3o exist\u00eancia em todos os casos, isso depende muito da fun\u00e7\u00e3o e do espa\u00e7o dos par\u00e2metros. N\u00e3o unicidade em todos os casos. N\u00e3o podemos interpretar MLE como o par\u00e2metro mais prov\u00e1vel, pois ter\u00edamos que ter um espa\u00e7o de probabilidade associado ao par\u00e2metro, o que n\u00e3o \u00e9 dado. Implementa\u00e7\u00e3o Como refer\u00eancia, estou utilizando este site . # importando bibliotecas import numpy as np , pandas as pd from matplotlib import pyplot as plt import seaborn as sns from scipy.optimize import minimize import scipy.stats as stats import pymc3 as pm3 import numdifftools as ndt import statsmodels.api as sm from statsmodels.base.model import GenericLikelihoodModel % matplotlib inline # Gerando os dados N = 100 x = np . linspace ( 0 , 20 , N ) # gerando lista igualmente espa\u00e7ada beta1 = 3 beta0 = 0 sigma = 5 error = np . random . normal ( 0 , sigma , size = N ) y = beta1 * x + beta0 + error data = pd . DataFrame ({ 'y' : y , 'x' : x }) data [ 'constant' ] = 1 sns . regplot ( 'x' , 'y' , data = data ) # Essa reta \u00e9 uma estimativa dos dados feito por seaborn plt . title ( 'Dados' ) plt . show () Y = \\beta_1 x + \\beta_0 + e Nesse exemplo, o nosso problema ser\u00e1 estimar a m\u00e9dia. Observe que os dados tem um comportamento linear. Sem nos concentrarmos muito na modelagem e os problemas que ela pode trazer, eu vou j\u00e1 supor que temos um problema de Regress\u00e3o Linear , onde os dados Y \\sim N(\\mu, \\sigma^2) , onde \\sigma^2 \u00e9 a vari\u00e2ncia do erro no processo, e \\mu = \\beta_0 + \\beta_1 x , isto \u00e9, depende de x, nesse caso. Essa \u00e9 uma dificuldade, as contas ficam mais dif\u00edceis e, por isso, vamos usar asrtif\u00edcios computacionais. Vamos supor que a vari\u00e2ncia \u00e9 conhecida . Al\u00e9m disso, vamos supor que temos uma amostra aleat\u00f3ria Y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2) Temos que a verossimilhan\u00e7a \u00e9 produto das pdfs(distribui\u00e7\u00e3o de densidade de probabilidade). Para otimizar podemos, entretanto, obter a soma dos logaritmos das pdfs . E por fim, vamos resolver um problema de minimizar o negativo desse valor. Veja que \u00e9 equivalente a maximixar a soma!! # Fun\u00e7\u00e3o de verossimilhan\u00e7a. Chamamos de Fun\u00e7\u00e3o de Perda def MLE ( params ): # Fun\u00e7\u00e3o Perda: - log-verossimilhan\u00e7a beta0 , beta1 = params [ 0 ], params [ 1 ] # Modelo Linear yhat = beta0 + beta1 * x #= mu #loc \u00e9 a m\u00e9dia e scale desvio padr\u00e3o. Note que sigma \u00e9 conhecido negLikelihood = - np . sum ( stats . norm . logpdf ( y , loc = yhat , scale = sigma )) return negLikelihood # Esse \u00e9 o chute inicial initial_guess = np . array ([ 3 , 6 ]) results = minimize ( MLE , initial_guess , method = 'Nelder-Mead' , options = { 'disp' : True }) Optimization terminated successfully. Current function value: 307.745486 Iterations: 56 Function evaluations: 107 print ( results ) final_simplex: (array([[-1.03428809, 3.11012856], [-1.0342294 , 3.110121 ], [-1.03433677, 3.11012912]]), array([293.95399071, 293.95399071, 293.95399071])) fun: 293.95399070678394 message: 'Optimization terminated successfully.' nfev: 103 nit: 55 status: 0 success: True x: array([-1.03428809, 3.11012856]) resultsdf = pd . DataFrame ({ 'coef' : results [ 'x' ]}) resultsdf . index = [ r '$\\beta_0$' , r '$\\beta_1$' ] np . round ( resultsdf . head ( 2 ), 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef $\\beta_0$ -1.0343 $\\beta_1$ 3.1101 Vamos estimar usando a biblioteca OLS. Ela faz esse processo e muito mais internamente. results_ols = sm . OLS ( data . y , data [[ 'constant' , 'x' ]]) . fit () results_ols . summary () OLS Regression Results Dep. Variable: y R-squared: 0.941 Model: OLS Adj. R-squared: 0.941 Method: Least Squares F-statistic: 1568. Date: Wed, 26 Aug 2020 Prob (F-statistic): 4.22e-62 Time: 21:20:55 Log-Likelihood: -293.06 No. Observations: 100 AIC: 590.1 Df Residuals: 98 BIC: 595.3 Df Model: 1 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] constant -1.0343 0.909 -1.138 0.258 -2.839 0.770 x 3.1101 0.079 39.599 0.000 2.954 3.266 Omnibus: 1.778 Durbin-Watson: 2.306 Prob(Omnibus): 0.411 Jarque-Bera (JB): 1.423 Skew: -0.289 Prob(JB): 0.491 Kurtosis: 3.084 Cond. No. 23.1 Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Veja que a estima\u00e7\u00e3o dos coeficientes foi a mesma! Apesar de ambas estarem erradas p para \\beta_0 . Na verdade se olharmos o intervalo de confian\u00e7a que OLS nos d\u00e1, vemos que de fato 0 est\u00e1 nele. Mas ainda n\u00e3o esta na hora de voc\u00eas verem isso! Conclus\u00e3o Podemos usar uma fun\u00e7\u00e3o de perda (que no caso ser\u00e1 menos a log-verossimilhan\u00e7a) e usar um algoritmo de otimiza\u00e7\u00e3o! Propriedades Invari\u00e2ncia Se \\hat{\\theta} \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de \\theta e g \u00e9 uma fun\u00e7\u00e3o injetiva, ent\u00e3o g(\\hat{\\theta}) \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de g(\\theta) . Na verdade, podemos retirar condi\u00e7\u00e3o de injetividade. MLE de uma Fun\u00e7\u00e3o Seja g(\\theta) uma fun\u00e7\u00e3o arbitr\u00e1ria do par\u00e2metro e G = g(\\Omega) . Para cada t \\in G , definimos G_t := \\{\\theta : g(\\theta) = t\\} e L^*(t) := \\max_{\\theta \\in G_t} log f_n(x|\\theta) Definimos a ML.E.de g(\\theta) := arg\\,max_{t\\in G} L^*(t) Teorema Seja \\hat{\\theta} MLE de \\theta e g(\\theta) fun\u00e7\u00e3o de \\theta . Ent\u00e3o uma MLE de g(\\theta) \u00e9 g(\\hat{\\theta}) . Consist\u00eancia Suponha que para uma amostra suficientemente grantde, existe um MLE \u00fanico para \\theta . Ent\u00e3o, sob algumas condi\u00e7\u00f5es, a sequ\u00eancia de MLE \u00e9 uma sequ\u00eancia consistente de estimadores de \\theta . A seuq\u00eancia convergee em probabilidade para o valor desconhecido de \\theta . O mesmo acontece com o Estimador de Bayes, dadas condi\u00e7\u00f5es de regularidade. Fun\u00e7\u00e3o Digamma: \\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)} M\u00e9todo dos Momentos Assuma que a amostra aleat\u00f3ria X_1,...,X_n vem da distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta k-dimensional. Por exemplo, a distribui\u00e7\u00e3o normal tem k = 2 . Tamb\u00e9m suponha que pelo menos os k primeiros momentos ( E[X_i^k] < \\infty ) sejam finitos. Defina \\mu_j(\\theta) = E[X_1^j|\\theta], j = 1,...k . Suponha que a fun\u00e7\u00e3o: \\begin{split} \\mu : ~&\\Omega \\to \\mathbb{R}^k \\\\ &\\theta \\mapsto \\mu(\\theta) = (\\mu_1(\\theta), ..., \\mu_k(\\theta)), \\end{split} \u00e9 injetiva em \\theta . Seja M(\\mu_1,...,\\mu_k) a fun\u00e7\u00e3o inversa, isto \u00e9, \\theta = M(\\mu_1,...,\\mu_k) O m\u00e9todo dos momentos ser\u00e1 M(m_1,...,m_j) , onde m_j = \\frac{1}{n}\\sum_{i=1}^n X_i^j, j = 1,...,k De forma mais simplificada, basta que sesolvemos o sistema: m_j = \\mu_j(\\theta), isto \u00e9, os momentos amostrais iguais aos momentos da amostra, condicionados em \\theta . Teorema Suponha que \\{X_n\\}_{n\\in\\mathbb{N}} i.i.d com distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta , k -dimensional. Suponha que os primeiros k momentos existem e s\u00e3o finitos para todo \\theta . Suponha que a inversa M definida acima \u00e9 cont\u00ednua. Ent\u00e3o a sequ\u00eancia de estimadores do m\u00e9todo de momentos em X_1,...,X_n \u00e9 consistente. M.L.E e Estimador de Bayes Se tivermos condi\u00e7\u00f5es de suavidade em f(x|\\theta) , podemos provar que quando n \\to \\infty , teremos que: L(\\theta|x) \\to c(x)\\cdot \\exp\\{-\\frac{1}{2V_n(\\theta)/n}(\\theta - \\hat{\\theta})^2\\}, onde \\hat{\\theta} \u00e9 MLE e V_n(\\theta) \u00e9 uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias convergente. No caso de termos uma priori relativamente flat, a posteriori ser\u00e1 aproximadamente uma distribui\u00e7\u00e3o normal com m\u00e9dia \\hat{\\theta} e vari\u00e2ncia V_n(\\hat{\\theta})/n . Exemplo 7.6.12 (Mortes ex\u00e9rcito pr\u00fassio) Bortkiewicz contou o n\u00famero de soldados mortos por horsekick em 14 unidades do ex\u00e9rcito em 20 anos, com 280 contagens ao total. Das contagens temos Valor 0 1 2 3 4 Total Contagem 144 91 32 11 2 280 Modelamos X_1, ..., X_{280} como uma vari\u00e1vel de contagem. Considere a distribui\u00e7\u00e3o Poisson(\\theta) . Escolhemos a distribui\u00e7\u00e3o Gamma(\\alpha,\\beta) , dada que ela pertence \u00e0 familia conjungada. Em particular, a distribui\u00e7\u00e3o a posteriori ser\u00e1 Gamma(\\alpha + \\sum X_i, \\beta + n) , onde \\sum X_i = 196 . Se assumirmos \\alpha inteiro por simplicidade, vemos que a distribui\u00e7\u00e3o Gamma pode ser vista como a soma de \\alpha + \\sum X_i distribui\u00e7\u00f5es Exponencial(\\beta + n) . Logo a soma dessas vari\u00e1veis ser\u00e1 aproximadamente normal com m\u00e9dia 196/280 e vari\u00e2ncia 196/280^2 . import numpy as np import matplotlib.pyplot as plt from scipy.stats import gamma alpha = 1 beta = 1 # Esse \u00e9 o MLE, a m\u00e9dia. Vou supor que esse \u00e9 o par\u00e2metro verdadeiro s\u00f3 para mostrar. theta = 196 / 280 sum_xi = 196 fig , ax = plt . subplots ( 2 , 3 , figsize = ( 18 , 6 )) fig . suptitle ( 'Avaliando a converg\u00eancia da distribui\u00e7\u00e3o Gamma' ) for index , n in enumerate ([ 1 , 10 , 100 , 1000 , 10000 , 280 ]): i = int ( index / 3 ) j = index % 3 X = np . random . poisson ( theta , size = n ) if n != 280 : T = X . sum () ax [ i ][ j ] . set_title ( 'n = {}' . format ( n )) else : T = sum_xi #Valor dos dados ax [ i ][ j ] . set_title ( 'Dados Oficiais: n = {}' . format ( n )) t = np . linspace ( start = 0.00001 , stop = 3 - i - 1 , num = 1000 ) posteriori = gamma ( alpha + T , scale = 1 / ( beta + n )) y = posteriori . pdf ( t ) ax [ i ][ j ] . plot ( t , y , color = 'darkblue' ) ax [ i ][ j ] . grid ( color = 'grey' , alpha = 0.6 , linestyle = '--' ) ax [ i ][ j ] . vlines ( theta , ymin = 0 , ymax = max ( y ), color = 'black' , linestyle = '--' ) Veja que com os dados reais, j\u00e1 temos uma boa aproxima\u00e7\u00e3o!","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#estimador-de-maxima-verossimilhanca","text":"","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#introducao","text":"\"Tradicionalmente a infer\u00eancia estat\u00edstica sobre a m\u00e9dia de uma popula\u00e7\u00e3o se apoia no Teorema Central do Limite para construir Intervalos de Confian\u00e7a ou testar hip\u00f3teses sobre o valor do par\u00e2metro. Esta abordagem da estat\u00edstica tradicional pode ser extendida para infer\u00eancias a respeito de qualquer par\u00e2metro, n\u00e3o s\u00f3 a m\u00e9dia. Da mesma forma que no caso da m\u00e9dia populacional se usa a distribui\u00e7\u00e3o t-Student ou a distribui\u00e7\u00e3o Normal Padr\u00e3o , no caso de outros par\u00e2metros se utiliza outras distribui\u00e7\u00f5es amostrais. Essas distribui\u00e7\u00f5es s\u00e3o chamadas amostrais porque representam o comportamento das estimativas baseado na repeti\u00e7\u00e3o incont\u00e1vel do processo de amostragem . Na pr\u00e1tica cient\u00edfica, no entanto, sempre se realiza uma \u00fanica amostragem , o que resulta em uma \u00fanica amostra. Assim, o conceito de distribui\u00e7\u00e3o amostral \u00e9 at\u00e9 certo ponto artificial, pois em pesquisa cient\u00edfica n\u00e3o raciocinamos em termos de repeti\u00e7\u00f5es incont\u00e1veis de experimentos ou processos de observa\u00e7\u00e3o . O resultado disto \u00e9 que o conceito de teste estat\u00edstico de hip\u00f3tese e de intervalo de confian\u00e7a s\u00e3o frequentemente mal compreendidos. O desenvolvimento da infer\u00eancia estat\u00edstica a partir do conceito de verossimilhan\u00e7a tem sido utilizado como uma alternativa \u00e0 abordagem estat\u00edstica frequentista e, segundo alguns autores (como por exemplo Royall, 1997), \u00e9 mais coerente com a pr\u00e1tica cient\u00edfica .\" (Batista, 2009) Site de Refer\u00eancia","title":"Introdu\u00e7\u00e3o"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#funcao-verossimilhanca","text":"Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x)","title":"Fun\u00e7\u00e3o Verossimilhan\u00e7a"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#estimador-de-maxima-verossimilhanca-mle","text":"Para cada observa\u00e7\u00e3o x , seja \\delta(x) um valor de \\theta \\in \\Omega tal que a fun\u00e7\u00e3o de verossimilhna\u00e7a seja m\u00e1xima . Defina \\hat{\\theta} = \\delta(X) o estimador. \u00c9 importante observar que o m\u00e1ximo dessa fun\u00e7\u00e3o pode n\u00e3o estar em um ponto de \\Omega . Nesse caso, MLE n\u00e3o existe. Ele pode n\u00e3o estar unicamente definido, tamb\u00e9m.","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a (MLE)"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#limitacoes","text":"N\u00e3o exist\u00eancia em todos os casos, isso depende muito da fun\u00e7\u00e3o e do espa\u00e7o dos par\u00e2metros. N\u00e3o unicidade em todos os casos. N\u00e3o podemos interpretar MLE como o par\u00e2metro mais prov\u00e1vel, pois ter\u00edamos que ter um espa\u00e7o de probabilidade associado ao par\u00e2metro, o que n\u00e3o \u00e9 dado.","title":"Limita\u00e7\u00f5es"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#implementacao","text":"Como refer\u00eancia, estou utilizando este site . # importando bibliotecas import numpy as np , pandas as pd from matplotlib import pyplot as plt import seaborn as sns from scipy.optimize import minimize import scipy.stats as stats import pymc3 as pm3 import numdifftools as ndt import statsmodels.api as sm from statsmodels.base.model import GenericLikelihoodModel % matplotlib inline # Gerando os dados N = 100 x = np . linspace ( 0 , 20 , N ) # gerando lista igualmente espa\u00e7ada beta1 = 3 beta0 = 0 sigma = 5 error = np . random . normal ( 0 , sigma , size = N ) y = beta1 * x + beta0 + error data = pd . DataFrame ({ 'y' : y , 'x' : x }) data [ 'constant' ] = 1 sns . regplot ( 'x' , 'y' , data = data ) # Essa reta \u00e9 uma estimativa dos dados feito por seaborn plt . title ( 'Dados' ) plt . show () Y = \\beta_1 x + \\beta_0 + e Nesse exemplo, o nosso problema ser\u00e1 estimar a m\u00e9dia. Observe que os dados tem um comportamento linear. Sem nos concentrarmos muito na modelagem e os problemas que ela pode trazer, eu vou j\u00e1 supor que temos um problema de Regress\u00e3o Linear , onde os dados Y \\sim N(\\mu, \\sigma^2) , onde \\sigma^2 \u00e9 a vari\u00e2ncia do erro no processo, e \\mu = \\beta_0 + \\beta_1 x , isto \u00e9, depende de x, nesse caso. Essa \u00e9 uma dificuldade, as contas ficam mais dif\u00edceis e, por isso, vamos usar asrtif\u00edcios computacionais. Vamos supor que a vari\u00e2ncia \u00e9 conhecida . Al\u00e9m disso, vamos supor que temos uma amostra aleat\u00f3ria Y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2) Temos que a verossimilhan\u00e7a \u00e9 produto das pdfs(distribui\u00e7\u00e3o de densidade de probabilidade). Para otimizar podemos, entretanto, obter a soma dos logaritmos das pdfs . E por fim, vamos resolver um problema de minimizar o negativo desse valor. Veja que \u00e9 equivalente a maximixar a soma!! # Fun\u00e7\u00e3o de verossimilhan\u00e7a. Chamamos de Fun\u00e7\u00e3o de Perda def MLE ( params ): # Fun\u00e7\u00e3o Perda: - log-verossimilhan\u00e7a beta0 , beta1 = params [ 0 ], params [ 1 ] # Modelo Linear yhat = beta0 + beta1 * x #= mu #loc \u00e9 a m\u00e9dia e scale desvio padr\u00e3o. Note que sigma \u00e9 conhecido negLikelihood = - np . sum ( stats . norm . logpdf ( y , loc = yhat , scale = sigma )) return negLikelihood # Esse \u00e9 o chute inicial initial_guess = np . array ([ 3 , 6 ]) results = minimize ( MLE , initial_guess , method = 'Nelder-Mead' , options = { 'disp' : True }) Optimization terminated successfully. Current function value: 307.745486 Iterations: 56 Function evaluations: 107 print ( results ) final_simplex: (array([[-1.03428809, 3.11012856], [-1.0342294 , 3.110121 ], [-1.03433677, 3.11012912]]), array([293.95399071, 293.95399071, 293.95399071])) fun: 293.95399070678394 message: 'Optimization terminated successfully.' nfev: 103 nit: 55 status: 0 success: True x: array([-1.03428809, 3.11012856]) resultsdf = pd . DataFrame ({ 'coef' : results [ 'x' ]}) resultsdf . index = [ r '$\\beta_0$' , r '$\\beta_1$' ] np . round ( resultsdf . head ( 2 ), 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef $\\beta_0$ -1.0343 $\\beta_1$ 3.1101 Vamos estimar usando a biblioteca OLS. Ela faz esse processo e muito mais internamente. results_ols = sm . OLS ( data . y , data [[ 'constant' , 'x' ]]) . fit () results_ols . summary () OLS Regression Results Dep. Variable: y R-squared: 0.941 Model: OLS Adj. R-squared: 0.941 Method: Least Squares F-statistic: 1568. Date: Wed, 26 Aug 2020 Prob (F-statistic): 4.22e-62 Time: 21:20:55 Log-Likelihood: -293.06 No. Observations: 100 AIC: 590.1 Df Residuals: 98 BIC: 595.3 Df Model: 1 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] constant -1.0343 0.909 -1.138 0.258 -2.839 0.770 x 3.1101 0.079 39.599 0.000 2.954 3.266 Omnibus: 1.778 Durbin-Watson: 2.306 Prob(Omnibus): 0.411 Jarque-Bera (JB): 1.423 Skew: -0.289 Prob(JB): 0.491 Kurtosis: 3.084 Cond. No. 23.1 Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Veja que a estima\u00e7\u00e3o dos coeficientes foi a mesma! Apesar de ambas estarem erradas p para \\beta_0 . Na verdade se olharmos o intervalo de confian\u00e7a que OLS nos d\u00e1, vemos que de fato 0 est\u00e1 nele. Mas ainda n\u00e3o esta na hora de voc\u00eas verem isso!","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#conclusao","text":"Podemos usar uma fun\u00e7\u00e3o de perda (que no caso ser\u00e1 menos a log-verossimilhan\u00e7a) e usar um algoritmo de otimiza\u00e7\u00e3o!","title":"Conclus\u00e3o"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#propriedades","text":"","title":"Propriedades"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#invariancia","text":"Se \\hat{\\theta} \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de \\theta e g \u00e9 uma fun\u00e7\u00e3o injetiva, ent\u00e3o g(\\hat{\\theta}) \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de g(\\theta) . Na verdade, podemos retirar condi\u00e7\u00e3o de injetividade.","title":"Invari\u00e2ncia"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#mle-de-uma-funcao","text":"Seja g(\\theta) uma fun\u00e7\u00e3o arbitr\u00e1ria do par\u00e2metro e G = g(\\Omega) . Para cada t \\in G , definimos G_t := \\{\\theta : g(\\theta) = t\\} e L^*(t) := \\max_{\\theta \\in G_t} log f_n(x|\\theta) Definimos a ML.E.de g(\\theta) := arg\\,max_{t\\in G} L^*(t)","title":"MLE de uma Fun\u00e7\u00e3o"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#teorema","text":"Seja \\hat{\\theta} MLE de \\theta e g(\\theta) fun\u00e7\u00e3o de \\theta . Ent\u00e3o uma MLE de g(\\theta) \u00e9 g(\\hat{\\theta}) .","title":"Teorema"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#consistencia","text":"Suponha que para uma amostra suficientemente grantde, existe um MLE \u00fanico para \\theta . Ent\u00e3o, sob algumas condi\u00e7\u00f5es, a sequ\u00eancia de MLE \u00e9 uma sequ\u00eancia consistente de estimadores de \\theta . A seuq\u00eancia convergee em probabilidade para o valor desconhecido de \\theta . O mesmo acontece com o Estimador de Bayes, dadas condi\u00e7\u00f5es de regularidade.","title":"Consist\u00eancia"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#funcao-digamma","text":"\\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)}","title":"Fun\u00e7\u00e3o Digamma:"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#metodo-dos-momentos","text":"Assuma que a amostra aleat\u00f3ria X_1,...,X_n vem da distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta k-dimensional. Por exemplo, a distribui\u00e7\u00e3o normal tem k = 2 . Tamb\u00e9m suponha que pelo menos os k primeiros momentos ( E[X_i^k] < \\infty ) sejam finitos. Defina \\mu_j(\\theta) = E[X_1^j|\\theta], j = 1,...k . Suponha que a fun\u00e7\u00e3o: \\begin{split} \\mu : ~&\\Omega \\to \\mathbb{R}^k \\\\ &\\theta \\mapsto \\mu(\\theta) = (\\mu_1(\\theta), ..., \\mu_k(\\theta)), \\end{split} \u00e9 injetiva em \\theta . Seja M(\\mu_1,...,\\mu_k) a fun\u00e7\u00e3o inversa, isto \u00e9, \\theta = M(\\mu_1,...,\\mu_k) O m\u00e9todo dos momentos ser\u00e1 M(m_1,...,m_j) , onde m_j = \\frac{1}{n}\\sum_{i=1}^n X_i^j, j = 1,...,k De forma mais simplificada, basta que sesolvemos o sistema: m_j = \\mu_j(\\theta), isto \u00e9, os momentos amostrais iguais aos momentos da amostra, condicionados em \\theta .","title":"M\u00e9todo dos Momentos"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#teorema_1","text":"Suponha que \\{X_n\\}_{n\\in\\mathbb{N}} i.i.d com distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta , k -dimensional. Suponha que os primeiros k momentos existem e s\u00e3o finitos para todo \\theta . Suponha que a inversa M definida acima \u00e9 cont\u00ednua. Ent\u00e3o a sequ\u00eancia de estimadores do m\u00e9todo de momentos em X_1,...,X_n \u00e9 consistente.","title":"Teorema"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#mle-e-estimador-de-bayes","text":"Se tivermos condi\u00e7\u00f5es de suavidade em f(x|\\theta) , podemos provar que quando n \\to \\infty , teremos que: L(\\theta|x) \\to c(x)\\cdot \\exp\\{-\\frac{1}{2V_n(\\theta)/n}(\\theta - \\hat{\\theta})^2\\}, onde \\hat{\\theta} \u00e9 MLE e V_n(\\theta) \u00e9 uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias convergente. No caso de termos uma priori relativamente flat, a posteriori ser\u00e1 aproximadamente uma distribui\u00e7\u00e3o normal com m\u00e9dia \\hat{\\theta} e vari\u00e2ncia V_n(\\hat{\\theta})/n .","title":"M.L.E e Estimador de Bayes"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#exemplo-7612-mortes-exercito-prussio","text":"Bortkiewicz contou o n\u00famero de soldados mortos por horsekick em 14 unidades do ex\u00e9rcito em 20 anos, com 280 contagens ao total. Das contagens temos Valor 0 1 2 3 4 Total Contagem 144 91 32 11 2 280 Modelamos X_1, ..., X_{280} como uma vari\u00e1vel de contagem. Considere a distribui\u00e7\u00e3o Poisson(\\theta) . Escolhemos a distribui\u00e7\u00e3o Gamma(\\alpha,\\beta) , dada que ela pertence \u00e0 familia conjungada. Em particular, a distribui\u00e7\u00e3o a posteriori ser\u00e1 Gamma(\\alpha + \\sum X_i, \\beta + n) , onde \\sum X_i = 196 . Se assumirmos \\alpha inteiro por simplicidade, vemos que a distribui\u00e7\u00e3o Gamma pode ser vista como a soma de \\alpha + \\sum X_i distribui\u00e7\u00f5es Exponencial(\\beta + n) . Logo a soma dessas vari\u00e1veis ser\u00e1 aproximadamente normal com m\u00e9dia 196/280 e vari\u00e2ncia 196/280^2 . import numpy as np import matplotlib.pyplot as plt from scipy.stats import gamma alpha = 1 beta = 1 # Esse \u00e9 o MLE, a m\u00e9dia. Vou supor que esse \u00e9 o par\u00e2metro verdadeiro s\u00f3 para mostrar. theta = 196 / 280 sum_xi = 196 fig , ax = plt . subplots ( 2 , 3 , figsize = ( 18 , 6 )) fig . suptitle ( 'Avaliando a converg\u00eancia da distribui\u00e7\u00e3o Gamma' ) for index , n in enumerate ([ 1 , 10 , 100 , 1000 , 10000 , 280 ]): i = int ( index / 3 ) j = index % 3 X = np . random . poisson ( theta , size = n ) if n != 280 : T = X . sum () ax [ i ][ j ] . set_title ( 'n = {}' . format ( n )) else : T = sum_xi #Valor dos dados ax [ i ][ j ] . set_title ( 'Dados Oficiais: n = {}' . format ( n )) t = np . linspace ( start = 0.00001 , stop = 3 - i - 1 , num = 1000 ) posteriori = gamma ( alpha + T , scale = 1 / ( beta + n )) y = posteriori . pdf ( t ) ax [ i ][ j ] . plot ( t , y , color = 'darkblue' ) ax [ i ][ j ] . grid ( color = 'grey' , alpha = 0.6 , linestyle = '--' ) ax [ i ][ j ] . vlines ( theta , ymin = 0 , ymax = max ( y ), color = 'black' , linestyle = '--' ) Veja que com os dados reais, j\u00e1 temos uma boa aproxima\u00e7\u00e3o!","title":"Exemplo 7.6.12 (Mortes ex\u00e9rcito pr\u00fassio)"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/","text":"Distribui\u00e7\u00f5es a Priori e a Posteriori Priori Tratamos \\theta de um modelo como uma vari\u00e1vel aleat\u00f3ria e atribuimos uma distribui\u00e7\u00e3o para esse par\u00e2metro. O nome ser\u00e1 distribui\u00e7\u00e3o a priori. Ao fazer modelagens, ela \u00e9 em geral pr\u00e9-definida pelo modelador, que \u00e9 em geral aconselhado por um especialista. Posteriori Sejam X_1, ..., X_n v.a. observadas e um par\u00e2metro \\theta desconhecido. A distribui\u00e7\u00e3o de \\theta condicionado nas vari\u00e1veis aleat\u00f3rias \u00e9 a distribui\u00e7\u00e3o a posteriori. Observe a rela\u00e7\u00e3o com o Teorema de Bayes. Teorema Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o f(x|\\theta) . Suponha que o par\u00e2metro seja desconhecido e que a distribui\u00e7\u00e3o da priori seja \\xi(\\theta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori \u00e9: \\xi(\\theta|x) = \\frac{f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta)}{g_n(x)}, \\theta \\in \\Omega Onde g_n \u00e9 a distribui\u00e7\u00e3o marginal conjunta de X_1,...,X_n Observe que, essencialmente \\xi(\\theta|x) \\propto f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta) , mas que sua integral seja 1 . Queremos que essa fun\u00e7\u00e3o seja integr\u00e1vel e a integral sobre o dom\u00ednio seja 1 . Fun\u00e7\u00e3o de Verossimilhan\u00e7a Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x) Observa\u00e7\u00f5es Sequenciais e Predi\u00e7\u00f5es Nesse caso a ordem das vari\u00e1veis X_1, ..., X_n importam (como uma s\u00e9rie temporal, por exemplo). Nesse caso, podemos, iterativamente fazer: \\xi(\\theta|x_1) \\propto f(x_1|\\theta)\\xi(\\theta) \\xi(\\theta|x_1,...,x_{n+1}) \\propto f(x_n|\\theta)\\xi(\\theta|x_1,....,x_n) Isso acontece dada a independ\u00eancia das vari\u00e1veis aleat\u00f3rias. Notebook de Refer\u00eancia Frequentistas Os dados observados s\u00e3o considerados aleat\u00f3rios, realidados de um processo aleat\u00f3rio. Os par\u00e2metros do modelo s\u00e3o fixos e desconhecidos Queremos derivas estimadores para os par\u00e2metros desconhecidos. Bayesianos Os dados s\u00e3o fixos, isto \u00e9, vieram de um processo aleat\u00f3rio, mas depois eles n\u00e3o se alteram. Os par\u00e2metros s\u00e3o usualmente representados por distribui\u00e7\u00f5es, s\u00e3o vari\u00e1veis aleat\u00f3rias. F\u00f3rmula de Bayes. Simples exemplo de infer\u00eancia Bayesiana Hemofilia \u00e9 uma disordem gen\u00e9tica que prejudica a coagula\u00e7\u00e3o em resposta a rupturas em vasos sangu\u00edneos. \u00c9 recessiva ligada ao cromossomo X. Isso implica que homens com 1 gene s\u00e3o afetados, enquanto as mulheres n\u00e3o s\u00e3o afetadas, mas portadoras. Considere uma mulher cuja m\u00e3e \u00e9 portadora e tem um irm\u00e3o afetado. Ela se casa com um homem n\u00e3o afetado. A mulher tem dois filhos consecutivos que n\u00e3o s\u00e3o afetados. Ser\u00e1 que a m\u00e3e \u00e9 portadora? A pergunra \u00e9 simples. Vamos tentar usar um pouco do que sabemos. Seja W = 1 se a mulher \u00e9 portadora e W = 0 se ela n\u00e3o for portadora. Queremos saber P(W = 1|s_1 = 0, s_2 = 0) , isto \u00e9, os filhos n\u00e3o s\u00e3o afetados. Que informa\u00e7\u00e3o n\u00f3s temos ? A m\u00e3e dela \u00e9 portadora, portanto uma priori interessante \u00e9: P(W = 1) = 0.5 \\Rightarrow O(W = 1) = \\frac{P(W=1)}{P(W=0)} = 1 \\text{ chances (odds) a priori } Podemos calcular a fun\u00e7\u00e3o de verossimilhan\u00e7a: L(W = 1|s_1 = 0, s_2 = 0) = F(s_1 = 0, s_2 = 0 | W = 1) = (0.5)(0.5) = 0.25 L(W = 0|s_1 = 0, s_2 = 0) = (1)(1) = 1 import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm priori = 0.5 # P(W = 1) p = 0.5 # prob de um filho ser afetado Likelihood = lambda w , s : np . prod ([( 1 - i , p ** i * ( 1 - p ) ** ( 1 - i ))[ w ] for i in s ]) s = [ 0 , 0 ] posteriori = Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori )) print ( \"A probabilidade da m\u00e3e portar \u00e9 {} com dois filhos n\u00e3o portadores.\" . format ( posteriori )) A probabilidade da m\u00e3e portar \u00e9 0.2 com dois filhos n\u00e3o portadores. s = [ 0 ] # terceiro filho priori = posteriori posteriori = Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori )) print ( \"A probabilidade da m\u00e3e portar \u00e9 {:.3f} com tr\u00eas filhos n\u00e3o portadores.\" . format ( posteriori )) A probabilidade da m\u00e3e portar \u00e9 0.111 com tr\u00eas filhos n\u00e3o portadores. priori = 0.5 p = 0.5 s = [ 0 ] posteriori = [] for i in range ( 50 ): posteriori . append ( Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori ))) priori = posteriori [ - 1 ] priori = 0.5 posteriori2 = [] for i in range ( 50 ): posteriori2 . append ( Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori ))) priori = posteriori2 [ - 1 ] s = [ np . random . choice ([ 0 , 1 ], p = ( 0.9 , 0.1 ))] plt . plot ( range ( 50 ), posteriori , label = 'Situa\u00e7\u00e3o 1' ) plt . plot ( range ( 50 ), posteriori2 , label = 'Situa\u00e7\u00e3o 2' ) plt . legend () plt . title ( 'Probabilidade dado cada filho' ) plt . show () Princ\u00edpio de Verossimilhan\u00e7a Afirma que para uma infer\u00eancia sobre um par\u00e2metro \\theta , toda evid\u00eancia de qualquer observa\u00e7\u00e3o de uma vari\u00e1vel aleat\u00f3ria X = x com distribui\u00e7\u00e3o X \\sim f(x|\\theta) se encontra na fun\u00e7\u00e3o de verossimilhan\u00e7a L(\\theta|x) . A interpreta\u00e7\u00e3o \u00e9 de que qualquer observa\u00e7\u00e3o de X pode construir conclus\u00f5es sobre \\theta . Al\u00e9m disso, se pud\u00e9ssemos obter informa\u00e7\u00e3o de \\theta sobre outra vari\u00e1vel aleat\u00f3ria Y com verossimilhan\u00e7a \\tilde{L} , teremos que L(\\theta|x) = c\\cdot \\tilde{L}(\\theta|y) . Isto \u00e9, as conclus\u00f5es sobre o par\u00e2metro n\u00e3o dependem da observa\u00e7\u00e3o feita. Qual o problema? Jeffreys : \"An hypothesis that may be true is rejected because it has failed to predict observable results that have not occurred. \" import pymc3 as pm from pymc3 import Model , Normal , Slice from pymc3 import sample from pymc3 import traceplot from pymc3.distributions import Interpolated from scipy import stats import matplotlib as mpl plt . style . use ( 'seaborn-darkgrid' ) Gerando os dados Y = \\alpha + \\beta_0\\cdot X_1 + \\beta_1\\cdot X_2 + \\text{erro} # True parameters alpha_true = 5 beta0_true = 7 beta1_true = 13 # Size of the dataset size = 100 # Random variables np . random . seed ( 1 ) X1 = np . random . randn ( size ) X2 = np . random . randn ( size ) * 0.2 e = np . random . randn ( size ) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e Especificando o modelo Vamos fazer um modelo simples aqui, s\u00f3 para mostrar essa biblioteca nova. A ideia \u00e9 mostrar como funciona a ideia de priori e posteriori. Vamos dizer que Y \\sim N(\\mu, 1) , onde \\mu = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 model = Model () #cria um novo modelo, que em Python, \u00e9 um objeto with model : # Isso cria um contexto em python # Vamos dizer nossas prioris! alpha = Normal ( 'alpha' , mu = 0 , sigma = 1 ) beta0 = Normal ( 'beta0' , mu = 12 , sigma = 1 ) beta1 = Normal ( 'beta1' , mu = 18 , sigma = 1 ) # Valor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Verossimilhan\u00e7a das observa\u00e7\u00f5es Y_obs = Normal ( 'Y_obs' , mu = mu , sigma = 1 , observed = Y ) # Amostras da distribui\u00e7\u00e3o trace = sample ( 1000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. A pr\u00f3xima fun\u00e7\u00e3o \u00e9 um plot de distribui\u00e7\u00e3o a priori amostrada (que \u00e9 basicamente um histograma) de cada par\u00e2metro. Veja que parece um chiado em torno da m\u00e9dia. traceplot ( trace ); Agora que n\u00f3s temos os dados gerados Y , vamos atualizar nosso conhecimento, nossa confian\u00e7a nos par\u00e2metros, atrav\u00e9s da distribui\u00e7\u00e3o a posteriori. Os dados devem ser independentes a ada intera\u00e7\u00e3o para que valha o que estudamos. Para isso, precisamos calcular a posteriori de cada par\u00e2metro. Nesse caso, vamos utilizar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o, utlizando aproxima\u00e7\u00e3o Kernel . N\u00e3o se preocupe com isso, \u00e9 s\u00f3 para mostrar que estamos calculado a posteriori def from_posterior ( param , samples ): smin , smax = np . min ( samples ), np . max ( samples ) width = smax - smin x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) # what was never sampled should have a small probability but not 0, # so we'll extend the domain and use linear approximation of density on it x = np . concatenate ([[ x [ 0 ] - 3 * width ], x , [ x [ - 1 ] + 3 * width ]]) y = np . concatenate ([[ 0 ], y , [ 0 ]]) return Interpolated ( param , x , y ) Agora, vamos gerar mais dados e usar a F\u00f3rmula de Bayes e usaremos uma forma sequencial das observa\u00e7\u00f5es, isto \u00e9, a posteriori da itera\u00e7\u00e3o n-1 ser\u00e1 a priori da itera\u00e7\u00e3o n . traces = [ trace ] # salva os tra\u00e7os para que plotamos depois. for _ in range ( 10 ): # _ indica uma vari\u00e1vel que n\u00e3o \u00e9 usada # Gerando mais e mais dados! X1 = np . random . randn ( size ) X2 = np . random . randn ( size ) * 0.2 e = np . random . randn ( size ) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e model = Model () with model : # As novas prioris s\u00e3o as posterioris alpha = from_posterior ( 'alpha' , trace [ 'alpha' ]) beta0 = from_posterior ( 'beta0' , trace [ 'beta0' ]) beta1 = from_posterior ( 'beta1' , trace [ 'beta1' ]) # EValor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Calculando a verossimilhan\u00e7a dos novos dados Y_obs = Normal ( 'Y_obs' , mu = mu , sigma = 1 , observed = Y ) # Amostrando da posteriori, porque n\u00e3o estamos calculando a forma fechada trace = sample ( 1000 ) traces . append ( trace ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The acceptance probability does not match the target. It is 0.8800702431829607, but should be close to 0.8. Try to increase the number of tuning steps. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. fig , ax = plt . subplots ( 1 , 3 , figsize = ( 20 , 5 )) # Definindo cores cmap = mpl . cm . autumn for index , param in enumerate ([ 'alpha' , 'beta0' , 'beta1' ]): for update_i , trace in enumerate ( traces ): samples = trace [ param ] smin , smax = np . min ( samples ), np . max ( samples ) x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) ax [ index ] . plot ( x , y , color = cmap ( 1 - update_i / len ( traces )), alpha = update_i / len ( traces )) ax [ index ] . axvline ({ 'alpha' : alpha_true , 'beta0' : beta0_true , 'beta1' : beta1_true }[ param ], c = 'k' ) ax [ index ] . set_ylabel ( 'Frequency' ) ax [ index ] . set_title ( param ) plt . tight_layout ();","title":"Priori e Posteriori"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#distribuicoes-a-priori-e-a-posteriori","text":"","title":"Distribui\u00e7\u00f5es a Priori e a Posteriori"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#priori","text":"Tratamos \\theta de um modelo como uma vari\u00e1vel aleat\u00f3ria e atribuimos uma distribui\u00e7\u00e3o para esse par\u00e2metro. O nome ser\u00e1 distribui\u00e7\u00e3o a priori. Ao fazer modelagens, ela \u00e9 em geral pr\u00e9-definida pelo modelador, que \u00e9 em geral aconselhado por um especialista.","title":"Priori"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#posteriori","text":"Sejam X_1, ..., X_n v.a. observadas e um par\u00e2metro \\theta desconhecido. A distribui\u00e7\u00e3o de \\theta condicionado nas vari\u00e1veis aleat\u00f3rias \u00e9 a distribui\u00e7\u00e3o a posteriori. Observe a rela\u00e7\u00e3o com o Teorema de Bayes.","title":"Posteriori"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#teorema","text":"Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o f(x|\\theta) . Suponha que o par\u00e2metro seja desconhecido e que a distribui\u00e7\u00e3o da priori seja \\xi(\\theta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori \u00e9: \\xi(\\theta|x) = \\frac{f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta)}{g_n(x)}, \\theta \\in \\Omega Onde g_n \u00e9 a distribui\u00e7\u00e3o marginal conjunta de X_1,...,X_n Observe que, essencialmente \\xi(\\theta|x) \\propto f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta) , mas que sua integral seja 1 . Queremos que essa fun\u00e7\u00e3o seja integr\u00e1vel e a integral sobre o dom\u00ednio seja 1 .","title":"Teorema"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#funcao-de-verossimilhanca","text":"Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x)","title":"Fun\u00e7\u00e3o de Verossimilhan\u00e7a"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#observacoes-sequenciais-e-predicoes","text":"Nesse caso a ordem das vari\u00e1veis X_1, ..., X_n importam (como uma s\u00e9rie temporal, por exemplo). Nesse caso, podemos, iterativamente fazer: \\xi(\\theta|x_1) \\propto f(x_1|\\theta)\\xi(\\theta) \\xi(\\theta|x_1,...,x_{n+1}) \\propto f(x_n|\\theta)\\xi(\\theta|x_1,....,x_n) Isso acontece dada a independ\u00eancia das vari\u00e1veis aleat\u00f3rias. Notebook de Refer\u00eancia","title":"Observa\u00e7\u00f5es Sequenciais e Predi\u00e7\u00f5es"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#frequentistas","text":"Os dados observados s\u00e3o considerados aleat\u00f3rios, realidados de um processo aleat\u00f3rio. Os par\u00e2metros do modelo s\u00e3o fixos e desconhecidos Queremos derivas estimadores para os par\u00e2metros desconhecidos.","title":"Frequentistas"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#bayesianos","text":"Os dados s\u00e3o fixos, isto \u00e9, vieram de um processo aleat\u00f3rio, mas depois eles n\u00e3o se alteram. Os par\u00e2metros s\u00e3o usualmente representados por distribui\u00e7\u00f5es, s\u00e3o vari\u00e1veis aleat\u00f3rias. F\u00f3rmula de Bayes.","title":"Bayesianos"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#simples-exemplo-de-inferencia-bayesiana","text":"Hemofilia \u00e9 uma disordem gen\u00e9tica que prejudica a coagula\u00e7\u00e3o em resposta a rupturas em vasos sangu\u00edneos. \u00c9 recessiva ligada ao cromossomo X. Isso implica que homens com 1 gene s\u00e3o afetados, enquanto as mulheres n\u00e3o s\u00e3o afetadas, mas portadoras. Considere uma mulher cuja m\u00e3e \u00e9 portadora e tem um irm\u00e3o afetado. Ela se casa com um homem n\u00e3o afetado. A mulher tem dois filhos consecutivos que n\u00e3o s\u00e3o afetados. Ser\u00e1 que a m\u00e3e \u00e9 portadora? A pergunra \u00e9 simples. Vamos tentar usar um pouco do que sabemos. Seja W = 1 se a mulher \u00e9 portadora e W = 0 se ela n\u00e3o for portadora. Queremos saber P(W = 1|s_1 = 0, s_2 = 0) , isto \u00e9, os filhos n\u00e3o s\u00e3o afetados. Que informa\u00e7\u00e3o n\u00f3s temos ? A m\u00e3e dela \u00e9 portadora, portanto uma priori interessante \u00e9: P(W = 1) = 0.5 \\Rightarrow O(W = 1) = \\frac{P(W=1)}{P(W=0)} = 1 \\text{ chances (odds) a priori } Podemos calcular a fun\u00e7\u00e3o de verossimilhan\u00e7a: L(W = 1|s_1 = 0, s_2 = 0) = F(s_1 = 0, s_2 = 0 | W = 1) = (0.5)(0.5) = 0.25 L(W = 0|s_1 = 0, s_2 = 0) = (1)(1) = 1 import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm priori = 0.5 # P(W = 1) p = 0.5 # prob de um filho ser afetado Likelihood = lambda w , s : np . prod ([( 1 - i , p ** i * ( 1 - p ) ** ( 1 - i ))[ w ] for i in s ]) s = [ 0 , 0 ] posteriori = Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori )) print ( \"A probabilidade da m\u00e3e portar \u00e9 {} com dois filhos n\u00e3o portadores.\" . format ( posteriori )) A probabilidade da m\u00e3e portar \u00e9 0.2 com dois filhos n\u00e3o portadores. s = [ 0 ] # terceiro filho priori = posteriori posteriori = Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori )) print ( \"A probabilidade da m\u00e3e portar \u00e9 {:.3f} com tr\u00eas filhos n\u00e3o portadores.\" . format ( posteriori )) A probabilidade da m\u00e3e portar \u00e9 0.111 com tr\u00eas filhos n\u00e3o portadores. priori = 0.5 p = 0.5 s = [ 0 ] posteriori = [] for i in range ( 50 ): posteriori . append ( Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori ))) priori = posteriori [ - 1 ] priori = 0.5 posteriori2 = [] for i in range ( 50 ): posteriori2 . append ( Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori ))) priori = posteriori2 [ - 1 ] s = [ np . random . choice ([ 0 , 1 ], p = ( 0.9 , 0.1 ))] plt . plot ( range ( 50 ), posteriori , label = 'Situa\u00e7\u00e3o 1' ) plt . plot ( range ( 50 ), posteriori2 , label = 'Situa\u00e7\u00e3o 2' ) plt . legend () plt . title ( 'Probabilidade dado cada filho' ) plt . show ()","title":"Simples exemplo de infer\u00eancia Bayesiana"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#principio-de-verossimilhanca","text":"Afirma que para uma infer\u00eancia sobre um par\u00e2metro \\theta , toda evid\u00eancia de qualquer observa\u00e7\u00e3o de uma vari\u00e1vel aleat\u00f3ria X = x com distribui\u00e7\u00e3o X \\sim f(x|\\theta) se encontra na fun\u00e7\u00e3o de verossimilhan\u00e7a L(\\theta|x) . A interpreta\u00e7\u00e3o \u00e9 de que qualquer observa\u00e7\u00e3o de X pode construir conclus\u00f5es sobre \\theta . Al\u00e9m disso, se pud\u00e9ssemos obter informa\u00e7\u00e3o de \\theta sobre outra vari\u00e1vel aleat\u00f3ria Y com verossimilhan\u00e7a \\tilde{L} , teremos que L(\\theta|x) = c\\cdot \\tilde{L}(\\theta|y) . Isto \u00e9, as conclus\u00f5es sobre o par\u00e2metro n\u00e3o dependem da observa\u00e7\u00e3o feita. Qual o problema? Jeffreys : \"An hypothesis that may be true is rejected because it has failed to predict observable results that have not occurred. \" import pymc3 as pm from pymc3 import Model , Normal , Slice from pymc3 import sample from pymc3 import traceplot from pymc3.distributions import Interpolated from scipy import stats import matplotlib as mpl plt . style . use ( 'seaborn-darkgrid' )","title":"Princ\u00edpio de Verossimilhan\u00e7a"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#gerando-os-dados","text":"Y = \\alpha + \\beta_0\\cdot X_1 + \\beta_1\\cdot X_2 + \\text{erro} # True parameters alpha_true = 5 beta0_true = 7 beta1_true = 13 # Size of the dataset size = 100 # Random variables np . random . seed ( 1 ) X1 = np . random . randn ( size ) X2 = np . random . randn ( size ) * 0.2 e = np . random . randn ( size ) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e","title":"Gerando os dados"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#especificando-o-modelo","text":"Vamos fazer um modelo simples aqui, s\u00f3 para mostrar essa biblioteca nova. A ideia \u00e9 mostrar como funciona a ideia de priori e posteriori. Vamos dizer que Y \\sim N(\\mu, 1) , onde \\mu = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 model = Model () #cria um novo modelo, que em Python, \u00e9 um objeto with model : # Isso cria um contexto em python # Vamos dizer nossas prioris! alpha = Normal ( 'alpha' , mu = 0 , sigma = 1 ) beta0 = Normal ( 'beta0' , mu = 12 , sigma = 1 ) beta1 = Normal ( 'beta1' , mu = 18 , sigma = 1 ) # Valor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Verossimilhan\u00e7a das observa\u00e7\u00f5es Y_obs = Normal ( 'Y_obs' , mu = mu , sigma = 1 , observed = Y ) # Amostras da distribui\u00e7\u00e3o trace = sample ( 1000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. A pr\u00f3xima fun\u00e7\u00e3o \u00e9 um plot de distribui\u00e7\u00e3o a priori amostrada (que \u00e9 basicamente um histograma) de cada par\u00e2metro. Veja que parece um chiado em torno da m\u00e9dia. traceplot ( trace ); Agora que n\u00f3s temos os dados gerados Y , vamos atualizar nosso conhecimento, nossa confian\u00e7a nos par\u00e2metros, atrav\u00e9s da distribui\u00e7\u00e3o a posteriori. Os dados devem ser independentes a ada intera\u00e7\u00e3o para que valha o que estudamos. Para isso, precisamos calcular a posteriori de cada par\u00e2metro. Nesse caso, vamos utilizar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o, utlizando aproxima\u00e7\u00e3o Kernel . N\u00e3o se preocupe com isso, \u00e9 s\u00f3 para mostrar que estamos calculado a posteriori def from_posterior ( param , samples ): smin , smax = np . min ( samples ), np . max ( samples ) width = smax - smin x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) # what was never sampled should have a small probability but not 0, # so we'll extend the domain and use linear approximation of density on it x = np . concatenate ([[ x [ 0 ] - 3 * width ], x , [ x [ - 1 ] + 3 * width ]]) y = np . concatenate ([[ 0 ], y , [ 0 ]]) return Interpolated ( param , x , y ) Agora, vamos gerar mais dados e usar a F\u00f3rmula de Bayes e usaremos uma forma sequencial das observa\u00e7\u00f5es, isto \u00e9, a posteriori da itera\u00e7\u00e3o n-1 ser\u00e1 a priori da itera\u00e7\u00e3o n . traces = [ trace ] # salva os tra\u00e7os para que plotamos depois. for _ in range ( 10 ): # _ indica uma vari\u00e1vel que n\u00e3o \u00e9 usada # Gerando mais e mais dados! X1 = np . random . randn ( size ) X2 = np . random . randn ( size ) * 0.2 e = np . random . randn ( size ) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e model = Model () with model : # As novas prioris s\u00e3o as posterioris alpha = from_posterior ( 'alpha' , trace [ 'alpha' ]) beta0 = from_posterior ( 'beta0' , trace [ 'beta0' ]) beta1 = from_posterior ( 'beta1' , trace [ 'beta1' ]) # EValor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Calculando a verossimilhan\u00e7a dos novos dados Y_obs = Normal ( 'Y_obs' , mu = mu , sigma = 1 , observed = Y ) # Amostrando da posteriori, porque n\u00e3o estamos calculando a forma fechada trace = sample ( 1000 ) traces . append ( trace ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The acceptance probability does not match the target. It is 0.8800702431829607, but should be close to 0.8. Try to increase the number of tuning steps. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. fig , ax = plt . subplots ( 1 , 3 , figsize = ( 20 , 5 )) # Definindo cores cmap = mpl . cm . autumn for index , param in enumerate ([ 'alpha' , 'beta0' , 'beta1' ]): for update_i , trace in enumerate ( traces ): samples = trace [ param ] smin , smax = np . min ( samples ), np . max ( samples ) x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) ax [ index ] . plot ( x , y , color = cmap ( 1 - update_i / len ( traces )), alpha = update_i / len ( traces )) ax [ index ] . axvline ({ 'alpha' : alpha_true , 'beta0' : beta0_true , 'beta1' : beta1_true }[ param ], c = 'k' ) ax [ index ] . set_ylabel ( 'Frequency' ) ax [ index ] . set_title ( param ) plt . tight_layout ();","title":"Especificando o modelo"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/","text":"Distribui\u00e7\u00e3o Chi-Quadrado Para cada m positivo, a distribui\u00e7\u00e3o Gamma(m/2, 1/2) \u00e9 chamada de distribui\u00e7\u00e3o \\chi^2 . Ela foi primeiramente descrita por Helmert para computar a distribui\u00e7\u00e3o amostral de uma popula\u00e7\u00e3o normal. Vamos ver como a normal se relaciona mais a frente. f(x) = \\frac{1}{2^{m/2}\\Gamma(m/2)}x^{m/2 - 1}e^{-x/2} Propriedades Se X \\sim \\chi^2(m) , ent\u00e3o: E(X) = m Var(X) = 2m \\psi(t) = \\left(\\frac{1}{1-2t}\\right)^{m/2}, t < \\frac{1}{2} Soma de \\chi^2 Se X_1, ..., X_k s\u00e3o independentes e cada uma tem grau de liberdade m_i , ent\u00e3o X_1 + ... + X_n tem distribui\u00e7\u00e3o \\chi^2(m_1 + .... + m_k) Rela\u00e7\u00e3o com a Normal Se X tem distribui\u00e7\u00e3o normal padr\u00e3o, Y = X^2 \\sim \\chi^2(1) De fato, se juntarmos as \u00faltimos dois teoremas, veremos que a soma de quadrados de normais independentes e identicamente distribuidas ser\u00e1 \\chi^2(m) , onde m \u00e9 o n\u00famero de parcelas. Implementa\u00e7\u00e3o import numpy as np import matplotlib.pyplot as plt from scipy.stats import chi2 from matplotlib import animation , cm from IPython.display import HTML # Random Object ro = np . random . default_rng ( 1000 ) # Para assegurar reprodutibilidade degree_freedom = 10 mean , var , skew , kurt = chi2 . stats ( degree_freedom , moments = 'mvsk' ) print ( 'Propriedades' ) print ( 'M\u00e9dia: {}' . format ( mean )) print ( 'Var: {}' . format ( var )) print ( 'Assimetria: {}' . format ( skew )) print ( 'Curtose: {}' . format ( kurt )) Propriedades M\u00e9dia: 10.0 Var: 20.0 Assimetria: 0.8944271909999159 Curtose: 1.2 fig , ax = plt . subplots ( 1 , 1 ) x = np . linspace ( chi2 . ppf ( 0.01 , degree_freedom ), chi2 . ppf ( 0.99 , degree_freedom ), 100 ) ax . plot ( x , chi2 . pdf ( x , degree_freedom ), 'r-' , lw = 5 , alpha = 0.6 , label = 'chi2 pdf' ) r = chi2 . rvs ( degree_freedom , size = 10000 ) ax . hist ( r , density = True , alpha = 0.2 ) ax . legend () plt . show () fig , ax = plt . subplots () line , = ax . plot ( x , chi2 . pdf ( x , degree_freedom ), 'r-' , lw = 5 , alpha = 0.6 ) ax . set_xlim (( 0 , 150 )) ax . set_title ( 'Chi-Square' ) def animate ( i , degree_freedom ): x = np . linspace ( 0 , chi2 . ppf ( 0.99 , degree_freedom + i ), 100 ) line . set_data ( x , chi2 . pdf ( x , degree_freedom + i )) return line , anim = animation . FuncAnimation ( fig , animate , frames = 100 , interval = 50 , fargs = ( degree_freedom ,), repeat = False ) HTML ( anim . to_html5_video ()) Your browser does not support the video tag. Distribui\u00e7\u00e3o Conjunta da m\u00e9dia e vari\u00e2ncia amostrais X_1,...,X_n formam uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal e com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 desconhecidos. Estamos interessados na distribui\u00e7\u00e3o conjunta dos estimadores de m\u00e1xima verossimilhan\u00e7a para m\u00e9dia e vari\u00e2ncia da amostra. Teorema de Basu Sejam \\hat{\\mu} = \\bar{X}_n e \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 a m\u00e9dia e vari\u00e2ncia amostrais, respectivamente. Ent\u00e3o \\hat{\\mu} tem distribui\u00e7\u00e3o normal com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 /n , enquanto \\hat{\\sigma}^2 tem a distribui\u00e7\u00e3o \\chi^2(n-1) , isto \u00e9, com n-1 grau de liberdade. Al\u00e9m disso elas s\u00e3o independentes. Esse teorema \u00e9 um pouco mais complexo e, na verdade, essa seria uma esp\u00e9cie de aplica\u00e7\u00e3o do teorema, na verdade. O teorema de Basu diz que: Se T \u00e9 uma estat\u00edstica suficiente completa (Considere, nesse teorema, g uma fun\u00e7\u00e3o integr\u00e1vel limitada) para \\theta e A uma estat\u00edstica ancillary, ent\u00e3o T \u00e9 independente de A . Nesse caso \\hat{\\mu} \u00e9 completa suficiente e \\hat{\\sigma}^2 \u00e9 ancillary, por que n\u00e3o depende de \\mu . O mais interessante \u00e9 que essa propriedade \u00e9 s\u00f3 vista com a distribui\u00e7\u00e3o normal ! Olhem a p\u00e1gina 9. Demonstra\u00e7\u00e3o O livro tem uma abordagem um pouco mais voltado \u00e0 \u00c1lgebra Linear. Aqui vou mostrar uma ideia um pouco diferente, onde voc\u00eas podem demonstrar os passos, como exerc\u00edcio. Passo 1: \\sum_{i=1}^n X_i^2 = n\\hat{\\sigma}^2 + n\\hat{\\mu}^2 Dica: Escrever \\hat{\\sigma}^2 e abrir em tr\u00eas somat\u00f3rios. Passo 2: \\sum_{i=1}^n (X_i - \\mu)^2 = n\\hat{\\sigma}^2 + n(\\hat{\\mu} - \\mu)^2 Dica: O Passo 1 \u00e9 um caso especial do Passo 2. O processo \u00e9 o mesmo. Passo 3: \\hat{\\mu} \u00e9 independente de X_i - \\hat{\\mu}, i = 1,...,n . Dica: Montar a pdf conjunta de X_1, ..., X_n (j\u00e1 fizemos isso atrave\u015b da verossimilhan\u00e7a) e fazer uma mudan\u00e7a de vari\u00e1vel Y_1 = \\hat{\\mu}, Y_2 = X_2 - \\hat{\\mu}, ..., Y_n = X_n - \\hat{\\mu} . Com essa mudan\u00e7a, \u00e9 poss\u00edvel montar a pdf como fun\u00e7\u00e3o de y_1,...,y_n . Esse processo \u00e9 um pouco mais chato, mas \u00e9 bom lembrar como fazez mudan\u00e7a de vari\u00e1vel para pdfs. Aqui voc\u00ea pode conferir como . \u00c9 importante lembrar que \u00e9 uma fun\u00e7\u00e3o de y ap\u00f3s transformada e n\u00e3o de x . Dica 2: Fatorizar a pdf conjunta. Voc\u00ea vai ver como se destaca a independ\u00eancia aqui. Passo 4: Mostrar que \\hat{\\mu} e \\hat{\\sigma}^2 s\u00e3o independentes. Refer\u00eancias 1 2 Simples visualiza\u00e7\u00e3o Eu gostaria de comparar o que acontece com a m\u00e9dia e vari\u00e2ncia amostral da distribui\u00e7\u00e3o normal e da distribui\u00e7\u00e3o gamma. Para isso, geero amostras de tamanho n , calculo as estat\u00edsticas e salvo. Fa\u00e7o esse procedimento o n\u00famero de pontos que quiser. ite = 10000 n = 10000 # Par\u00e2metros da Normal mu = 5 sigma = 2 # Par\u00e2metros da Gamma alpha = 5 beta = 4 means = np . zeros (( ite , 2 )) variances = np . zeros (( ite , 2 )) for i in range ( ite ): X = ro . normal ( loc = mu , scale = sigma , size = n ) Y = ro . gamma ( shape = alpha , scale = 1 / beta , size = n ) means [ i , 0 ] = np . mean ( X ) means [ i , 1 ] = np . mean ( Y ) variances [ i , 0 ] = np . var ( X , ddof = 0 ) variances [ i , 1 ] = np . var ( Y , ddof = 0 ) coef_normal = np . polyfit ( x = means [:, 0 ], y = variances [:, 0 ], deg = 1 ) coef_gamma = np . polyfit ( x = means [:, 1 ], y = variances [:, 1 ], deg = 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) fig . suptitle ( 'Comparando m\u00e9dia e vari\u00e2ncia amostral' ) ax [ 0 ] . scatter ( means [:, 0 ], variances [:, 0 ]) ax [ 1 ] . scatter ( means [:, 1 ], variances [:, 1 ]) ax [ 0 ] . plot ( means [:, 0 ], coef_normal [ 0 ] * means [:, 0 ] + coef_normal [ 1 ], color = 'red' ) ax [ 1 ] . plot ( means [:, 1 ], coef_gamma [ 0 ] * means [:, 1 ] + coef_gamma [ 1 ], color = 'red' ) ax [ 0 ] . set_xlabel ( r '$\\bar{X}_n$' , fontsize = 18 ) ax [ 1 ] . set_xlabel ( r '$\\bar{X}_n$' , fontsize = 18 ) ax [ 0 ] . set_ylabel ( r '$\\sum (X_i - \\bar{X}_n)^2$' , fontsize = 18 ) ax [ 1 ] . set_ylabel ( r '$\\sum (X_i - \\bar{X}_n)^2$' , fontsize = 18 ) ax [ 0 ] . set_title ( 'Distribui\u00e7\u00e3o Normal' ) ax [ 1 ] . set_title ( 'Distribui\u00e7\u00e3o Gamma' ) ax [ 0 ] . grid ( alpha = 0.5 , linestyle = '--' ) ax [ 1 ] . grid ( alpha = 0.5 , linestyle = '--' ) plt . show () Obs: A n\u00e3o inclina\u00e7\u00e3o da reta n\u00e3o significa que existe independ\u00eancia, mas como s\u00e3o independentes, a gente espera que a inclina\u00e7\u00e3o seja pequena. Distribui\u00e7\u00f5es T Student Artigo original : Olhe a p\u00e1gina 9! Defini\u00e7\u00e3o Sejam Y \\sim \\chi^2(m) e Z \\sim N(0,1) independentes. Ent\u00e3o X = \\frac{Z}{\\left(\\frac{Y}{m}\\right)^{1/2}} \\sim t(m) onde t(m) \u00e9 a distribui\u00e7\u00e3o t-student com m graus de liberdade. Fun\u00e7\u00e3o densidade de probabilidade Para escrever essa fun\u00e7\u00e3o de probabilidade, defina X como acima e W = Y . J\u00e1 sabemos a distribui\u00e7\u00e3o conjunta de Y e Z , pois eles s\u00e3o independentes. Com essa mudan\u00e7a de vari\u00e1vel ( confira aqui se n\u00e3o lembra como \u00e9 feito ), voc\u00ea conseque escrever a distribui\u00e7\u00e3o conjunta de X e W . Depois, basta calcular a distribui\u00e7\u00e3o marginal de X , integrando em W . f(x) = \\frac{\\Gamma\\left(\\frac{m+1}{2}\\right)}{(m\\pi)^{1/2}\\Gamma\\left(\\frac{m}{2}\\right)}\\left(1 + \\frac{x^2}{m} \\right)^{-(m+1)/2}, x \\in \\mathbb{R}, onde \\Gamma \u00e9 a fun\u00e7\u00e3o Gamma , tal que, n \\in \\mathbb{N}, \\Gamma(n) = (n-1)! \\Gamma(z+1) = z\\Gamma(z) \\Gamma(1/2) = \\sqrt{\\pi} Quando m \\leq 1 , a m\u00e9dia \u00e9 divergente. Isso pode ser vizualizado pelo expoente que ser\u00e1 \\leq -1 , o que diverge (lembre de \\int 1/x ). Quando m > 1 , a m\u00e9dia existe e \u00e9 0 pela simetria da distribui\u00e7\u00e3o. Em particular, podemos mostrar que se k < m , E[|X^k|] < + \\infty e se k \\geq m , o momento diverge. Se X \\sim t(m), m > 2 , Var(X) = \\frac{m}{m-2} Teorema Seja X_1, ..., X_n \\overset{iid}{\\sim} N(\\mu,\\sigma^2) . Seja \\sigma ' = \\left[\\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}{n-1}\\right]^{1/2} Ent\u00e3o n^{1/2}(\\bar{X}_n - \\mu)/\\sigma ' \\sim t(n-1) Rela\u00e7\u00e3o com a Normal e Cauchy Da mesma forma que a distribui\u00e7\u00e3o normal e a distribui\u00e7\u00e3o Cauchy, a distribui\u00e7\u00e3o t \u00e9 centrada em 0 e tem sua moda nesse valor. Entretanto a cauda a distribui\u00e7\u00e3o t (quando x \\to -\\infty ou x \\to +\\infty ), \u00e9 mais pesada, no sentido de que tende para 0 em uma velocidade menor do que a normal. Outra coisa interessante \u00e9 que a ditrivui\u00e7\u00e3o t(1) \u00e9 a distribui\u00e7\u00e3o Cauchy . Al\u00e9m disso, quando n \\to \\infty , converge para a pdf da normal padr\u00e3o ( Normal(0,1) ). Ferramentas para demonstrar a converg\u00eancia Teorema de Slutsky : Considere o corol\u00e1rio com f(x,y) = \\frac{x}{y} Lei dos Grandes N\u00fameros : Escreva a qui-quadrado como soma de normais. from scipy.stats import t , norm , cauchy Implementa\u00e7\u00e3o Primeiro vamos ver a cara da distribui\u00e7\u00e3o t m = 10 X = t ( df = m ) w = np . arange ( - 3 , 3 , 0.1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax [ 0 ] . plot ( w , X . pdf ( w ), lw = 5 , color = 'orange' ) ax [ 1 ] . plot ( w , X . cdf ( w ), lw = 5 , color = 'orange' ) ax [ 0 ] . set_title ( 'PDF t-Student' ) ax [ 1 ] . set_title ( 'CDF t-Student' ) plt . show () Vamos ver o que acontece quando m \\leq 1 ? ite = 1000 n = 10000 m1 = 10 m2 = 0.5 means = np . zeros (( ite , 2 )) for i in range ( ite ): X = ro . standard_t ( df = m1 , size = n ) Y = ro . standard_t ( df = m2 , size = n ) means [ i , 0 ] = np . mean ( X ) means [ i , 1 ] = np . mean ( Y ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) ax [ 0 ] . hist ( means [:, 0 ], bins = 100 ) ax [ 1 ] . hist ( np . log ( means [:, 1 ]), bins = 10 ) ax [ 0 ] . set_xlabel ( 'E[X]' ) ax [ 1 ] . set_xlabel ( 'log E[X]' ) ax [ 0 ] . set_title ( 'm = 10' ) ax [ 1 ] . set_title ( 'm = 0.5' ) plt . show () <ipython-input-11-2bfd1961d53b>:3: RuntimeWarning: invalid value encountered in log ax[1].hist(np.log(means[:,1]), bins = 10) No eixo x do segundo gr\u00e1fico plotei o logaritmo, dado que alguns resultados eram extremamente grandes! Isso indica visualmente que a m\u00e9dia diverge! Rela\u00e7\u00e3o com a Normal e com Cauchy C = cauchy () Z = norm ( loc = 0 , scale = 1 ) T = t ( df = 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) ax [ 0 ] . plot ( w , C . pdf ( w ), label = 'Cauchy' ) ax [ 0 ] . scatter ( w , T . pdf ( w ), c = 'red' , marker = \"*\" , label = 't-Student' ) ax [ 0 ] . legend () ax [ 0 ] . set_title ( 't-Student e Cauchy quando m = 1' ) ax [ 1 ] . plot ( w , Z . pdf ( w ), label = 'N(0,1)' ) ax [ 1 ] . set_title ( 'Converg\u00eancia da t para a normal' ) for i in np . logspace ( np . log10 ( 1 ), np . log10 ( 20 ), 5 ): T = t ( df = int ( i )) ax [ 1 ] . plot ( w , T . pdf ( w ), linestyle = '--' , alpha = i / 40 + 0.5 , color = 'grey' , label = 't({})' . format ( int ( i ))) ax [ 1 ] . legend ( loc = 'upper right' ) plt . show () Distribui\u00e7\u00e3o F Sejam Y \\sim \\chi^2_m e W \\sim \\chi^2_n independentes. Defina X = \\frac{Y/m}{W/n} = \\frac{nY}{mW} Dizemos que X tem distribui\u00e7\u00e3o F . A sua motiva\u00e7\u00e3o vem do teste de hip\u00f3teses que compara vari\u00e2ncias de duas normais. Fun\u00e7\u00e3o de densidade de probabilidade Seja X \\sim F_{m,n} . Ent\u00e3o sua pdf tem suporte em x > 0 e pe definida f(x) = \\frac{\\Gamma\\left[\\frac{1}{2}(m+n)\\right]m^{m/2}n^{n/2}}{\\Gamma\\left(\\frac{1}{2}m\\right)\\Gamma\\left(\\frac{1}{2}n\\right)}\\cdot \\frac{x^{(m/2) - 1}}{(mx + n)^{(m+n)/2)}} Observe que ela n\u00e3o \u00e9 sim\u00e9trica em m e n . Assim, se trocarmos eles de lugar, teremos um resultado diferente. Propriedades Seja X \\sim F_{m,n} . Ent\u00e3o 1/X \\sim F_{n,m} . Se Y \\sim t_n , ent\u00e3o Y^2 \\sim F_{1,n} . Existem diversas rela\u00e7\u00f5es que s\u00e3o encontradas com outras distribui\u00e7\u00f5es. Confira aqui E[X] = \\frac{n}{n-2}, n > 2 Var[X] = \\frac{2n^2(m + n - 2)}{m(n-2)^2(n-4)} import numpy as np from scipy.stats import f import matplotlib.pyplot as plt Vamos ver como \u00e9 a cara dessa distribui\u00e7\u00e3o: m , n = 20 , 10 X = f ( dfn = m , dfd = n ) w = np . arange ( 0 , 5 , 0.1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax [ 0 ] . plot ( w , X . pdf ( w ), lw = 5 , color = 'orange' ) ax [ 1 ] . plot ( w , X . cdf ( w ), lw = 5 , color = 'orange' ) ax [ 0 ] . set_title ( 'PDF Distribui\u00e7\u00e3o F' ) ax [ 1 ] . set_title ( 'CDF Distribui\u00e7\u00e3o F' ) plt . show ()","title":"Distribui\u00e7\u00e3o Amostral"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#distribuicao-chi-quadrado","text":"Para cada m positivo, a distribui\u00e7\u00e3o Gamma(m/2, 1/2) \u00e9 chamada de distribui\u00e7\u00e3o \\chi^2 . Ela foi primeiramente descrita por Helmert para computar a distribui\u00e7\u00e3o amostral de uma popula\u00e7\u00e3o normal. Vamos ver como a normal se relaciona mais a frente. f(x) = \\frac{1}{2^{m/2}\\Gamma(m/2)}x^{m/2 - 1}e^{-x/2}","title":"Distribui\u00e7\u00e3o Chi-Quadrado"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#propriedades","text":"Se X \\sim \\chi^2(m) , ent\u00e3o: E(X) = m Var(X) = 2m \\psi(t) = \\left(\\frac{1}{1-2t}\\right)^{m/2}, t < \\frac{1}{2}","title":"Propriedades"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#soma-de-chi2","text":"Se X_1, ..., X_k s\u00e3o independentes e cada uma tem grau de liberdade m_i , ent\u00e3o X_1 + ... + X_n tem distribui\u00e7\u00e3o \\chi^2(m_1 + .... + m_k)","title":"Soma de \\chi^2"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#relacao-com-a-normal","text":"Se X tem distribui\u00e7\u00e3o normal padr\u00e3o, Y = X^2 \\sim \\chi^2(1) De fato, se juntarmos as \u00faltimos dois teoremas, veremos que a soma de quadrados de normais independentes e identicamente distribuidas ser\u00e1 \\chi^2(m) , onde m \u00e9 o n\u00famero de parcelas.","title":"Rela\u00e7\u00e3o com a Normal"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#implementacao","text":"import numpy as np import matplotlib.pyplot as plt from scipy.stats import chi2 from matplotlib import animation , cm from IPython.display import HTML # Random Object ro = np . random . default_rng ( 1000 ) # Para assegurar reprodutibilidade degree_freedom = 10 mean , var , skew , kurt = chi2 . stats ( degree_freedom , moments = 'mvsk' ) print ( 'Propriedades' ) print ( 'M\u00e9dia: {}' . format ( mean )) print ( 'Var: {}' . format ( var )) print ( 'Assimetria: {}' . format ( skew )) print ( 'Curtose: {}' . format ( kurt )) Propriedades M\u00e9dia: 10.0 Var: 20.0 Assimetria: 0.8944271909999159 Curtose: 1.2 fig , ax = plt . subplots ( 1 , 1 ) x = np . linspace ( chi2 . ppf ( 0.01 , degree_freedom ), chi2 . ppf ( 0.99 , degree_freedom ), 100 ) ax . plot ( x , chi2 . pdf ( x , degree_freedom ), 'r-' , lw = 5 , alpha = 0.6 , label = 'chi2 pdf' ) r = chi2 . rvs ( degree_freedom , size = 10000 ) ax . hist ( r , density = True , alpha = 0.2 ) ax . legend () plt . show () fig , ax = plt . subplots () line , = ax . plot ( x , chi2 . pdf ( x , degree_freedom ), 'r-' , lw = 5 , alpha = 0.6 ) ax . set_xlim (( 0 , 150 )) ax . set_title ( 'Chi-Square' ) def animate ( i , degree_freedom ): x = np . linspace ( 0 , chi2 . ppf ( 0.99 , degree_freedom + i ), 100 ) line . set_data ( x , chi2 . pdf ( x , degree_freedom + i )) return line , anim = animation . FuncAnimation ( fig , animate , frames = 100 , interval = 50 , fargs = ( degree_freedom ,), repeat = False ) HTML ( anim . to_html5_video ()) Your browser does not support the video tag.","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#distribuicao-conjunta-da-media-e-variancia-amostrais","text":"X_1,...,X_n formam uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal e com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 desconhecidos. Estamos interessados na distribui\u00e7\u00e3o conjunta dos estimadores de m\u00e1xima verossimilhan\u00e7a para m\u00e9dia e vari\u00e2ncia da amostra.","title":"Distribui\u00e7\u00e3o Conjunta da m\u00e9dia e vari\u00e2ncia amostrais"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#teorema-de-basu","text":"Sejam \\hat{\\mu} = \\bar{X}_n e \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 a m\u00e9dia e vari\u00e2ncia amostrais, respectivamente. Ent\u00e3o \\hat{\\mu} tem distribui\u00e7\u00e3o normal com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 /n , enquanto \\hat{\\sigma}^2 tem a distribui\u00e7\u00e3o \\chi^2(n-1) , isto \u00e9, com n-1 grau de liberdade. Al\u00e9m disso elas s\u00e3o independentes. Esse teorema \u00e9 um pouco mais complexo e, na verdade, essa seria uma esp\u00e9cie de aplica\u00e7\u00e3o do teorema, na verdade. O teorema de Basu diz que: Se T \u00e9 uma estat\u00edstica suficiente completa (Considere, nesse teorema, g uma fun\u00e7\u00e3o integr\u00e1vel limitada) para \\theta e A uma estat\u00edstica ancillary, ent\u00e3o T \u00e9 independente de A . Nesse caso \\hat{\\mu} \u00e9 completa suficiente e \\hat{\\sigma}^2 \u00e9 ancillary, por que n\u00e3o depende de \\mu . O mais interessante \u00e9 que essa propriedade \u00e9 s\u00f3 vista com a distribui\u00e7\u00e3o normal ! Olhem a p\u00e1gina 9.","title":"Teorema de Basu"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#demonstracao","text":"O livro tem uma abordagem um pouco mais voltado \u00e0 \u00c1lgebra Linear. Aqui vou mostrar uma ideia um pouco diferente, onde voc\u00eas podem demonstrar os passos, como exerc\u00edcio. Passo 1: \\sum_{i=1}^n X_i^2 = n\\hat{\\sigma}^2 + n\\hat{\\mu}^2 Dica: Escrever \\hat{\\sigma}^2 e abrir em tr\u00eas somat\u00f3rios. Passo 2: \\sum_{i=1}^n (X_i - \\mu)^2 = n\\hat{\\sigma}^2 + n(\\hat{\\mu} - \\mu)^2 Dica: O Passo 1 \u00e9 um caso especial do Passo 2. O processo \u00e9 o mesmo. Passo 3: \\hat{\\mu} \u00e9 independente de X_i - \\hat{\\mu}, i = 1,...,n . Dica: Montar a pdf conjunta de X_1, ..., X_n (j\u00e1 fizemos isso atrave\u015b da verossimilhan\u00e7a) e fazer uma mudan\u00e7a de vari\u00e1vel Y_1 = \\hat{\\mu}, Y_2 = X_2 - \\hat{\\mu}, ..., Y_n = X_n - \\hat{\\mu} . Com essa mudan\u00e7a, \u00e9 poss\u00edvel montar a pdf como fun\u00e7\u00e3o de y_1,...,y_n . Esse processo \u00e9 um pouco mais chato, mas \u00e9 bom lembrar como fazez mudan\u00e7a de vari\u00e1vel para pdfs. Aqui voc\u00ea pode conferir como . \u00c9 importante lembrar que \u00e9 uma fun\u00e7\u00e3o de y ap\u00f3s transformada e n\u00e3o de x . Dica 2: Fatorizar a pdf conjunta. Voc\u00ea vai ver como se destaca a independ\u00eancia aqui. Passo 4: Mostrar que \\hat{\\mu} e \\hat{\\sigma}^2 s\u00e3o independentes.","title":"Demonstra\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#referencias","text":"1 2","title":"Refer\u00eancias"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#simples-visualizacao","text":"Eu gostaria de comparar o que acontece com a m\u00e9dia e vari\u00e2ncia amostral da distribui\u00e7\u00e3o normal e da distribui\u00e7\u00e3o gamma. Para isso, geero amostras de tamanho n , calculo as estat\u00edsticas e salvo. Fa\u00e7o esse procedimento o n\u00famero de pontos que quiser. ite = 10000 n = 10000 # Par\u00e2metros da Normal mu = 5 sigma = 2 # Par\u00e2metros da Gamma alpha = 5 beta = 4 means = np . zeros (( ite , 2 )) variances = np . zeros (( ite , 2 )) for i in range ( ite ): X = ro . normal ( loc = mu , scale = sigma , size = n ) Y = ro . gamma ( shape = alpha , scale = 1 / beta , size = n ) means [ i , 0 ] = np . mean ( X ) means [ i , 1 ] = np . mean ( Y ) variances [ i , 0 ] = np . var ( X , ddof = 0 ) variances [ i , 1 ] = np . var ( Y , ddof = 0 ) coef_normal = np . polyfit ( x = means [:, 0 ], y = variances [:, 0 ], deg = 1 ) coef_gamma = np . polyfit ( x = means [:, 1 ], y = variances [:, 1 ], deg = 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) fig . suptitle ( 'Comparando m\u00e9dia e vari\u00e2ncia amostral' ) ax [ 0 ] . scatter ( means [:, 0 ], variances [:, 0 ]) ax [ 1 ] . scatter ( means [:, 1 ], variances [:, 1 ]) ax [ 0 ] . plot ( means [:, 0 ], coef_normal [ 0 ] * means [:, 0 ] + coef_normal [ 1 ], color = 'red' ) ax [ 1 ] . plot ( means [:, 1 ], coef_gamma [ 0 ] * means [:, 1 ] + coef_gamma [ 1 ], color = 'red' ) ax [ 0 ] . set_xlabel ( r '$\\bar{X}_n$' , fontsize = 18 ) ax [ 1 ] . set_xlabel ( r '$\\bar{X}_n$' , fontsize = 18 ) ax [ 0 ] . set_ylabel ( r '$\\sum (X_i - \\bar{X}_n)^2$' , fontsize = 18 ) ax [ 1 ] . set_ylabel ( r '$\\sum (X_i - \\bar{X}_n)^2$' , fontsize = 18 ) ax [ 0 ] . set_title ( 'Distribui\u00e7\u00e3o Normal' ) ax [ 1 ] . set_title ( 'Distribui\u00e7\u00e3o Gamma' ) ax [ 0 ] . grid ( alpha = 0.5 , linestyle = '--' ) ax [ 1 ] . grid ( alpha = 0.5 , linestyle = '--' ) plt . show () Obs: A n\u00e3o inclina\u00e7\u00e3o da reta n\u00e3o significa que existe independ\u00eancia, mas como s\u00e3o independentes, a gente espera que a inclina\u00e7\u00e3o seja pequena.","title":"Simples visualiza\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#distribuicoes-t-student","text":"Artigo original : Olhe a p\u00e1gina 9!","title":"Distribui\u00e7\u00f5es T Student"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#definicao","text":"Sejam Y \\sim \\chi^2(m) e Z \\sim N(0,1) independentes. Ent\u00e3o X = \\frac{Z}{\\left(\\frac{Y}{m}\\right)^{1/2}} \\sim t(m) onde t(m) \u00e9 a distribui\u00e7\u00e3o t-student com m graus de liberdade.","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#funcao-densidade-de-probabilidade","text":"Para escrever essa fun\u00e7\u00e3o de probabilidade, defina X como acima e W = Y . J\u00e1 sabemos a distribui\u00e7\u00e3o conjunta de Y e Z , pois eles s\u00e3o independentes. Com essa mudan\u00e7a de vari\u00e1vel ( confira aqui se n\u00e3o lembra como \u00e9 feito ), voc\u00ea conseque escrever a distribui\u00e7\u00e3o conjunta de X e W . Depois, basta calcular a distribui\u00e7\u00e3o marginal de X , integrando em W . f(x) = \\frac{\\Gamma\\left(\\frac{m+1}{2}\\right)}{(m\\pi)^{1/2}\\Gamma\\left(\\frac{m}{2}\\right)}\\left(1 + \\frac{x^2}{m} \\right)^{-(m+1)/2}, x \\in \\mathbb{R}, onde \\Gamma \u00e9 a fun\u00e7\u00e3o Gamma , tal que, n \\in \\mathbb{N}, \\Gamma(n) = (n-1)! \\Gamma(z+1) = z\\Gamma(z) \\Gamma(1/2) = \\sqrt{\\pi} Quando m \\leq 1 , a m\u00e9dia \u00e9 divergente. Isso pode ser vizualizado pelo expoente que ser\u00e1 \\leq -1 , o que diverge (lembre de \\int 1/x ). Quando m > 1 , a m\u00e9dia existe e \u00e9 0 pela simetria da distribui\u00e7\u00e3o. Em particular, podemos mostrar que se k < m , E[|X^k|] < + \\infty e se k \\geq m , o momento diverge. Se X \\sim t(m), m > 2 , Var(X) = \\frac{m}{m-2}","title":"Fun\u00e7\u00e3o densidade de probabilidade"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#teorema","text":"Seja X_1, ..., X_n \\overset{iid}{\\sim} N(\\mu,\\sigma^2) . Seja \\sigma ' = \\left[\\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}{n-1}\\right]^{1/2} Ent\u00e3o n^{1/2}(\\bar{X}_n - \\mu)/\\sigma ' \\sim t(n-1)","title":"Teorema"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#relacao-com-a-normal-e-cauchy","text":"Da mesma forma que a distribui\u00e7\u00e3o normal e a distribui\u00e7\u00e3o Cauchy, a distribui\u00e7\u00e3o t \u00e9 centrada em 0 e tem sua moda nesse valor. Entretanto a cauda a distribui\u00e7\u00e3o t (quando x \\to -\\infty ou x \\to +\\infty ), \u00e9 mais pesada, no sentido de que tende para 0 em uma velocidade menor do que a normal. Outra coisa interessante \u00e9 que a ditrivui\u00e7\u00e3o t(1) \u00e9 a distribui\u00e7\u00e3o Cauchy . Al\u00e9m disso, quando n \\to \\infty , converge para a pdf da normal padr\u00e3o ( Normal(0,1) ).","title":"Rela\u00e7\u00e3o com a Normal e Cauchy"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#ferramentas-para-demonstrar-a-convergencia","text":"Teorema de Slutsky : Considere o corol\u00e1rio com f(x,y) = \\frac{x}{y} Lei dos Grandes N\u00fameros : Escreva a qui-quadrado como soma de normais. from scipy.stats import t , norm , cauchy","title":"Ferramentas para demonstrar a converg\u00eancia"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#implementacao_1","text":"Primeiro vamos ver a cara da distribui\u00e7\u00e3o t m = 10 X = t ( df = m ) w = np . arange ( - 3 , 3 , 0.1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax [ 0 ] . plot ( w , X . pdf ( w ), lw = 5 , color = 'orange' ) ax [ 1 ] . plot ( w , X . cdf ( w ), lw = 5 , color = 'orange' ) ax [ 0 ] . set_title ( 'PDF t-Student' ) ax [ 1 ] . set_title ( 'CDF t-Student' ) plt . show () Vamos ver o que acontece quando m \\leq 1 ? ite = 1000 n = 10000 m1 = 10 m2 = 0.5 means = np . zeros (( ite , 2 )) for i in range ( ite ): X = ro . standard_t ( df = m1 , size = n ) Y = ro . standard_t ( df = m2 , size = n ) means [ i , 0 ] = np . mean ( X ) means [ i , 1 ] = np . mean ( Y ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) ax [ 0 ] . hist ( means [:, 0 ], bins = 100 ) ax [ 1 ] . hist ( np . log ( means [:, 1 ]), bins = 10 ) ax [ 0 ] . set_xlabel ( 'E[X]' ) ax [ 1 ] . set_xlabel ( 'log E[X]' ) ax [ 0 ] . set_title ( 'm = 10' ) ax [ 1 ] . set_title ( 'm = 0.5' ) plt . show () <ipython-input-11-2bfd1961d53b>:3: RuntimeWarning: invalid value encountered in log ax[1].hist(np.log(means[:,1]), bins = 10) No eixo x do segundo gr\u00e1fico plotei o logaritmo, dado que alguns resultados eram extremamente grandes! Isso indica visualmente que a m\u00e9dia diverge!","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#relacao-com-a-normal-e-com-cauchy","text":"C = cauchy () Z = norm ( loc = 0 , scale = 1 ) T = t ( df = 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) ax [ 0 ] . plot ( w , C . pdf ( w ), label = 'Cauchy' ) ax [ 0 ] . scatter ( w , T . pdf ( w ), c = 'red' , marker = \"*\" , label = 't-Student' ) ax [ 0 ] . legend () ax [ 0 ] . set_title ( 't-Student e Cauchy quando m = 1' ) ax [ 1 ] . plot ( w , Z . pdf ( w ), label = 'N(0,1)' ) ax [ 1 ] . set_title ( 'Converg\u00eancia da t para a normal' ) for i in np . logspace ( np . log10 ( 1 ), np . log10 ( 20 ), 5 ): T = t ( df = int ( i )) ax [ 1 ] . plot ( w , T . pdf ( w ), linestyle = '--' , alpha = i / 40 + 0.5 , color = 'grey' , label = 't({})' . format ( int ( i ))) ax [ 1 ] . legend ( loc = 'upper right' ) plt . show ()","title":"Rela\u00e7\u00e3o com a Normal e com Cauchy"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#distribuicao-f","text":"Sejam Y \\sim \\chi^2_m e W \\sim \\chi^2_n independentes. Defina X = \\frac{Y/m}{W/n} = \\frac{nY}{mW} Dizemos que X tem distribui\u00e7\u00e3o F . A sua motiva\u00e7\u00e3o vem do teste de hip\u00f3teses que compara vari\u00e2ncias de duas normais.","title":"Distribui\u00e7\u00e3o F"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#funcao-de-densidade-de-probabilidade","text":"Seja X \\sim F_{m,n} . Ent\u00e3o sua pdf tem suporte em x > 0 e pe definida f(x) = \\frac{\\Gamma\\left[\\frac{1}{2}(m+n)\\right]m^{m/2}n^{n/2}}{\\Gamma\\left(\\frac{1}{2}m\\right)\\Gamma\\left(\\frac{1}{2}n\\right)}\\cdot \\frac{x^{(m/2) - 1}}{(mx + n)^{(m+n)/2)}} Observe que ela n\u00e3o \u00e9 sim\u00e9trica em m e n . Assim, se trocarmos eles de lugar, teremos um resultado diferente.","title":"Fun\u00e7\u00e3o de densidade de probabilidade"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#propriedades_1","text":"Seja X \\sim F_{m,n} . Ent\u00e3o 1/X \\sim F_{n,m} . Se Y \\sim t_n , ent\u00e3o Y^2 \\sim F_{1,n} . Existem diversas rela\u00e7\u00f5es que s\u00e3o encontradas com outras distribui\u00e7\u00f5es. Confira aqui E[X] = \\frac{n}{n-2}, n > 2 Var[X] = \\frac{2n^2(m + n - 2)}{m(n-2)^2(n-4)} import numpy as np from scipy.stats import f import matplotlib.pyplot as plt Vamos ver como \u00e9 a cara dessa distribui\u00e7\u00e3o: m , n = 20 , 10 X = f ( dfn = m , dfd = n ) w = np . arange ( 0 , 5 , 0.1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax [ 0 ] . plot ( w , X . pdf ( w ), lw = 5 , color = 'orange' ) ax [ 1 ] . plot ( w , X . cdf ( w ), lw = 5 , color = 'orange' ) ax [ 0 ] . set_title ( 'PDF Distribui\u00e7\u00e3o F' ) ax [ 1 ] . set_title ( 'CDF Distribui\u00e7\u00e3o F' ) plt . show ()","title":"Propriedades"},{"location":"infestatistica/StatisticalInference/StatisticalInference/","text":"Infer\u00eancia Estat\u00edstica Procedimento que objetiva produzir uma proposi\u00e7\u00e3o probabil\u00edsitca sobre um modelo estat\u00edstico. Modelo Estat\u00edstico Identificar vari\u00e1veis aleat\u00f3rias de interesse, especificar uma distribui\u00e7\u00e3o conjunta (ou fam\u00edlia), par\u00e2metros relevantes e uma especifica\u00e7\u00e3o para uma distribui\u00e7\u00e3o para os par\u00e2metros desconhecidos (baysianos adoram essa parte, p \\sim N(0,1) ) Espa\u00e7o dos Par\u00e2metros Uma caracter\u00edstica ou uma combina\u00e7\u00e3o de caracter\u00edsticas para determinar uma distribui\u00e7\u00e3o conjunta para as vari\u00e1veis aleat\u00f3rias forma o par\u00e2metro, que pertence a um espa\u00e7o denominado \\Omega . Estat\u00edstica Fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias observ\u00e1veis Problemas estudados Predi\u00e7\u00e3o: Baseado na \u00e9poca do ano que estamos, fatores climatol\u00f3gicos dos \u00faltimos dias, entre outros fatores, qual a probabilidade de chuva amanh\u00e3? Problemas de decis\u00e3o estat\u00edstica: \u00c9 relacionado ao risco e teste de hip\u00f3teses. Resposta consider\u00e1vel Desenho de experimentos: um psic\u00f3logo quer inferir qu\u00e3o avesso ao risco \u00e9 uma determinada popula\u00e7\u00e3o. Ele pode determinar, desenhar o experimento para isso. Infer\u00eancia Estat\u00edstica com Python import numpy as np import pandas as pd from scipy.stats import poisson import matplotlib.pyplot as plt import seaborn as sns sns . set () Importando os Dados Este banco de dados inclui um registro para cada vazamento ou derramamento de oleoduto relatado \u00e0 Administra\u00e7\u00e3o de Seguran\u00e7a de Dutos e Materiais Perigosos desde 2010. Esses registros incluem a data e hora do incidente, operador e oleoduto, causa do incidente, tipo de l\u00edquido perigoso e quantidade perdida, ferimentos e fatalidades e custos associados. oil_accident_df = pd . read_csv ( '../data/oil_pipeline.csv' ) oil_accident_df . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Report Number Supplemental Number Accident Year Accident Date/Time Operator ID Operator Name Pipeline/Facility Name Pipeline Location Pipeline Type Liquid Type ... Other Fatalities Public Fatalities All Fatalities Property Damage Costs Lost Commodity Costs Public/Private Property Damage Costs Emergency Response Costs Environmental Remediation Costs Other Costs All Costs 871 20120202 17135 2012 6/15/2012 3:50 PM 31476 ROSE ROCK MIDSTREAM L.P. BURKETT DISCHARGE ONSHORE UNDERGROUND CRUDE OIL ... NaN NaN NaN 6020.0 200.0 2500.0 10500.0 8500.0 16000.0 43720 1 rows \u00d7 48 columns cols_of_interest = [ 'Accident Date/Time' , 'Accident State' , 'Pipeline Location' , 'Liquid Type' , 'Net Loss (Barrels)' , 'All Costs' ] data = oil_accident_df [ cols_of_interest ] data [ 'All Costs' ] = data [ 'All Costs' ] / 1000000 # unidade em milh\u00e3o. data . sample () /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Accident Date/Time Accident State Pipeline Location Liquid Type Net Loss (Barrels) All Costs 263 10/11/2010 4:10 PM NJ ONSHORE REFINED AND/OR PETROLEUM PRODUCT (NON-HVL), LI... 0.0 0.0 Vamos entender um pouco como esta informa\u00e7\u00e3o esta organizada. data . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Net Loss (Barrels) All Costs count 2795.000000 2795.000000 mean 132.194050 0.834033 std 1185.019252 16.578298 min 0.000000 0.000000 25% 0.000000 0.005040 50% 0.000000 0.023129 75% 2.000000 0.117232 max 30565.000000 840.526118 Vamos analisar os dados utilizando leis da probabilidade para aprender sobre a popula\u00e7\u00e3o. Veja que n\u00e3o temos a informa\u00e7\u00e3o completa, apenas a partir de 2010. fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 4 )) sns . boxplot ( data [ 'All Costs' ], data = data , ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Custos dos Acidentes por Milh\u00e3o US$' ) sns . boxplot ( data [ 'Net Loss (Barrels)' ], data = data , ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Preju\u00edzo L\u00edquido (Barris)' ) plt . show () Mas esse n\u00e3o era para ser um boxplot? Cade a caixa? Isso indica que valores grandes nos dois dados s\u00e3o muito maiores relativamente aos outros dados. Poder\u00edamos prever o custo de um acidente usando a mediana dos valores? \u00c9 de fato um modelo, mas nesse caso, parece ser ruim dado os valores grandes. O que s\u00e3o esses valores grandes, afinal? Em alguns casos, podem realmente apresentar erros, mas nesse caso fica dif\u00edcil de afirmar. Bom. Podemos, dados esses problemas, trabalhar com outra vari\u00e1vel dispon\u00edvel: o tempo do acidente. Conhecemos uma fam\u00edlia de distribui\u00e7\u00f5es de probabilidade que modela frequ\u00eancia de acidentes em um intervalo de tempo? Distribui\u00e7\u00e3o de Poisson: probabilidade de uma s\u00e9rie de eventos ocorrer num certo per\u00edodo de tempo se estes eventos ocorrem independentemente de quando ocorreu o \u00faltimo evento. De forma geral, podemos dizer que isso \u00e9 verdade para acidentes de \u00f3leo. Assim, temos uma vari\u00e1vel aleat\u00f3ria de interesser X , que indica o n\u00famero de acidentes, j\u00e1 temos uma distribui\u00e7\u00e3o para essa vari\u00e1vel (Poisson) e j\u00e1 temos o par\u00e2metro \\lambda desconhecido. data [ 'Accident Date/Time' ] = pd . to_datetime ( data [ 'Accident Date/Time' ]) totaltimespan = np . max ( data [ 'Accident Date/Time' ]) - np . min ( data [ 'Accident Date/Time' ]) totaltime_hour = ( totaltimespan . days * 24 + totaltimespan . seconds / ( 3600 )) totaltime_month = ( totaltimespan . days + totaltimespan . seconds / ( 3600 * 24 )) * 12 / 365 lmda_h = len ( data ) / totaltime_hour lmda_m = len ( data ) / totaltime_month print ( 'N\u00famero estimado de acidentes por hora: {}' . format ( lmda_h )) print ( 'N\u00famero estimado de acidentes por m\u00eas {}' . format ( lmda_m )) N\u00famero estimado de acidentes por hora: 0.04540255169379675 N\u00famero estimado de acidentes por m\u00eas 33.14386273647162 /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy \"\"\"Entry point for launching an IPython kernel. Poder\u00edamos ter procedimentos para estimar \\lambda , mas por hora, vamos tomar ele como a m\u00e9dia das observa\u00e7\u00f5es. Pela Lei dos Grandes N\u00fameros, sabemos que a m\u00e9dia da Poisson \u00e9 \\lambda e a m\u00e9dia amostral tende para ela. lamda = 33 X = poisson ( lamda ) I = np . arange ( 0 , 60 , 1 ) #intervalo(0,60), passo = 1 samples_poisson = np . sort ( np . random . poisson ( lamda , 10000 )) Y = X . cdf ( samples_poisson ) #fun\u00e7\u00e3o de densidade acumulada fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 8 )) ax [ 0 ] . scatter ( I , X . pmf ( I ) , color = 'purple' ) ax [ 0 ] . set_xlabel ( 'N\u00famero de Acidentes por m\u00eas (n)' ) ax [ 0 ] . set_ylabel ( 'P(X <= n)' ) ax [ 0 ] . set_title ( 'Fun\u00e7\u00e3o de Massa de Probabilidade' ) ax [ 1 ] . scatter ( samples_poisson , Y , color = 'purple' ) ax [ 1 ] . hlines ( 0.5 , xmin = min ( samples_poisson ), xmax = max ( samples_poisson ), linestyle = '--' , color = 'black' ) ax [ 1 ] . set_xlabel ( 'N\u00famero de acidentes por m\u00eas (n)' ) ax [ 1 ] . set_ylabel ( 'P(X <= n)' ) ax [ 1 ] . set_title ( 'Fun\u00e7\u00e3o de Distribui\u00e7\u00e3o Acumulada' ) plt . show () A partir de nosso modelo, j\u00e1 podemos fazer acerta\u00e7\u00f5es probabil\u00edstica! real_data = np . array ( data [ 'Accident Date/Time' ] . apply ( lambda x : ( x . year , x . month ))) accidents_count = { 2010 + i : { m : 0 for m in range ( 1 , 13 )} for i in range ( 8 )} for info in real_data : accidents_count [ info [ 0 ]][ info [ 1 ]] += 1 distribution = [ accidents_count [ y ][ m ] for y in accidents_count . keys () for m in accidents_count [ y ] . keys ()] distribution = distribution [: - 12 ] #Tirando 2 observa\u00e7\u00f5es de 2017 fig , ax = plt . subplots () sns . distplot ( distribution , bins = 15 , ax = ax , label = 'Original data' , kde = False , norm_hist = True ) ax . scatter ( I , X . pmf ( I ) , color = 'purple' , label = 'Nosso modelo' ) ax . legend () ax . set_title ( 'Comparando modelo com dados reais' ) plt . show ()","title":"Introdu\u00e7\u00e3o"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#inferencia-estatistica","text":"Procedimento que objetiva produzir uma proposi\u00e7\u00e3o probabil\u00edsitca sobre um modelo estat\u00edstico.","title":"Infer\u00eancia Estat\u00edstica"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#modelo-estatistico","text":"Identificar vari\u00e1veis aleat\u00f3rias de interesse, especificar uma distribui\u00e7\u00e3o conjunta (ou fam\u00edlia), par\u00e2metros relevantes e uma especifica\u00e7\u00e3o para uma distribui\u00e7\u00e3o para os par\u00e2metros desconhecidos (baysianos adoram essa parte, p \\sim N(0,1) )","title":"Modelo Estat\u00edstico"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#espaco-dos-parametros","text":"Uma caracter\u00edstica ou uma combina\u00e7\u00e3o de caracter\u00edsticas para determinar uma distribui\u00e7\u00e3o conjunta para as vari\u00e1veis aleat\u00f3rias forma o par\u00e2metro, que pertence a um espa\u00e7o denominado \\Omega .","title":"Espa\u00e7o dos Par\u00e2metros"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#estatistica","text":"Fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias observ\u00e1veis","title":"Estat\u00edstica"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#problemas-estudados","text":"Predi\u00e7\u00e3o: Baseado na \u00e9poca do ano que estamos, fatores climatol\u00f3gicos dos \u00faltimos dias, entre outros fatores, qual a probabilidade de chuva amanh\u00e3? Problemas de decis\u00e3o estat\u00edstica: \u00c9 relacionado ao risco e teste de hip\u00f3teses. Resposta consider\u00e1vel Desenho de experimentos: um psic\u00f3logo quer inferir qu\u00e3o avesso ao risco \u00e9 uma determinada popula\u00e7\u00e3o. Ele pode determinar, desenhar o experimento para isso. Infer\u00eancia Estat\u00edstica com Python import numpy as np import pandas as pd from scipy.stats import poisson import matplotlib.pyplot as plt import seaborn as sns sns . set ()","title":"Problemas estudados"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#importando-os-dados","text":"Este banco de dados inclui um registro para cada vazamento ou derramamento de oleoduto relatado \u00e0 Administra\u00e7\u00e3o de Seguran\u00e7a de Dutos e Materiais Perigosos desde 2010. Esses registros incluem a data e hora do incidente, operador e oleoduto, causa do incidente, tipo de l\u00edquido perigoso e quantidade perdida, ferimentos e fatalidades e custos associados. oil_accident_df = pd . read_csv ( '../data/oil_pipeline.csv' ) oil_accident_df . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Report Number Supplemental Number Accident Year Accident Date/Time Operator ID Operator Name Pipeline/Facility Name Pipeline Location Pipeline Type Liquid Type ... Other Fatalities Public Fatalities All Fatalities Property Damage Costs Lost Commodity Costs Public/Private Property Damage Costs Emergency Response Costs Environmental Remediation Costs Other Costs All Costs 871 20120202 17135 2012 6/15/2012 3:50 PM 31476 ROSE ROCK MIDSTREAM L.P. BURKETT DISCHARGE ONSHORE UNDERGROUND CRUDE OIL ... NaN NaN NaN 6020.0 200.0 2500.0 10500.0 8500.0 16000.0 43720 1 rows \u00d7 48 columns cols_of_interest = [ 'Accident Date/Time' , 'Accident State' , 'Pipeline Location' , 'Liquid Type' , 'Net Loss (Barrels)' , 'All Costs' ] data = oil_accident_df [ cols_of_interest ] data [ 'All Costs' ] = data [ 'All Costs' ] / 1000000 # unidade em milh\u00e3o. data . sample () /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Accident Date/Time Accident State Pipeline Location Liquid Type Net Loss (Barrels) All Costs 263 10/11/2010 4:10 PM NJ ONSHORE REFINED AND/OR PETROLEUM PRODUCT (NON-HVL), LI... 0.0 0.0 Vamos entender um pouco como esta informa\u00e7\u00e3o esta organizada. data . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Net Loss (Barrels) All Costs count 2795.000000 2795.000000 mean 132.194050 0.834033 std 1185.019252 16.578298 min 0.000000 0.000000 25% 0.000000 0.005040 50% 0.000000 0.023129 75% 2.000000 0.117232 max 30565.000000 840.526118 Vamos analisar os dados utilizando leis da probabilidade para aprender sobre a popula\u00e7\u00e3o. Veja que n\u00e3o temos a informa\u00e7\u00e3o completa, apenas a partir de 2010. fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 4 )) sns . boxplot ( data [ 'All Costs' ], data = data , ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Custos dos Acidentes por Milh\u00e3o US$' ) sns . boxplot ( data [ 'Net Loss (Barrels)' ], data = data , ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Preju\u00edzo L\u00edquido (Barris)' ) plt . show () Mas esse n\u00e3o era para ser um boxplot? Cade a caixa? Isso indica que valores grandes nos dois dados s\u00e3o muito maiores relativamente aos outros dados. Poder\u00edamos prever o custo de um acidente usando a mediana dos valores? \u00c9 de fato um modelo, mas nesse caso, parece ser ruim dado os valores grandes. O que s\u00e3o esses valores grandes, afinal? Em alguns casos, podem realmente apresentar erros, mas nesse caso fica dif\u00edcil de afirmar. Bom. Podemos, dados esses problemas, trabalhar com outra vari\u00e1vel dispon\u00edvel: o tempo do acidente. Conhecemos uma fam\u00edlia de distribui\u00e7\u00f5es de probabilidade que modela frequ\u00eancia de acidentes em um intervalo de tempo? Distribui\u00e7\u00e3o de Poisson: probabilidade de uma s\u00e9rie de eventos ocorrer num certo per\u00edodo de tempo se estes eventos ocorrem independentemente de quando ocorreu o \u00faltimo evento. De forma geral, podemos dizer que isso \u00e9 verdade para acidentes de \u00f3leo. Assim, temos uma vari\u00e1vel aleat\u00f3ria de interesser X , que indica o n\u00famero de acidentes, j\u00e1 temos uma distribui\u00e7\u00e3o para essa vari\u00e1vel (Poisson) e j\u00e1 temos o par\u00e2metro \\lambda desconhecido. data [ 'Accident Date/Time' ] = pd . to_datetime ( data [ 'Accident Date/Time' ]) totaltimespan = np . max ( data [ 'Accident Date/Time' ]) - np . min ( data [ 'Accident Date/Time' ]) totaltime_hour = ( totaltimespan . days * 24 + totaltimespan . seconds / ( 3600 )) totaltime_month = ( totaltimespan . days + totaltimespan . seconds / ( 3600 * 24 )) * 12 / 365 lmda_h = len ( data ) / totaltime_hour lmda_m = len ( data ) / totaltime_month print ( 'N\u00famero estimado de acidentes por hora: {}' . format ( lmda_h )) print ( 'N\u00famero estimado de acidentes por m\u00eas {}' . format ( lmda_m )) N\u00famero estimado de acidentes por hora: 0.04540255169379675 N\u00famero estimado de acidentes por m\u00eas 33.14386273647162 /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy \"\"\"Entry point for launching an IPython kernel. Poder\u00edamos ter procedimentos para estimar \\lambda , mas por hora, vamos tomar ele como a m\u00e9dia das observa\u00e7\u00f5es. Pela Lei dos Grandes N\u00fameros, sabemos que a m\u00e9dia da Poisson \u00e9 \\lambda e a m\u00e9dia amostral tende para ela. lamda = 33 X = poisson ( lamda ) I = np . arange ( 0 , 60 , 1 ) #intervalo(0,60), passo = 1 samples_poisson = np . sort ( np . random . poisson ( lamda , 10000 )) Y = X . cdf ( samples_poisson ) #fun\u00e7\u00e3o de densidade acumulada fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 8 )) ax [ 0 ] . scatter ( I , X . pmf ( I ) , color = 'purple' ) ax [ 0 ] . set_xlabel ( 'N\u00famero de Acidentes por m\u00eas (n)' ) ax [ 0 ] . set_ylabel ( 'P(X <= n)' ) ax [ 0 ] . set_title ( 'Fun\u00e7\u00e3o de Massa de Probabilidade' ) ax [ 1 ] . scatter ( samples_poisson , Y , color = 'purple' ) ax [ 1 ] . hlines ( 0.5 , xmin = min ( samples_poisson ), xmax = max ( samples_poisson ), linestyle = '--' , color = 'black' ) ax [ 1 ] . set_xlabel ( 'N\u00famero de acidentes por m\u00eas (n)' ) ax [ 1 ] . set_ylabel ( 'P(X <= n)' ) ax [ 1 ] . set_title ( 'Fun\u00e7\u00e3o de Distribui\u00e7\u00e3o Acumulada' ) plt . show () A partir de nosso modelo, j\u00e1 podemos fazer acerta\u00e7\u00f5es probabil\u00edstica! real_data = np . array ( data [ 'Accident Date/Time' ] . apply ( lambda x : ( x . year , x . month ))) accidents_count = { 2010 + i : { m : 0 for m in range ( 1 , 13 )} for i in range ( 8 )} for info in real_data : accidents_count [ info [ 0 ]][ info [ 1 ]] += 1 distribution = [ accidents_count [ y ][ m ] for y in accidents_count . keys () for m in accidents_count [ y ] . keys ()] distribution = distribution [: - 12 ] #Tirando 2 observa\u00e7\u00f5es de 2017 fig , ax = plt . subplots () sns . distplot ( distribution , bins = 15 , ax = ax , label = 'Original data' , kde = False , norm_hist = True ) ax . scatter ( I , X . pmf ( I ) , color = 'purple' , label = 'Nosso modelo' ) ax . legend () ax . set_title ( 'Comparando modelo com dados reais' ) plt . show ()","title":"Importando os Dados"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/","text":"Teste de Hip\u00f3teses: Defini\u00e7\u00f5es Hip\u00f3tese Nula e Alternativa Regi\u00e3o Cr\u00edtica Estat\u00edstica de Teste Fun\u00e7\u00e3o de Poder Tipos de Erro N\u00edvel e tamanho do teste P-valor Temos um problema estat\u00edstico que envolve um par\u00e2metro \\theta tal que tenha valor desconhecido, mas reside em um espa\u00e7o \\Omega . Suponha que particionemos \\Omega = \\Omega_1 \\dot\\cup ~\\Omega_2 e o estat\u00edstico est\u00e1 interessado se \\theta est\u00e1 em \\Omega_0 ou est\u00e1 em \\Omega_1 . Hip\u00f3tese Nula e Alternativa Dizemos que H_0 \u00e9 a hip\u00f3tese de que \\theta \\in \\Omega_0 e chamamos H_0 de hip\u00f3tese nula , enquanto H_1 \u00e9 a hip\u00f3tese alternativa e representa \\theta \\in H_1 . Queremos decidir qual das hip\u00f3teses \u00e9 verdadeira (e s\u00f3 uma ser\u00e1, porque a parti\u00e7\u00e3o \u00e9 disjunta). Se decidimos que \\theta \\in \\Omega_1 , rejeitamos H_0 , e se \\theta \\in \\Omega_0 , n\u00e3o rejeitamos H_0 . Hip\u00f3tese Simples e Composta Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria com pdf f(x|\\theta) . Queremos testar a hip\u00f3tese de que H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Se \\Omega_i contem apenas um valor, ent\u00e3o H_i \u00e9 dita hip\u00f3tese simples. Se cont\u00e9m mais de um valor, dizemos que \u00e9 composta. Hip\u00f3tese Unilateral e Bilateral Seja \\theta um par\u00e2metro unidimensional. Dizemos que a hip\u00f3tese H_0 \u00e9 unilateral (ou one tailed ) quando \u00e9 da forma \\theta \\leq \\theta_0 ou \\theta \\geq \\theta_0 . Ela ser\u00e1 bilateral quando \u00e9 do tipo H_0 \\neq \\theta_0 . Regi\u00e3o Cr\u00edtica Suponha que queremos testar a hip\u00f3tese de que H_0: \\theta \\in \\Omega_0 \\text{ e } H_1: \\theta \\in \\Omega_1 Quando queremos decidir qual hip\u00f3tese escolher, observamos uma amostra dessa distribui\u00e7\u00e3o no espa\u00e7o de amostras S . O dever do estat\u00edstico \u00e9 especificar um procedimento que particione o conjunto em dois subconjuntos S_0 e S_1 , onde S_1 cont\u00e9m os valores de X que rejeitam H_0 . Regi\u00e3o cr\u00edtica \u00e9 o conjunto S_1 , isto \u00e9, o conjunto de amostras que, a partir de um procedimento, rejeita H_0 . Estat\u00edstica de Teste Seja X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Sejam T = r(X) uma estat\u00edstica e R um subconjunto da reta. Suponha que nosso procedimento de teste \u00e9 o seguinte: Rejeitamos H_0 se T\\in R . Chamamos T de estat\u00edstica de teste e R de regi\u00e3o de rejei\u00e7\u00e3o . Dessa forma a regi\u00e3o cr\u00edtica ser\u00e1: S_1 = \\{x \\in S: r(x) \\in R\\} . Na pr\u00e1tica a maioria dos testes \u00e9 do tipo Rejeitamos H_0 se T \\geq c, c \\in \\mathbb{R} . Observa\u00e7\u00e3o sobre a divis\u00e3o de conjuntos \u00c9 importante lembrar que h\u00e1 duas diferentes divis\u00f5es: \\Omega = \\Omega_0 \\dot\\cup \\Omega_1 , que \u00e9 a divis\u00e3o do espa\u00e7o dos par\u00e2metros, e S = S_1 \\dot\\cup S_1 \u00e9 a divis\u00e3o do espa\u00e7o das amostras. Mas qual a rela\u00e7\u00e3o entre eles? Se X \\in S_1 , ent\u00e3o rejeitamos a hip\u00f3tese \\theta \\in \\Omega_0 . Al\u00e9m do mais, podemos encontrar S_1 e S_2 , mas dificilmente saberemos em qual dos conjuntos \\theta pertence. Fun\u00e7\u00e3o de Poder e Tipos de Erro Fun\u00e7\u00e3o Poder Seja \\delta um procedimento de teste (como esse assinalado acima). Se S_1 \u00e9 a regi\u00e3o cr\u00edtica, \\pi(\\theta|\\delta) = P(X \\in S_1|\\theta) = P(T \\in R|\\theta) Sendo que a \u00faltima igualdade ocorre quando o proocedimento de teste \u00e9 o citado acima. O seu significado? \u00c9 a probabilidade, para cada valor de \\theta , de que \\delta rejeita H_0 . Queremos, intuitivamente que: \\theta \\in \\Omega_0 \\rightarrow \\text{ Queremos n\u00e3o rejeitar} H_0 \\rightarrow \\pi(\\theta|\\delta) = 0 \\theta \\in \\Omega_1 \\rightarrow \\text{ Queremos rejeitar} H_0 \\rightarrow \\pi(\\theta|\\delta) = 1 Entretanto isso n\u00e3o \u00e9 em geral o que acontece. Por isso definimos: Erros I e II \\theta \\in \\Omega_0 \\theta \\in \\Omega_1 \\delta rejeita H_0 Erro Tipo I Certo \\delta n\u00e3o rejeita H_0 Certo Erro Tipo II Portanto se \\theta \\in \\Omega_0, \\pi(\\theta|\\delta) \u00e9 a probabilidade de cometermos o erro do tipo I. Se \\theta \\in \\Omega_1, 1 - \\pi(\\theta|\\delta) \u00e9 a probabilidade de cometer o erro do tipo II. N\u00edvel/Tamanho Um teste que satisfaz \\pi(\\theta|\\delta) \\leq \\alpha_0, \\forall \\theta \\in \\Omega_0 \u00e9 chamado de teste n\u00edvel \\alpha_0 , ou que o teste tem n\u00edvel de signific\u00e2ncia \\alpha_0 . O tamanho de um teste \u00e9 \\alpha(\\delta) = \\sup_{\\theta \\in \\Omega_0} \\pi(\\theta, \\delta) . Um teste ter\u00e1 n\u00edvel \\alpha_0 se, e s\u00f3 se, seu tamanho for no m\u00e1ximo \\alpha_0 . P-valor \u00c9 o menor n\u00edvel \\alpha_0 tal que rejeitar\u00edamos a hip\u00f3tese nula a n\u00edvel \\alpha_0 com os dados observados. Se rejeitamos a hip\u00f3tese nula se, e somente se, o p-valor \u00e9 no m\u00e1ximo \\alpha_0 , estamos usando um teste com n\u00edvel de signific\u00e2ncia \\alpha_0 . Equival\u00eancia entre Testes e Conjuntos de Confian\u00e7a Teorema Seja \\vec{X} = (X_1,...,X_n) \\overset{iid}{\\sim} F(\\theta) . Seja g(\\theta) , e suponha que para todo valor c na imagem de g (ou seja, c = g(x) , para algum x ), exista um teste \\delta_c de n\u00edvel \\alpha_0 para a hip\u00f3tese H_{0,c}:g(\\theta) = c, ~ H_{1,c}: g(\\theta) \\neq c Defina \\omega(x) := \\{c: \\delta_c \\text{ n\u00e3o rejeita } H_{0,c} \\text{ se } \\vec{X} = \\vec{x} \\text{ \u00e9 observado } \\} . Ent\u00e3o: P[g(\\theta_0) \\in \\omega(\\vec{X})|\\theta = \\theta_0] \\geq 1 - \\alpha_0, para todo valor \\theta \\in \\Omega . Compreens\u00e3o e Implementa\u00e7\u00e3o Teste de hip\u00f3tese \u00e9 um m\u00e9todo para que fa\u00e7amos decis\u00f5es estat\u00edsticas a partir dos dados. \u00c9 uma forma de compreender (fazer infer\u00e2ncia sobre) um par\u00e2metro. Exemplo: Belgas tem, em m\u00e9dia, maior altura do que peruanos. Exemplo 2: Temperatura n\u00e3o \u00e9 um fator relevante para o processo de cultivo de uva. Estamos avaliando afirma\u00e7\u00f5es mutualmente exclusivas, ou os belgas tem maior altura do que os peruanos, ou n\u00e3o tem! Queremos saber qual dessas afirma\u00e7\u00f5es \u00e9 suportada pelos dados que obtivermos. A hip\u00f3tese nula \u00e9 a afirma\u00e7\u00e3o a ser testada e muitas vezes estabelece uma conjectura de que as caracter\u00edsticas observadas em uma popula\u00e7\u00e3o s\u00e3o por um acaso, isto \u00e9, o fator a ser estudado \"n\u00e3o existe\". Por exemplo: o n\u00famero de voos entre Rio de Janeiro e S\u00e3o Paulo n\u00e3o tem correla\u00e7\u00e3o com o n\u00edvel do mar no Jap\u00e3o. Em geral queremos anul\u00e1-la, rejeit\u00e1-la (da\u00ed o nome). Exemplo Vamos considerar um exemplo simples utilizando a distribui\u00e7\u00e3o normal. \u00c9 a distribui\u00e7\u00e3o com c\u00e1lculos simples e uma boa visualiza\u00e7\u00e3o. A ideia nesse exemplo vai ser a seguinte: O pre\u00e7o do quilo ouro varia diariamente e essa varia\u00e7\u00e3o em unidade de d\u00f3lares ser\u00e1 nosso objeto de interesse. Por exemplo: Se no dia 1 o pre\u00e7o mil e no dia 2 o pre\u00e7o era 1 050 e no dia 3 o pre\u00e7o \u00e9 1 025 temos que X_1 = 50 e X_2 = -25 . Vamos supor que as varia\u00e7\u00f5es entre dois diferentes pares de dias s\u00e3o independentes (essa j\u00e1 uma simplifica\u00e7\u00e3o da realidade!) Primeiro vamos importar os dados. import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.patches as mpatches import seaborn as sns sns . set () gold_df = pd . read_csv ( '../data/gold.csv' , low_memory = False , header = [ 2 , 3 , 4 ]) # There has a lot of data. I will get average diary from USD price gold_df = gold_df [[( 'Priced In' , 'Price Type' , 'Summary' ), ( 'USD' , 'Ask' , 'Average' )]] gold_df . columns = [ 'Day' , 'Price' ] gold_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Day Price 0 1/01/68 NaN 1 2/01/68 NaN 2 3/01/68 NaN 3 4/01/68 NaN 4 5/01/68 NaN Observe que existem diversos Nan values. Na pr\u00e1tica eu teria que fazer alguma esp\u00e9cie de limpeza rigorosa. Nesse caso, para tornar tudo bem simples, vou apenas limpar. Tamb\u00e9m precisamos garantir que as informa\u00e7\u00f5es estejam em formato float . # Inplace assegura que eu n\u00e3o crie outro DataFrame gold_df . dropna ( inplace = True ) gold_df . Price = gold_df . Price . apply ( lambda x : float ( x . strip () . replace ( ',' , '' ))) plt . plot ( gold_df [ 'Price' ]) plt . title ( 'Pre\u00e7o do Ouro em D\u00f3lares' ) plt . show () variation = gold_df . Price . diff () sns . violinplot ( x = variation ) plt . xlim (( - 50 , 50 )) plt . title ( 'Distribui\u00e7\u00e3o da varia\u00e7\u00e3o di\u00e1ria' ) plt . show () De fato n\u00e3o parece uma normal (na verdade uma distribui\u00e7\u00e3o de cauda mais pesada talvez fosse interessante. Mas tudo bem!), mas vamos modelar dessa forma. A partir de agora vamos nos preocupar mais com as defini\u00e7\u00f5es para dar a devida interpreta\u00e7\u00e3o. O exemplo \u00e9 s\u00f3 motivador. Qual hip\u00f3tese queremos testar? O que queremos saber sobre a varia\u00e7\u00e3o? A pergunta que nasce \u00e9 o seguinte: ser\u00e1 que a m\u00e9dia dessa distribui\u00e7\u00e3o \u00e9 0? Isto \u00e9, ser\u00e1 que se calcularmos as m\u00e9dias das varia\u00e7\u00f5es, teremos que com infinitas observa\u00e7\u00f5es, o resultado seria 0? Isso \u00e9 importante porque vai nos ajudar a identificar se existe uma tend\u00eancia de crescimento nas varia\u00e7\u00f5es di\u00e1rias. Hip\u00f3tese Nula: \\mu = 0 , onde X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Hip\u00f3tese Alternativa: \\mu \\neq 0 . Vamos supor que \\sigma \u00e9 conhecido e que \\sigma^2 = 73 , mas a m\u00e9dia \u00e9 desconhecida. Quem \u00e9 \\Omega_0 e \\Omega_1 ? \\Omega_0 \u00e9 a regi\u00e3o dos par\u00e2metros onde a hip\u00f3tese nula \u00e9 verdadeira, isto \u00e9 \\Omega_0 = \\{0\\} , \u00e9 um conjunto unit\u00e1rio. Por outro lado \\Omega_1 = \\mathbb{R} - \\{0\\} , por que a m\u00e9dia pode assumir, em teoria, qualquer valor real. Qual \u00e9 a regi\u00e3o cr\u00edtica? Bom, ainda n\u00e3o podemos determinar essa resposta, afinal para determinar a regi\u00e3o cr\u00edtica (subconjunto do espa\u00e7o dos estados em que se rejeita a hip\u00f3tese nula), precisamos de um procedimento de teste. Mas vamos imaginar que o espa\u00e7o de estados \u00e9 S = \\mathbb{R}^2 , pois vamos considerar apenas duas amostras, inicialmente (queremos visualizar S). Qual ser\u00e1 nosso procedimento de teste? Procedimento de teste \u00e9 uma maneira de tomarmos uma decis\u00e3o. Ele tem a forma: rejeitamos H_0 se isso acontecer. Um exemplo bobo seria: Rejeitamos a hip\u00f3tese nula se 0 \\not \\in (\\min(X_1, X_2), \\max(X_1, X_2)) O problema \u00e9 que em geral esse tipo de procedimento n\u00e3o \u00e9 interessante. Para isso estabelecemos uma estat\u00edstica de teste T e uma Regi\u00e3o de Rejei\u00e7\u00e3o T , tal que nosso procedimento seja: Rejeitamos H_0 se T \\in R . Nesse caso vamos considerar T = \\frac{X_1 + X_2}{2} e vamos rejeitar a hip\u00f3tese se |T| estiver muito longe de 0 , isto \u00e9, se |T| \\geq c . Portanto definimor nossa Regi\u00e3o de Rejei\u00e7\u00e3o como (-\\infty,-c]\\cup[c, + \\infty) , o que reduz nosso problema a determinar c . Qual c seria razo\u00e1vel? 4, 5, 1? Essa pergunta n\u00e3o vai ser respondida. Antes vamos visualizar como fica a regi\u00e3o cr\u00edtica (2). Rejeitamos a hip\u00f3tese se \\left|\\frac{X_1 + X_2}{2}\\right| \\geq c \\rightarrow |X_1 + X_2| \\geq 2c \\rightarrow X_1 + X_2 \\geq 2c \\text{ ou } X_1 + X_2 \\leq -2c Assim: S_1 = \\{(x_1, x_2): x_1 + x_2 \\geq 2c \\text{ ou } x_1 + x_2 \\leq -2c\\} C = [ 1 , 10 , 30 ] decision = lambda x1 , x2 , c : 1 * np . logical_or ( x1 + x2 >= 2 * c , x1 + x2 <= - 2 * c ) x1 , x2 = np . meshgrid ( np . arange ( - 50 , 50 , 0.5 ), np . arange ( - 50 , 50 , 0.5 )) fig , ax = plt . subplots ( 1 , 3 , figsize = ( 21 , 7 )) for i , c in enumerate ( C ): ax [ i ] . contourf ( x1 , x2 , decision ( x1 , x2 , c ), levels = [ 0 , 0.5 , 1 ], colors = [ '#fdcdac' , '#cbd5e8' ]) ax [ i ] . set_xlabel ( r '$x_1$' , fontsize = 20 ) ax [ i ] . set_ylabel ( r '$x_2$' , fontsize = 20 ) ax [ i ] . set_title ( 'Regi\u00e3o Cr\u00edtica quando c = {}' . format ( c ), fontsize = 20 ) ax [ i ] . legend ( handles = [ mpatches . Patch ( color = '#fdcdac' , label = r '$S_0$' ), mpatches . Patch ( color = '#cbd5e8' , label = r '$S_1$' )]) sample = gold_df . Price . diff () . sample ( n = 2 ) X1 , X2 = sample . iloc [ 0 ], sample . iloc [ 1 ] ax [ i ] . scatter ( X1 , X2 , color = 'black' ) ax [ i ] . text ( X1 + 1 , X2 + 1 , s = r '$(X_1, X_2)$' , fontsize = 15 ) Como a fun\u00e7\u00e3o poder entra nessa hist\u00f3ria? A fun\u00e7\u00e3o poder \u00e9 uma fun\u00e7\u00e3o do par\u00e2metro, no caso \\mu , e retorna a probabilidade de rejeitarmos a hip\u00f3tese, considerando esse par\u00e2metro. Isto \u00e9, \\pi(\\mu) = P_{\\mu}((X_1, X_2) \\in S_1) O que poder\u00edamos fazer, ent\u00e3o, \u00e9 obter a distribui\u00e7\u00e3o conjunta de (X_1, X_2) e integrar na regi\u00e3o S_1 . Vamos considerar dois casos separados: \\mu \\in \\Omega_0 : N\u00e3o sabemos disso, e em geral n\u00e3o \u00e9 poss\u00edvel sabermos. Nesse caso \\pi(\\mu) indica a probabilidade de rejeitarmos a hip\u00f3tese nula, mesmo ela sendo verdadeira (chamamos isso de Erro do Tipo I). \\mu \\in \\Omega_1 : Nesse caso \\pi(\\mu) indica a probabilidade de rejeitarmos a hip\u00f3tese nula, quando de ela \u00e9 falsa. Nesse caso 1 - \\pi(\\mu) \u00e9 a probabilidade de n\u00e3o rejeitarmos a hip\u00f3tese nula, quando de fato dever\u00edamos (chamamos de Erro do Tipo II). ] A fun\u00e7\u00e3o poder se trata mais do teste que estamos usando do que os dados em si. Por isso, podemos comparar testes usando essa fun\u00e7\u00e3o poder. Ent\u00e3o vamos ver nesse caso quem \u00e9 a fun\u00e7\u00e3o poder! Vou calcular usando um m\u00e9todo num\u00e9rico para que peguem a ideia. Agora \u00e9 poss\u00edvel fazer as contas, mas nem sempre \u00e9 trivial. Assim teremos apenas aproxima\u00e7\u00e3o da fun\u00e7\u00e3o poder. C = [ 1 , 10 , 30 ] mu_v = np . arange ( - 100 , 100 , 1 ) n = 10000 power = np . zeros (( len ( C ), len ( mu_v ))) for i , c in enumerate ( C ): for j , mu in enumerate ( mu_v ): X1 = np . random . normal ( loc = mu , scale = 73 , size = n ) X2 = np . random . normal ( loc = mu , scale = 73 , size = n ) p = ( sum ( X1 + X2 >= 2 * c ) + sum ( X1 + X2 <= - 2 * c )) / n power [ i , j ] = p plt . plot ( mu_v , power [ i ,:], label = 'c = {}' . format ( c )) plt . title ( 'Fun\u00e7\u00e3o poder' ) plt . xlabel ( r '$\\mu$' ) plt . ylabel ( 'Prob' ) plt . legend () plt . show () Vamos definir c agora? Sim, vamos. Para isso vamos usar o ponto (4) e a defini\u00e7\u00e3o de tamanho do teste. Uma forma poss\u00edvel de se fazer isso \u00e9 a seguinte: limitamos o Erro I por \\alpha_0 e miniminizamos o Erro II, isso \u00e9 minimizamos 1 - \\pi(\\mu) quando \\mu \\neq 0 , ou melhor, maximizamos \\pi(\\mu) . Para isso dizemos que o tamanho do teste \u00e9 o m\u00e1ximo da fun\u00e7\u00e3o poder, quando \\theta \\in \\Omega_0 . Nesse caso \\alpha(\\delta) = \\pi(0) . Queremos, ent\u00e3o que: \\pi(0) = P_{\\mu = 0}(T \\geq c) \\leq \\alpha_0 Precisamos ent\u00e3o encontrar c tal que \\pi(\\mu) = P_{\\mu \\neq 0}(T \\ge c) seja maximado. Observamos que, quando \\mu = 0 , T \\sim N(0, \\sigma^2/2) \\rightarrow \\sqrt{2}T/\\sigma = Z \\sim N(0,1) . Logo P(|T| \\ge c) = P(|Z| \\ge \\sqrt{2}c/\\sigma) = 2(1 - \\Phi(\\sqrt{2}c/\\sigma)) \\leq \\alpha_0 Para maximizar \\pi(\\mu) em \\Omega_1 , observamos que \\pi(\\mu) decresce com c (os gr\u00e1ficos acima representam bem isso). Como queremos maximizar, gostar\u00edamos de tomar c o m\u00ednimo poss\u00edvel, restrito a 2(1 - \\Phi(\\sqrt{2}c/\\sigma)) \\leq \\alpha_0 \\rightarrow 1 - \\alpha_0/2 \\leq \\Phi(\\sqrt{2}c/\\sigma) como vimos acima. Estamos lidando com uma fun\u00e7\u00e3o invers\u00edvel, ent\u00e3o \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) \\leq c O melhor valor de c que respeita essa condi\u00e7\u00e3o e maximiza a rela\u00e7\u00e3o \u00e9, portanto c = \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) from scipy.stats import norm Lembre que \\alpha_0 indica o m\u00e1ximo de Erro I que aceitamos. alpha0 = 0.05 c = np . sqrt ( 73 ) / np . sqrt ( 2 ) * norm . ppf ( 1 - alpha0 / 2 ) print ( c ) 11.841167465893536 \u00c9 bem pr\u00f3ximo do gr\u00e1fico acima mostrado, quando testamos para c = 10 . t = np . arange ( - 20 , 20 , 0.1 ) X = norm ( loc = 0 , scale = np . sqrt ( 73 ) / np . sqrt ( 2 )) plt . plot ( t , X . pdf ( t )) plt . fill_between ( t [( t < - c )], X . pdf ( t [( t < - c )]), color = 'blue' ) plt . fill_between ( t [( t > c )], X . pdf ( t [( t > c )]), color = 'blue' ) plt . title ( 'Distribui\u00e7\u00e3o Normal e Regi\u00e3o de Rejei\u00e7\u00e3o' ) plt . show () Por exemplo vamos tirar duas amostras de nossa distribui\u00e7\u00e3o X1 , X2 = gold_df . Price . diff () . sample ( 2 ) T = np . abs ( X1 + X2 ) / 2 T >= c False Mas como escolher \\alpha_0 agora? Agora entra o conceito mais complexo, o do p-valor. Ele est\u00e1 associado \u00e0 ideia de escolher o menor \\alpha_0 poss\u00edvel, para que rejeitemos a hip\u00f3tese nula. Isso significa o seguinte: Queremos minimizar o Erro do Tipo I e rejeitar a Hip\u00f3tese Nula com os dados que obtivemos. Se o p-valor for muito alto, significa que o Erro do Tipo I \u00e9 grande se rejeitarmos a hip\u00f3tese nula. Voc\u00ea apostaria que podemos rejeitar a hip\u00f3tese nula nesse caso? Agora, se o p-valor for pequeno e rejeitarmos nossa hip\u00f3tese nula, o erro do tipo I vai ser pequeno, ent\u00e3o apostar que a hip\u00f3tese nula deva ser rejeitada \u00e9 mais confort\u00e1vel. Assim n\u00e3o escolhemos \\alpha_0 , s\u00f3 observamos seu menor valor e vemos se faz sentido. Em geral se p-valor < 0.05, as pessoas rejeitam a hip\u00f3tese nula. No nosso caso calcular o p-valor \u00e9 tranquilo. Para calcular o p-valor, precisamos dos dados . Queremos rejeitar a hip\u00f3tese nula, isto \u00e9, queremos que t \\geq c que \\alpha_0 seja o menor poss\u00edvel, onde t \u00e9 o valor observado de T . Vamos diminuindo \\alpha_0 e para cada \\alpha_0 podemos calcular c e verificamos se t \\ge c . Podemos fazer isso at\u00e9 que c = t , assim: t = \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) \\rightarrow \\alpha_0 = 2\\left(1 - \\Phi\\left(\\frac{\\sqrt{2}}{\\sigma}t\\right)\\right) p_value = 2 * ( 1 - norm . cdf ( np . sqrt ( 2 ) / np . sqrt ( 73 ) * T )) print ( p_value ) 0.8477386286989927 Como o p-valor \u00e9 alto, n\u00e3o faz sentido rejeitar a hip\u00f3tese nula. Encerramos a atividade aqui! Testes de Raz\u00e3o Verossimilhan\u00e7a S\u00e3o testes baseados na verossimilha\u00e7a do modelo f_n(x|\\theta) . Suponha que queremos testar a hip\u00f3tese: H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Vamos lembrar que a fun\u00e7\u00e3o de verossimilhan\u00e7a tende a ser mais alta pr\u00f3ximo do valor verdadeiro do par\u00e2metro. Com isso em mente, gostar\u00edamos de saber se a verossimilhan\u00e7a \u00e9 maior em \\Omega_0 ou em \\Omega_1 . Para isso, definimos a estat\u00edstica de raz\u00e3o de verossimilhan\u00e7a : \\Lambda(x) = \\frac{\\sup_{\\theta \\in \\Omega_0}f_n(x|\\theta)}{\\sup_{\\theta \\in \\Omega}f_n(x|\\theta)} Observe que o denominador \u00e9 o valor da fun\u00e7\u00e3o de verossimilhan\u00e7a no Estimador de M\u00e1xima Verossimilhan\u00e7a. Se o par\u00e2metro verdadeiro estiver em \\Omega_0 , o n\u00famerador deve ser mais alto em \\Omega_0 , ent\u00e3o a estat\u00edstica se aproxima de 1. Baseado nisso, o teste de raz\u00e3o de verossimilhan\u00e7a \u00e9: Rejeitamos H_0 se \\Lambda(x) \\le k , para algum k . Teorema: Seja \\Omega \\in \\mathbb{R}^p aberto e suponha que H_0 seja \\theta_{i_1} = \\theta_{01}, ..., \\theta_{i_k} = \\theta_{0k} , onde \\theta = (\\theta_1, ..., \\theta_p) . Assuma que H_0 seja verdadeira e a fun\u00e7\u00e3o de verossimilhan\u00e7a satisfa\u00e7a as condi\u00e7\u00f5es para que o MLE seja assintoticamente normal e assintoticamente eficiente. Ent\u00e3o: -2\\log\\Lambda(x) \\overset{d}{\\to} \\chi^2(k) (converge em distribui\u00e7\u00e3o quando n \\to \\infty ). A demonstra\u00e7\u00e3o pode ser encontrada no StatLect Testes n\u00e3o enviesados Um teste \u00e9 dito n\u00e3o enviesado se \\forall \\theta \\in \\Omega_0 e \\theta ' \\in \\Omega_1, \\pi(\\theta|\\delta) \\le \\pi(\\theta '|\\delta) N\u00e3o \u00e9 muito utilizado dado seu dif\u00edcil c\u00e1lculo num\u00e9rico e n\u00e3o traz resultados quem valem a pena.","title":"Teste de Hip\u00f3teses"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#teste-de-hipoteses-definicoes","text":"Hip\u00f3tese Nula e Alternativa Regi\u00e3o Cr\u00edtica Estat\u00edstica de Teste Fun\u00e7\u00e3o de Poder Tipos de Erro N\u00edvel e tamanho do teste P-valor Temos um problema estat\u00edstico que envolve um par\u00e2metro \\theta tal que tenha valor desconhecido, mas reside em um espa\u00e7o \\Omega . Suponha que particionemos \\Omega = \\Omega_1 \\dot\\cup ~\\Omega_2 e o estat\u00edstico est\u00e1 interessado se \\theta est\u00e1 em \\Omega_0 ou est\u00e1 em \\Omega_1 .","title":"Teste de Hip\u00f3teses: Defini\u00e7\u00f5es"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#hipotese-nula-e-alternativa","text":"Dizemos que H_0 \u00e9 a hip\u00f3tese de que \\theta \\in \\Omega_0 e chamamos H_0 de hip\u00f3tese nula , enquanto H_1 \u00e9 a hip\u00f3tese alternativa e representa \\theta \\in H_1 . Queremos decidir qual das hip\u00f3teses \u00e9 verdadeira (e s\u00f3 uma ser\u00e1, porque a parti\u00e7\u00e3o \u00e9 disjunta). Se decidimos que \\theta \\in \\Omega_1 , rejeitamos H_0 , e se \\theta \\in \\Omega_0 , n\u00e3o rejeitamos H_0 .","title":"Hip\u00f3tese Nula e Alternativa"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#hipotese-simples-e-composta","text":"Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria com pdf f(x|\\theta) . Queremos testar a hip\u00f3tese de que H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Se \\Omega_i contem apenas um valor, ent\u00e3o H_i \u00e9 dita hip\u00f3tese simples. Se cont\u00e9m mais de um valor, dizemos que \u00e9 composta.","title":"Hip\u00f3tese Simples e Composta"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#hipotese-unilateral-e-bilateral","text":"Seja \\theta um par\u00e2metro unidimensional. Dizemos que a hip\u00f3tese H_0 \u00e9 unilateral (ou one tailed ) quando \u00e9 da forma \\theta \\leq \\theta_0 ou \\theta \\geq \\theta_0 . Ela ser\u00e1 bilateral quando \u00e9 do tipo H_0 \\neq \\theta_0 .","title":"Hip\u00f3tese Unilateral e Bilateral"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#regiao-critica","text":"Suponha que queremos testar a hip\u00f3tese de que H_0: \\theta \\in \\Omega_0 \\text{ e } H_1: \\theta \\in \\Omega_1 Quando queremos decidir qual hip\u00f3tese escolher, observamos uma amostra dessa distribui\u00e7\u00e3o no espa\u00e7o de amostras S . O dever do estat\u00edstico \u00e9 especificar um procedimento que particione o conjunto em dois subconjuntos S_0 e S_1 , onde S_1 cont\u00e9m os valores de X que rejeitam H_0 . Regi\u00e3o cr\u00edtica \u00e9 o conjunto S_1 , isto \u00e9, o conjunto de amostras que, a partir de um procedimento, rejeita H_0 .","title":"Regi\u00e3o Cr\u00edtica"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#estatistica-de-teste","text":"Seja X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Sejam T = r(X) uma estat\u00edstica e R um subconjunto da reta. Suponha que nosso procedimento de teste \u00e9 o seguinte: Rejeitamos H_0 se T\\in R . Chamamos T de estat\u00edstica de teste e R de regi\u00e3o de rejei\u00e7\u00e3o . Dessa forma a regi\u00e3o cr\u00edtica ser\u00e1: S_1 = \\{x \\in S: r(x) \\in R\\} . Na pr\u00e1tica a maioria dos testes \u00e9 do tipo Rejeitamos H_0 se T \\geq c, c \\in \\mathbb{R} .","title":"Estat\u00edstica de Teste"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#observacao-sobre-a-divisao-de-conjuntos","text":"\u00c9 importante lembrar que h\u00e1 duas diferentes divis\u00f5es: \\Omega = \\Omega_0 \\dot\\cup \\Omega_1 , que \u00e9 a divis\u00e3o do espa\u00e7o dos par\u00e2metros, e S = S_1 \\dot\\cup S_1 \u00e9 a divis\u00e3o do espa\u00e7o das amostras. Mas qual a rela\u00e7\u00e3o entre eles? Se X \\in S_1 , ent\u00e3o rejeitamos a hip\u00f3tese \\theta \\in \\Omega_0 . Al\u00e9m do mais, podemos encontrar S_1 e S_2 , mas dificilmente saberemos em qual dos conjuntos \\theta pertence.","title":"Observa\u00e7\u00e3o sobre a divis\u00e3o de conjuntos"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#funcao-de-poder-e-tipos-de-erro","text":"","title":"Fun\u00e7\u00e3o de Poder e Tipos de Erro"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#funcao-poder","text":"Seja \\delta um procedimento de teste (como esse assinalado acima). Se S_1 \u00e9 a regi\u00e3o cr\u00edtica, \\pi(\\theta|\\delta) = P(X \\in S_1|\\theta) = P(T \\in R|\\theta) Sendo que a \u00faltima igualdade ocorre quando o proocedimento de teste \u00e9 o citado acima. O seu significado? \u00c9 a probabilidade, para cada valor de \\theta , de que \\delta rejeita H_0 . Queremos, intuitivamente que: \\theta \\in \\Omega_0 \\rightarrow \\text{ Queremos n\u00e3o rejeitar} H_0 \\rightarrow \\pi(\\theta|\\delta) = 0 \\theta \\in \\Omega_1 \\rightarrow \\text{ Queremos rejeitar} H_0 \\rightarrow \\pi(\\theta|\\delta) = 1 Entretanto isso n\u00e3o \u00e9 em geral o que acontece. Por isso definimos:","title":"Fun\u00e7\u00e3o Poder"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#erros-i-e-ii","text":"\\theta \\in \\Omega_0 \\theta \\in \\Omega_1 \\delta rejeita H_0 Erro Tipo I Certo \\delta n\u00e3o rejeita H_0 Certo Erro Tipo II Portanto se \\theta \\in \\Omega_0, \\pi(\\theta|\\delta) \u00e9 a probabilidade de cometermos o erro do tipo I. Se \\theta \\in \\Omega_1, 1 - \\pi(\\theta|\\delta) \u00e9 a probabilidade de cometer o erro do tipo II.","title":"Erros I e II"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#niveltamanho","text":"Um teste que satisfaz \\pi(\\theta|\\delta) \\leq \\alpha_0, \\forall \\theta \\in \\Omega_0 \u00e9 chamado de teste n\u00edvel \\alpha_0 , ou que o teste tem n\u00edvel de signific\u00e2ncia \\alpha_0 . O tamanho de um teste \u00e9 \\alpha(\\delta) = \\sup_{\\theta \\in \\Omega_0} \\pi(\\theta, \\delta) . Um teste ter\u00e1 n\u00edvel \\alpha_0 se, e s\u00f3 se, seu tamanho for no m\u00e1ximo \\alpha_0 .","title":"N\u00edvel/Tamanho"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#p-valor","text":"\u00c9 o menor n\u00edvel \\alpha_0 tal que rejeitar\u00edamos a hip\u00f3tese nula a n\u00edvel \\alpha_0 com os dados observados. Se rejeitamos a hip\u00f3tese nula se, e somente se, o p-valor \u00e9 no m\u00e1ximo \\alpha_0 , estamos usando um teste com n\u00edvel de signific\u00e2ncia \\alpha_0 .","title":"P-valor"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#equivalencia-entre-testes-e-conjuntos-de-confianca","text":"","title":"Equival\u00eancia entre Testes e Conjuntos de Confian\u00e7a"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#teorema","text":"Seja \\vec{X} = (X_1,...,X_n) \\overset{iid}{\\sim} F(\\theta) . Seja g(\\theta) , e suponha que para todo valor c na imagem de g (ou seja, c = g(x) , para algum x ), exista um teste \\delta_c de n\u00edvel \\alpha_0 para a hip\u00f3tese H_{0,c}:g(\\theta) = c, ~ H_{1,c}: g(\\theta) \\neq c Defina \\omega(x) := \\{c: \\delta_c \\text{ n\u00e3o rejeita } H_{0,c} \\text{ se } \\vec{X} = \\vec{x} \\text{ \u00e9 observado } \\} . Ent\u00e3o: P[g(\\theta_0) \\in \\omega(\\vec{X})|\\theta = \\theta_0] \\geq 1 - \\alpha_0, para todo valor \\theta \\in \\Omega .","title":"Teorema"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#compreensao-e-implementacao","text":"Teste de hip\u00f3tese \u00e9 um m\u00e9todo para que fa\u00e7amos decis\u00f5es estat\u00edsticas a partir dos dados. \u00c9 uma forma de compreender (fazer infer\u00e2ncia sobre) um par\u00e2metro. Exemplo: Belgas tem, em m\u00e9dia, maior altura do que peruanos. Exemplo 2: Temperatura n\u00e3o \u00e9 um fator relevante para o processo de cultivo de uva. Estamos avaliando afirma\u00e7\u00f5es mutualmente exclusivas, ou os belgas tem maior altura do que os peruanos, ou n\u00e3o tem! Queremos saber qual dessas afirma\u00e7\u00f5es \u00e9 suportada pelos dados que obtivermos. A hip\u00f3tese nula \u00e9 a afirma\u00e7\u00e3o a ser testada e muitas vezes estabelece uma conjectura de que as caracter\u00edsticas observadas em uma popula\u00e7\u00e3o s\u00e3o por um acaso, isto \u00e9, o fator a ser estudado \"n\u00e3o existe\". Por exemplo: o n\u00famero de voos entre Rio de Janeiro e S\u00e3o Paulo n\u00e3o tem correla\u00e7\u00e3o com o n\u00edvel do mar no Jap\u00e3o. Em geral queremos anul\u00e1-la, rejeit\u00e1-la (da\u00ed o nome).","title":"Compreens\u00e3o e Implementa\u00e7\u00e3o"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#exemplo","text":"Vamos considerar um exemplo simples utilizando a distribui\u00e7\u00e3o normal. \u00c9 a distribui\u00e7\u00e3o com c\u00e1lculos simples e uma boa visualiza\u00e7\u00e3o. A ideia nesse exemplo vai ser a seguinte: O pre\u00e7o do quilo ouro varia diariamente e essa varia\u00e7\u00e3o em unidade de d\u00f3lares ser\u00e1 nosso objeto de interesse. Por exemplo: Se no dia 1 o pre\u00e7o mil e no dia 2 o pre\u00e7o era 1 050 e no dia 3 o pre\u00e7o \u00e9 1 025 temos que X_1 = 50 e X_2 = -25 . Vamos supor que as varia\u00e7\u00f5es entre dois diferentes pares de dias s\u00e3o independentes (essa j\u00e1 uma simplifica\u00e7\u00e3o da realidade!) Primeiro vamos importar os dados. import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.patches as mpatches import seaborn as sns sns . set () gold_df = pd . read_csv ( '../data/gold.csv' , low_memory = False , header = [ 2 , 3 , 4 ]) # There has a lot of data. I will get average diary from USD price gold_df = gold_df [[( 'Priced In' , 'Price Type' , 'Summary' ), ( 'USD' , 'Ask' , 'Average' )]] gold_df . columns = [ 'Day' , 'Price' ] gold_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Day Price 0 1/01/68 NaN 1 2/01/68 NaN 2 3/01/68 NaN 3 4/01/68 NaN 4 5/01/68 NaN Observe que existem diversos Nan values. Na pr\u00e1tica eu teria que fazer alguma esp\u00e9cie de limpeza rigorosa. Nesse caso, para tornar tudo bem simples, vou apenas limpar. Tamb\u00e9m precisamos garantir que as informa\u00e7\u00f5es estejam em formato float . # Inplace assegura que eu n\u00e3o crie outro DataFrame gold_df . dropna ( inplace = True ) gold_df . Price = gold_df . Price . apply ( lambda x : float ( x . strip () . replace ( ',' , '' ))) plt . plot ( gold_df [ 'Price' ]) plt . title ( 'Pre\u00e7o do Ouro em D\u00f3lares' ) plt . show () variation = gold_df . Price . diff () sns . violinplot ( x = variation ) plt . xlim (( - 50 , 50 )) plt . title ( 'Distribui\u00e7\u00e3o da varia\u00e7\u00e3o di\u00e1ria' ) plt . show () De fato n\u00e3o parece uma normal (na verdade uma distribui\u00e7\u00e3o de cauda mais pesada talvez fosse interessante. Mas tudo bem!), mas vamos modelar dessa forma. A partir de agora vamos nos preocupar mais com as defini\u00e7\u00f5es para dar a devida interpreta\u00e7\u00e3o. O exemplo \u00e9 s\u00f3 motivador. Qual hip\u00f3tese queremos testar? O que queremos saber sobre a varia\u00e7\u00e3o? A pergunta que nasce \u00e9 o seguinte: ser\u00e1 que a m\u00e9dia dessa distribui\u00e7\u00e3o \u00e9 0? Isto \u00e9, ser\u00e1 que se calcularmos as m\u00e9dias das varia\u00e7\u00f5es, teremos que com infinitas observa\u00e7\u00f5es, o resultado seria 0? Isso \u00e9 importante porque vai nos ajudar a identificar se existe uma tend\u00eancia de crescimento nas varia\u00e7\u00f5es di\u00e1rias. Hip\u00f3tese Nula: \\mu = 0 , onde X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Hip\u00f3tese Alternativa: \\mu \\neq 0 . Vamos supor que \\sigma \u00e9 conhecido e que \\sigma^2 = 73 , mas a m\u00e9dia \u00e9 desconhecida. Quem \u00e9 \\Omega_0 e \\Omega_1 ? \\Omega_0 \u00e9 a regi\u00e3o dos par\u00e2metros onde a hip\u00f3tese nula \u00e9 verdadeira, isto \u00e9 \\Omega_0 = \\{0\\} , \u00e9 um conjunto unit\u00e1rio. Por outro lado \\Omega_1 = \\mathbb{R} - \\{0\\} , por que a m\u00e9dia pode assumir, em teoria, qualquer valor real. Qual \u00e9 a regi\u00e3o cr\u00edtica? Bom, ainda n\u00e3o podemos determinar essa resposta, afinal para determinar a regi\u00e3o cr\u00edtica (subconjunto do espa\u00e7o dos estados em que se rejeita a hip\u00f3tese nula), precisamos de um procedimento de teste. Mas vamos imaginar que o espa\u00e7o de estados \u00e9 S = \\mathbb{R}^2 , pois vamos considerar apenas duas amostras, inicialmente (queremos visualizar S). Qual ser\u00e1 nosso procedimento de teste? Procedimento de teste \u00e9 uma maneira de tomarmos uma decis\u00e3o. Ele tem a forma: rejeitamos H_0 se isso acontecer. Um exemplo bobo seria: Rejeitamos a hip\u00f3tese nula se 0 \\not \\in (\\min(X_1, X_2), \\max(X_1, X_2)) O problema \u00e9 que em geral esse tipo de procedimento n\u00e3o \u00e9 interessante. Para isso estabelecemos uma estat\u00edstica de teste T e uma Regi\u00e3o de Rejei\u00e7\u00e3o T , tal que nosso procedimento seja: Rejeitamos H_0 se T \\in R . Nesse caso vamos considerar T = \\frac{X_1 + X_2}{2} e vamos rejeitar a hip\u00f3tese se |T| estiver muito longe de 0 , isto \u00e9, se |T| \\geq c . Portanto definimor nossa Regi\u00e3o de Rejei\u00e7\u00e3o como (-\\infty,-c]\\cup[c, + \\infty) , o que reduz nosso problema a determinar c . Qual c seria razo\u00e1vel? 4, 5, 1? Essa pergunta n\u00e3o vai ser respondida. Antes vamos visualizar como fica a regi\u00e3o cr\u00edtica (2). Rejeitamos a hip\u00f3tese se \\left|\\frac{X_1 + X_2}{2}\\right| \\geq c \\rightarrow |X_1 + X_2| \\geq 2c \\rightarrow X_1 + X_2 \\geq 2c \\text{ ou } X_1 + X_2 \\leq -2c Assim: S_1 = \\{(x_1, x_2): x_1 + x_2 \\geq 2c \\text{ ou } x_1 + x_2 \\leq -2c\\} C = [ 1 , 10 , 30 ] decision = lambda x1 , x2 , c : 1 * np . logical_or ( x1 + x2 >= 2 * c , x1 + x2 <= - 2 * c ) x1 , x2 = np . meshgrid ( np . arange ( - 50 , 50 , 0.5 ), np . arange ( - 50 , 50 , 0.5 )) fig , ax = plt . subplots ( 1 , 3 , figsize = ( 21 , 7 )) for i , c in enumerate ( C ): ax [ i ] . contourf ( x1 , x2 , decision ( x1 , x2 , c ), levels = [ 0 , 0.5 , 1 ], colors = [ '#fdcdac' , '#cbd5e8' ]) ax [ i ] . set_xlabel ( r '$x_1$' , fontsize = 20 ) ax [ i ] . set_ylabel ( r '$x_2$' , fontsize = 20 ) ax [ i ] . set_title ( 'Regi\u00e3o Cr\u00edtica quando c = {}' . format ( c ), fontsize = 20 ) ax [ i ] . legend ( handles = [ mpatches . Patch ( color = '#fdcdac' , label = r '$S_0$' ), mpatches . Patch ( color = '#cbd5e8' , label = r '$S_1$' )]) sample = gold_df . Price . diff () . sample ( n = 2 ) X1 , X2 = sample . iloc [ 0 ], sample . iloc [ 1 ] ax [ i ] . scatter ( X1 , X2 , color = 'black' ) ax [ i ] . text ( X1 + 1 , X2 + 1 , s = r '$(X_1, X_2)$' , fontsize = 15 ) Como a fun\u00e7\u00e3o poder entra nessa hist\u00f3ria? A fun\u00e7\u00e3o poder \u00e9 uma fun\u00e7\u00e3o do par\u00e2metro, no caso \\mu , e retorna a probabilidade de rejeitarmos a hip\u00f3tese, considerando esse par\u00e2metro. Isto \u00e9, \\pi(\\mu) = P_{\\mu}((X_1, X_2) \\in S_1) O que poder\u00edamos fazer, ent\u00e3o, \u00e9 obter a distribui\u00e7\u00e3o conjunta de (X_1, X_2) e integrar na regi\u00e3o S_1 . Vamos considerar dois casos separados: \\mu \\in \\Omega_0 : N\u00e3o sabemos disso, e em geral n\u00e3o \u00e9 poss\u00edvel sabermos. Nesse caso \\pi(\\mu) indica a probabilidade de rejeitarmos a hip\u00f3tese nula, mesmo ela sendo verdadeira (chamamos isso de Erro do Tipo I). \\mu \\in \\Omega_1 : Nesse caso \\pi(\\mu) indica a probabilidade de rejeitarmos a hip\u00f3tese nula, quando de ela \u00e9 falsa. Nesse caso 1 - \\pi(\\mu) \u00e9 a probabilidade de n\u00e3o rejeitarmos a hip\u00f3tese nula, quando de fato dever\u00edamos (chamamos de Erro do Tipo II). ] A fun\u00e7\u00e3o poder se trata mais do teste que estamos usando do que os dados em si. Por isso, podemos comparar testes usando essa fun\u00e7\u00e3o poder. Ent\u00e3o vamos ver nesse caso quem \u00e9 a fun\u00e7\u00e3o poder! Vou calcular usando um m\u00e9todo num\u00e9rico para que peguem a ideia. Agora \u00e9 poss\u00edvel fazer as contas, mas nem sempre \u00e9 trivial. Assim teremos apenas aproxima\u00e7\u00e3o da fun\u00e7\u00e3o poder. C = [ 1 , 10 , 30 ] mu_v = np . arange ( - 100 , 100 , 1 ) n = 10000 power = np . zeros (( len ( C ), len ( mu_v ))) for i , c in enumerate ( C ): for j , mu in enumerate ( mu_v ): X1 = np . random . normal ( loc = mu , scale = 73 , size = n ) X2 = np . random . normal ( loc = mu , scale = 73 , size = n ) p = ( sum ( X1 + X2 >= 2 * c ) + sum ( X1 + X2 <= - 2 * c )) / n power [ i , j ] = p plt . plot ( mu_v , power [ i ,:], label = 'c = {}' . format ( c )) plt . title ( 'Fun\u00e7\u00e3o poder' ) plt . xlabel ( r '$\\mu$' ) plt . ylabel ( 'Prob' ) plt . legend () plt . show () Vamos definir c agora? Sim, vamos. Para isso vamos usar o ponto (4) e a defini\u00e7\u00e3o de tamanho do teste. Uma forma poss\u00edvel de se fazer isso \u00e9 a seguinte: limitamos o Erro I por \\alpha_0 e miniminizamos o Erro II, isso \u00e9 minimizamos 1 - \\pi(\\mu) quando \\mu \\neq 0 , ou melhor, maximizamos \\pi(\\mu) . Para isso dizemos que o tamanho do teste \u00e9 o m\u00e1ximo da fun\u00e7\u00e3o poder, quando \\theta \\in \\Omega_0 . Nesse caso \\alpha(\\delta) = \\pi(0) . Queremos, ent\u00e3o que: \\pi(0) = P_{\\mu = 0}(T \\geq c) \\leq \\alpha_0 Precisamos ent\u00e3o encontrar c tal que \\pi(\\mu) = P_{\\mu \\neq 0}(T \\ge c) seja maximado. Observamos que, quando \\mu = 0 , T \\sim N(0, \\sigma^2/2) \\rightarrow \\sqrt{2}T/\\sigma = Z \\sim N(0,1) . Logo P(|T| \\ge c) = P(|Z| \\ge \\sqrt{2}c/\\sigma) = 2(1 - \\Phi(\\sqrt{2}c/\\sigma)) \\leq \\alpha_0 Para maximizar \\pi(\\mu) em \\Omega_1 , observamos que \\pi(\\mu) decresce com c (os gr\u00e1ficos acima representam bem isso). Como queremos maximizar, gostar\u00edamos de tomar c o m\u00ednimo poss\u00edvel, restrito a 2(1 - \\Phi(\\sqrt{2}c/\\sigma)) \\leq \\alpha_0 \\rightarrow 1 - \\alpha_0/2 \\leq \\Phi(\\sqrt{2}c/\\sigma) como vimos acima. Estamos lidando com uma fun\u00e7\u00e3o invers\u00edvel, ent\u00e3o \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) \\leq c O melhor valor de c que respeita essa condi\u00e7\u00e3o e maximiza a rela\u00e7\u00e3o \u00e9, portanto c = \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) from scipy.stats import norm Lembre que \\alpha_0 indica o m\u00e1ximo de Erro I que aceitamos. alpha0 = 0.05 c = np . sqrt ( 73 ) / np . sqrt ( 2 ) * norm . ppf ( 1 - alpha0 / 2 ) print ( c ) 11.841167465893536 \u00c9 bem pr\u00f3ximo do gr\u00e1fico acima mostrado, quando testamos para c = 10 . t = np . arange ( - 20 , 20 , 0.1 ) X = norm ( loc = 0 , scale = np . sqrt ( 73 ) / np . sqrt ( 2 )) plt . plot ( t , X . pdf ( t )) plt . fill_between ( t [( t < - c )], X . pdf ( t [( t < - c )]), color = 'blue' ) plt . fill_between ( t [( t > c )], X . pdf ( t [( t > c )]), color = 'blue' ) plt . title ( 'Distribui\u00e7\u00e3o Normal e Regi\u00e3o de Rejei\u00e7\u00e3o' ) plt . show () Por exemplo vamos tirar duas amostras de nossa distribui\u00e7\u00e3o X1 , X2 = gold_df . Price . diff () . sample ( 2 ) T = np . abs ( X1 + X2 ) / 2 T >= c False Mas como escolher \\alpha_0 agora? Agora entra o conceito mais complexo, o do p-valor. Ele est\u00e1 associado \u00e0 ideia de escolher o menor \\alpha_0 poss\u00edvel, para que rejeitemos a hip\u00f3tese nula. Isso significa o seguinte: Queremos minimizar o Erro do Tipo I e rejeitar a Hip\u00f3tese Nula com os dados que obtivemos. Se o p-valor for muito alto, significa que o Erro do Tipo I \u00e9 grande se rejeitarmos a hip\u00f3tese nula. Voc\u00ea apostaria que podemos rejeitar a hip\u00f3tese nula nesse caso? Agora, se o p-valor for pequeno e rejeitarmos nossa hip\u00f3tese nula, o erro do tipo I vai ser pequeno, ent\u00e3o apostar que a hip\u00f3tese nula deva ser rejeitada \u00e9 mais confort\u00e1vel. Assim n\u00e3o escolhemos \\alpha_0 , s\u00f3 observamos seu menor valor e vemos se faz sentido. Em geral se p-valor < 0.05, as pessoas rejeitam a hip\u00f3tese nula. No nosso caso calcular o p-valor \u00e9 tranquilo. Para calcular o p-valor, precisamos dos dados . Queremos rejeitar a hip\u00f3tese nula, isto \u00e9, queremos que t \\geq c que \\alpha_0 seja o menor poss\u00edvel, onde t \u00e9 o valor observado de T . Vamos diminuindo \\alpha_0 e para cada \\alpha_0 podemos calcular c e verificamos se t \\ge c . Podemos fazer isso at\u00e9 que c = t , assim: t = \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) \\rightarrow \\alpha_0 = 2\\left(1 - \\Phi\\left(\\frac{\\sqrt{2}}{\\sigma}t\\right)\\right) p_value = 2 * ( 1 - norm . cdf ( np . sqrt ( 2 ) / np . sqrt ( 73 ) * T )) print ( p_value ) 0.8477386286989927 Como o p-valor \u00e9 alto, n\u00e3o faz sentido rejeitar a hip\u00f3tese nula. Encerramos a atividade aqui!","title":"Exemplo"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#testes-de-razao-verossimilhanca","text":"S\u00e3o testes baseados na verossimilha\u00e7a do modelo f_n(x|\\theta) . Suponha que queremos testar a hip\u00f3tese: H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Vamos lembrar que a fun\u00e7\u00e3o de verossimilhan\u00e7a tende a ser mais alta pr\u00f3ximo do valor verdadeiro do par\u00e2metro. Com isso em mente, gostar\u00edamos de saber se a verossimilhan\u00e7a \u00e9 maior em \\Omega_0 ou em \\Omega_1 . Para isso, definimos a estat\u00edstica de raz\u00e3o de verossimilhan\u00e7a : \\Lambda(x) = \\frac{\\sup_{\\theta \\in \\Omega_0}f_n(x|\\theta)}{\\sup_{\\theta \\in \\Omega}f_n(x|\\theta)} Observe que o denominador \u00e9 o valor da fun\u00e7\u00e3o de verossimilhan\u00e7a no Estimador de M\u00e1xima Verossimilhan\u00e7a. Se o par\u00e2metro verdadeiro estiver em \\Omega_0 , o n\u00famerador deve ser mais alto em \\Omega_0 , ent\u00e3o a estat\u00edstica se aproxima de 1. Baseado nisso, o teste de raz\u00e3o de verossimilhan\u00e7a \u00e9: Rejeitamos H_0 se \\Lambda(x) \\le k , para algum k .","title":"Testes de Raz\u00e3o Verossimilhan\u00e7a"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#teorema_1","text":"Seja \\Omega \\in \\mathbb{R}^p aberto e suponha que H_0 seja \\theta_{i_1} = \\theta_{01}, ..., \\theta_{i_k} = \\theta_{0k} , onde \\theta = (\\theta_1, ..., \\theta_p) . Assuma que H_0 seja verdadeira e a fun\u00e7\u00e3o de verossimilhan\u00e7a satisfa\u00e7a as condi\u00e7\u00f5es para que o MLE seja assintoticamente normal e assintoticamente eficiente. Ent\u00e3o: -2\\log\\Lambda(x) \\overset{d}{\\to} \\chi^2(k) (converge em distribui\u00e7\u00e3o quando n \\to \\infty ). A demonstra\u00e7\u00e3o pode ser encontrada no StatLect","title":"Teorema:"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#testes-nao-enviesados","text":"Um teste \u00e9 dito n\u00e3o enviesado se \\forall \\theta \\in \\Omega_0 e \\theta ' \\in \\Omega_1, \\pi(\\theta|\\delta) \\le \\pi(\\theta '|\\delta) N\u00e3o \u00e9 muito utilizado dado seu dif\u00edcil c\u00e1lculo num\u00e9rico e n\u00e3o traz resultados quem valem a pena.","title":"Testes n\u00e3o enviesados"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/","text":"Teste de Hip\u00f3teses II Nesse notebook veremos: Teste de Hip\u00f3teses Simples Hip\u00f3tese Alternativa Bilateral Teste T Comparando m\u00e9dias de duas Normais Comparando vari\u00e2ncias de duas Normais Teste de Hip\u00f3tese Simples O objetivo \u00e9 considerar se um vetor de observa\u00e7\u00f5es vem de uma entre duas observa\u00e7\u00f5es. Nesse caso o espa\u00e7o \\Omega \u00e9 formado por dois pontos, e n\u00e3o \u00e9 um espa\u00e7o de par\u00e2metros, mas espa\u00e7o de distribui\u00e7\u00f5es, em particular dessas duas distribui\u00e7\u00f5es. Isto \u00e9, vamos assumir que X = (X_1, ..., X_n) vem de f_0(x) ou f_1(x) . Assim \\Omega = \\{\\theta_0, \\theta_1\\} e \\theta = \\theta_i se os dados tem distribui\u00e7\u00e3o f_i(x), i = 0,1 . Vamos denotar: \\alpha(\\delta) = P(\\text{Rejeitar} H_0|\\theta = \\theta_0) = P(\\text{Erro I}) \\beta(\\delta) = P(\\text{N\u00e3o rejeitar} H_0|\\theta = \\theta_1) = P(\\text{Erro II}) Teorema Seja \\delta^* o procedimento de teste que n\u00e3o rejeita H_0 se af_0(x) > bf_1(x) e rejeita se af_0(x) < bf_1(x) . Ent\u00e3o, para todo outro procedimento de teste \\delta , a\\alpha(\\delta^*) + b\\beta(\\delta^*) \\le a\\alpha(\\delta) + b\\beta(\\delta) Queremos escolher um teste que minimize essa combina\u00e7\u00e3o linear a\\alpha(\\delta) + b\\beta(\\delta) . Claro que seria \u00f3timo ter esse erro zerado, mas sabemos que existe uma esp\u00e9cie de trade off entre esses erros. Esse teorema d\u00e1 o teste necess\u00e1rio para que isso aconten\u00e7a. Corol\u00e1rio Considere as hip\u00f3teses do teorema anterior, a > 0 e b > 0 . Defina estat\u00edstica de teste raz\u00e3o de verossimilhan\u00e7a : \\Lambda(x) = \\begin{cases} \\frac{f_0(x)}{f_1(x)}, \\text{ se } f_0(x) \\le f_1(x) \\\\ 1, \\text{ caso contr\u00e1rio }. \\end{cases} Defina o procedimento de teste \\delta : Rejeita H_0 se \\Lambda(x) > a/b . Ent\u00e3o o valor de af_0(x) + bf_1(x) \u00e9 m\u00ednimo. Lema Nayman-Pearson Suponha que \\delta ' tem a seguinte forma, para algum k > 0 : H_0 n\u00e3o \u00e9 rejeitada se f_1(x) < kf_0(x) e o \u00e9 quando f_1(x) > kf_0(x). Se \\delta \u00e9 outro procedimento de teste tal que \\alpha(\\delta) \\le \\alpha(\\delta ') , ent\u00e3o \\beta(\\delta) \\ge \\beta(\\delta '). Implementa\u00e7\u00e3o Vamos fazer uma simples implementa\u00e7\u00e3o de uso para esse tipo de problema. import numpy as np from scipy.stats import bernoulli , binom from scipy.optimize import brute Nesse caso, vamos fazer uma simples simula\u00e7\u00e3o, onde um par\u00e2metro de uma distribui\u00e7\u00e3o de Bernoulli pode ser p = 0.4 ou p = 0.6 . Vamos gerar essa amostra, mas sem de fato conhecer p verdadeiro. ro = np . random . RandomState ( 1000000 ) #random state p = ro . choice ([ 0.4 , 0.6 ]) Teremos uma amostra de tamanho n . n = 20 X = ro . binomial ( 1 , p , size = n ) Vamos utilizar o Lema Nayman-Pearson. O objetivo \u00e9 testar as seguintes hip\u00f3teses: H_0: p = 0.4 H_1: p = 0.6 Vamos fixar \\alpha_0 = 0.05 o tamanho do teste. Temos que, se y = \\sum_{i=1}^n x_i \\sim Binomial(n,p) , \\frac{f_1(x)}{f_0(x)} = \\frac{0.6^y0.4^{n-y}}{0.4^y0.6^{n-y}} = \\left(\\frac{3}{2}\\right)^y\\left(\\frac{2}{3}\\right)^{n-y} = \\left(\\frac{3}{2}\\right)^{2y - n} Assim: \\begin{split} 0.05 &= P(f_1(x) > kf_0(x)|p = 0.4) = P\\left(\\left(\\frac{3}{2}\\right)^{2y - n} > k\\right) \\\\ &= P\\left(2y - n > \\frac{\\log(k)}{\\log(3/2)}\\right) \\\\ &= P\\left(y > \\frac{\\log(k)}{2\\log(3/2)} + \\frac{n}{2}\\right), y \\sim Binomial(n,0.4) \\end{split} Isto \u00e9, preciso escolher k que satisfa\u00e7a essa rela\u00e7\u00e3o. Vamos calcular k numericamente utilizando um m\u00e9todo de otimiza\u00e7\u00e3o por bruta for\u00e7a (s\u00e3o poucas as op\u00e7\u00f5es). Como n\u00e3o queremos que seja marior do que 0.05, precisamos colocar peso para que n\u00e3o seja. Veja que existem v\u00e1rios valores de k que satisfazem isso. alpha0 = 0.05 Y = binom ( n = n , p = 0.4 ) func = lambda k , n : np . abs ( 0.95 - Y . cdf (( 1 / 2 ) * np . log ( k ) / np . log ( 3 / 2 ) + n / 2 )) + \\ 10 * ( 0.95 > Y . cdf (( 1 / 2 ) * np . log ( k ) / np . log ( 3 / 2 ) + n / 2 )) k = brute ( func , ranges = ( slice ( 1 , 20 , 1 ),), args = ( n ,))[ 0 ] k 6.0 Por esse motivo, vamos tomar k=6 . Pela Lema de Neyman Pearson, esse teste \u00e9 o que minimiza o Erro do Tipo II. Vamos ver se rejeitamos ou n\u00e3o a hip\u00f3tese nula baseado nos dados obtidos. f0 = lambda x : 0.4 ** ( sum ( x )) * 0.6 ** ( len ( x ) - sum ( x )) f1 = lambda x : 0.6 ** ( sum ( x )) * 0.4 ** ( len ( x ) - sum ( x )) if f1 ( X ) > k * f0 ( X ): print ( r 'Rejeitamos H0.' ) else : print ( r 'N\u00e3o rejeitamos H0.' ) N\u00e3o rejeitamos H0. Vamos ver quem \u00e9 p , ent\u00e3o. print ( 'O valor de p \u00e9 .... ' ) print ( p ) O valor de p \u00e9 .... 0.4 Fizemos bem em n\u00e3o rejeitar a hip\u00f3tese nula! Hip\u00f3tese Alternativa Bilateral Seja X = (X_1, ..., X_n) uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o normal com m\u00e9dia \\mu desconhecida e vari\u00e2ncia \\sigma^2 conhecida e queremos testar a hip\u00f3tese H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0 Como \\bar{X}_n \u00e9 um estimador consistente de \\mu , faz sentido rejeitar a hip\u00f3tese nula quando a m\u00e9dia amostral se afasta de \\mu_0 . Para isso, vamos escolher c_1, c_2 de forma que P(\\bar{X}_n \\leq c_1|\\mu = \\mu_0) + P(\\bar{X}_n \\geq c_2|\\mu = \\mu_0) = \\alpha_0 \\Rightarrow P\\left(Z \\leq \\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) + P\\left(Z \\geq \\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = \\alpha_0 \\Rightarrow \\Phi\\left(\\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) + 1 - \\Phi\\left(\\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = \\alpha_0 \\Rightarrow \\Phi\\left(\\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) = \\alpha_1 \\text{ e } \\Phi\\left(\\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = 1 - \\alpha_2, \\text{ com } \\alpha_1 + \\alpha_2 = \\alpha_0 Observa\u00e7\u00e3o: \\bar{X}_n \\sim N(\\mu, \\sigma^2/n) \\Rightarrow Z = \\sqrt{n}\\frac{\\bar{X}_n - \\mu}{\\sigma} \\sim N(0,1) Observa\u00e7\u00e3o 2: No c\u00e1lculo substituimos \\mu por \\mu_0 , porque estamos \"condicionando\" no conhecimento deles serem iguais. Isto \u00e9, queremos que o tamanho do teste seja \\alpha_0 , lembrando que o tamanho do teste \u00e9 o supremo das probabilidades de se rejeitar a hip\u00f3tese nula quando ela \u00e9 verdadeira. Teste t Suponha que (X_1,...,X_n) \u00e9 uma amostra aleat\u00f3ria da distribui\u00e7\u00e3o N(\\mu,\\sigma^2) , com par\u00e2metros desconhecidos e queremos testar a hip\u00f3tese: H_0: \\mu \\le \\mu_0 \\implies \\Omega_0 = \\{(x,y) \\in \\mathbb{R}^2 | x \\le \\mu_0 \\text{ e } y > 0\\} H_1: \\mu > \\mu_0 \\implies \\Omega_1 = \\{(x,y) \\in \\mathbb{R}^2 | x > \\mu_0 \\text{ e } y > 0\\} Sabemos que U = n^{1/2}\\frac{\\bar{X}_n - \\mu_0}{\\sigma '} \u00e9 uma boa estat\u00edstica de teste e rejeitamos H_0 se U \\ge c . Essa estat\u00edstica \u00e9 interessante porque quando \\mu = \\mu_0, U \\sim t(n-1) . Por isso chamamos de testes t quando baseados na estat\u00edstica U . Podemos tamb\u00e9m inverter os sinais de desigualdade e rejeitar H_0 quando U \\le c . from pandas import DataFrame from scipy.stats import t import matplotlib.pyplot as plt import seaborn as sns sns . set () % matplotlib notebook mu0 = 10 # Vamos escolher mu e sigma de forma aleat\u00f3ria, mas n\u00e3o significa que \u00e9 uma vari\u00e1vel aleat\u00f3ria. n = 20 Distibui\u00e7\u00e3o de U Vamos gerar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o de U para um determinado \\mu . U_values = {} for i in range ( 6 ): mu = ro . normal ( mu0 , 1 ) if i < 5 else 10 sigma = ro . exponential ( mu0 ) key = 'mu = {}, sigma = {}' . format ( np . round ( mu , 2 ), np . round ( sigma , 2 )) U_values [ key ] = np . zeros ( 10000 ) for j in range ( 10000 ): X = ro . normal ( mu , sigma , size = n ) U = np . sqrt ( n ) * ( np . mean ( X ) - mu0 ) / np . std ( X , ddof = 1 ) U_values [ key ][ j ] = U U_values = DataFrame ( U_values ) fig , ax = plt . subplots ( figsize = ( 10 , 6 )) sns . kdeplot ( data = U_values , ax = ax ) ax . set_title ( 'Distribui\u00e7\u00e3o aproximada de U' ) plt . show () Teorema Seja c o 1 - \\alpha_0 quartil da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Ent\u00e3o, segundo o teste citado acima, a fun\u00e7\u00e3o poder tem as seguintes propriedades: \\pi(\\mu, \\sigma^2|\\delta) = \\alpha_0 , quando \\mu = \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) < \\alpha_0 , quando \\mu < \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) > \\alpha_0 , quando \\mu > \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) \\to 0 , quando \\mu \\to -\\infty . \\pi(\\mu, \\sigma^2|\\delta) \\to 1 , quando \\mu \\to \\infty . O teste tamb\u00e9m \u00e9 n\u00e3o enviesado como consequ\u00eancia. P-valores para testes t Seja u a estat\u00edstica U quando observada. Seja T_{n-1}(\\cdot) a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Ent\u00e3o o p-valor para H_0: \\mu \\leq \\mu_0 \u00e9 1 - T_{n-1}(u) , enquanto o p-valor para H_0: \\mu \\ge \\mu_0 \u00e9 T_{n-1}(u) . Distribui\u00e7\u00e3o t n\u00e3o central O objetivo \u00e9 encontrar a distribui\u00e7\u00e3o de U mesmo quando \\mu \\neq \\mu_0 . Seja W e Y vari\u00e1veis alet\u00f3rias independentes com distribui\u00e7\u00e3o N(\\psi, 1) e \\chi^2(m) , respectivamente. Ent\u00e3o X = \\frac{W}{\\left(\\frac{Y}{m}\\right)^{1/2}} tem distribui\u00e7\u00e3o t n\u00e3o central com m graus de liberdade e n\u00e3o centralidade \\psi . Denotaremos T_m(x|\\psi) a cdf dessa distribui\u00e7\u00e3o. Teorema (Fun\u00e7\u00e3o Poder) Seja X_1, ..., X_n amostra aleat\u00f3ria de N(\\mu,\\sigma^2) . A distribui\u00e7\u00e3o de U \u00e9 t n\u00e3o central com n-1 graus de liberdade e par\u00e2metro de n\u00e3o centralidade \\psi = n^{1/2}(\\mu - \\mu_0)/\\sigma ( Observe que isso ocorre porque dividimos o numerador e o denominador por \\sigma . Al\u00e9m disso, note que X n\u00e3o \u00e9 uma quantidade pivotal, dado que sua distribui\u00e7\u00e3o depende de par\u00e2metros desconhecidos ) Suponha que o procedimento \\delta rejeita H_0: \\mu \\le \\mu_0 se U \\ge c . Ent\u00e3o a fun\u00e7\u00e3o poder \u00e9 \\pi(\\mu,\\sigma^2|\\delta) = 1 - T_{n-1}(c,\\psi) Se \\delta ' rejeita H_0: \\mu \\ge \\mu_0 se U \\le c . Ent\u00e3o a fun\u00e7\u00e3o poder \u00e9 \\pi(\\mu,\\sigma^2|\\delta) = T_{n-1}(c,\\psi) from scipy.stats import nct #noncentral t dsitribution from matplotlib import animation from IPython.display import HTML import warnings warnings . filterwarnings ( 'ignore' ) n = 10 mu0 = 5 sigma = 2 psi = lambda mu : np . sqrt ( n ) * ( mu - mu0 ) / sigma X = nct ( df = n - 1 , nc = psi ( - 20 )) Vamos ver o que acontece quando variamos \\mu . Nesse caso -20 \\leq \\mu \\geq 20 . fig , ax = plt . subplots () x = np . linspace ( X . ppf ( 0.01 ), X . ppf ( 0.99 ), 100 ) line , = ax . plot ( x , X . pdf ( x ), 'r-' , lw = 5 , alpha = 0.6 ) ax . set_xlim (( - 60 , 60 )) ax . set_ylim (( 0 , 0.3 )) ax . set_title ( 't n\u00e3o central' ) def animate ( i , n ): x = np . linspace ( - 60 , 60 , 100 ) line . set_data ( x , nct . pdf ( x , df = n - 1 , nc = psi ( i - 20 ))) return line , HTML ( animation . FuncAnimation ( fig , animate , frames = 40 , interval = 100 , fargs = ( n ,), repeat = False ) . to_html5_video ()) Your browser does not support the video tag. Alternativa Bilateral Tome agora a hip\u00f3tese H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0 Podemos usar a mesma estat\u00edstica U , mas agora que temos dois lados, vamos fazer o seguinte processo (vou construir de forma intuitiva, no livro tem uma formaliza\u00e7\u00e3o): O procedimento de teste \u00e9 do tipo: Rejeitamos H_0 se U \\le c_1 ou U \\ge c_2 . Vamos considerar c_1 = -c e c_2 = c , para simplificar. Seja \\alpha_0 o tamanho do teste, isto \u00e9, a probabilidade de rejeitarmos a hip\u00f3tese nula quando \\mu = \\mu_0 . Quando \\mu = \\mu_0 , U tem distribui\u00e7\u00e3o t com n-1 graus de liberdade. Assim: P(|U| \\ge c|\\mu = \\mu_0) = \\alpha_0 = P(U \\le -c) + P(U \\ge c) \\overset{simetria}{=} 2P( U \\ge c) = 2(1 - P(U \\le c)) n = 20 alpha0 = 0.05 c = t . ppf ( df = n - 1 , q = 1 - alpha0 / 2 ) X = t ( df = n - 1 ) x = np . arange ( - 5 , 5 , 0.1 ) plt . plot ( x , X . pdf ( x )) plt . fill_between ( x [( x < - c )], X . pdf ( x [( x < - c )]), color = 'blue' ) plt . fill_between ( x [( x > c )], X . pdf ( x [( x > c )]), color = 'blue' ) plt . title ( 'Distribui\u00e7\u00e3o de U e Regi\u00e3o de Rejei\u00e7\u00e3o' ) plt . show () Fun\u00e7\u00e3o Poder \\pi(\\mu,\\sigma^2,|\\delta) = T_{n-1}(-x|\\psi) + 1 - T_{n-1}(c|\\psi) P-valor Seja u o valor observado da vari\u00e1vel U . Vamos lembrar que o p-valor \u00e9 o menor tamanho \\alpha_0 tal que se rejeita a hip\u00f3tese com esse valor observado. Como s\u00f3 rejeitamos se: |u| \\ge c = T_{n-1}^{-1}(1 - \\alpha_0/2) \\implies \\alpha_0 \\ge 2 - 2T_{n-1}(|u|) Logo o p-valor \u00e9 2 - 2T_{n-1}(|u|) . Comparando m\u00e9dias de duas normais Assumimos que X = (X_1,...,X_m) \u00e9 uma amostra da distribui\u00e7\u00e3o normal com m\u00e9dia \\mu_1 e vari\u00e2ncia \\sigma^2 , enquanto Y = (Y_1, ..., Y_n) \u00e9 normal com m\u00e9dia \\mu_2 e vari\u00e2ncia \\sigma^2 . Estamos interessados no teste H_0: \\mu_1 \\le \\mu_2 H_1: \\mu_1 > \\mu_2 A fun\u00e7\u00e3o poder \u00e9 dada por \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) . A discuss\u00e3o quando as normais tem diferentes normais ser\u00e1 postergada. Defina S_x = \\sum_{i=1}^m (X_i - \\bar{X}_m)^2 S_y = \\sum_{i=1}^n (Y_i - \\bar{Y}_n)^2 U = \\frac{(m + n - 2)^{1/2}(\\bar{X}_m - \\bar{Y}_n)}{\\left(\\frac{1}{n} + \\frac{1}{m}\\right)^{1/2}(S_x^2 + S_y^2)^{1/2}} A distribui\u00e7\u00e3o: U \\sim t com m + n - 2 graus de liberdade, com par\u00e2metro de n\u00e3o centralidade \\psi= \\frac{\\mu_1 - \\mu_2}{\\sigma\\left(\\frac{1}{m} + \\frac{1}{n}\\right)^{1/2}} Note que se \\mu_1 = \\mu_2 , U \u00e9 uma distribui\u00e7\u00e3o t padr\u00e3o. Fun\u00e7\u00e3o Poder Considere o procedimento de teste \\delta que rejeite H_0 se U \\ge T_{m+n-2}^{-1}(1 - \\alpha_0) . Assim: \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) = \\alpha_0 , quando \\mu_1 = \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) < \\alpha_0 , quando \\mu_1 < \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) > \\alpha_0 , quando \\mu_1 > \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) \\to 0 , quando \\mu_1 - \\mu_2 \\to -\\infty . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) \\to 1 , quando \\mu_1 - \\mu_2 \\to \\infty . Al\u00e9m do mais o teste \u00e9 n\u00e3o enviesado. P-valor Depois de termos observado as amostras, seja u a estat\u00edstica observada de U . O p-valor da hip\u00f3tese \u00e9 1 - T_{m+n-2}(u) . Equivalentemente com o teste t do item 3, podemos expressar tudo com a hip\u00f3tese bilateral e s\u00f3 altera o graude liberade quando comparado com o teste t anterior. Vari\u00e2ncias diferentes Raz\u00e3o entre as vari\u00e2ncias \u00e9 conhecida Suponha que se as vari\u00e2ncias de X e Y s\u00e3o \\sigma_1^2 e \\sigma_2^2 e que \\sigma^2_2 = k\\sigma^2_1, k > 0 . Ent\u00e3o podemos usar a estat\u00edstica U = \\frac{(m + n - 2)^{1/2}(\\bar{X}_m - \\bar{Y}_n)}{\\left(\\frac{1}{n} + \\frac{k}{m}\\right)^{1/2}(S_x^2 + \\frac{S_y^2}{k})^{1/2}} Problema de Behrens-Fisher Quando os 4 par\u00e2metros das normais s\u00e3o desconhecidos, t\u00e3o pouco a raz\u00e3o de vari\u00e2ncias, nem a estat\u00edstica de raz\u00e3o de verossimilhan\u00e7a tem distribui\u00e7\u00e3o conhecida. Algumas tentativas j\u00e1 foram feitas, como Welch e outros . Comparando vari\u00e2ncias de duas Normais Assumimos que X = (X_1,...,X_m) \u00e9 uma amostra da distribui\u00e7\u00e3o normal com m\u00e9dia \\mu_1 e vari\u00e2ncia \\sigma^2 , enquanto Y = (Y_1, ..., Y_n) \u00e9 normal com m\u00e9dia \\mu_2 e vari\u00e2ncia \\sigma^2 . Estamos interessados no teste H_0: \\sigma_1^2 \\le \\sigma_2^2 H_1: \\sigma_1^2 > \\sigma_2^2 A fun\u00e7\u00e3o poder \u00e9 dada por \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) . COnsidere S_x^2 e S_y^2 definidos anteriormente. Ent\u00e3o temos que S_x^2/(m-1) \u00e9 estimador para \\sigma_1^2 , enquanto S_y^2/(n-1) \u00e9 estimador para \\sigma_2^2 . Defina V = \\frac{S_x^2/(m-1)}{S_y^2/(n-1)} Rejeitaremos X_0 se V \\ge c , onde c ser\u00e1 escolhido para que esse teste tenha n\u00edvel de signific\u00e2ncia \\alpha_0 . Esse teste \u00e9 chamado de teste F, pois a distribui\u00e7\u00e3o de (\\sigma_1^2/\\sigma_2^2)V \u00e9 F com par\u00e2metros m-1 e n-1 . Em particular se \\sigma_1^2 = \\sigma_2^2 , V tem distribui\u00e7\u00e3o F. Onde a distribui\u00e7\u00e3o F \u00e9 descrita aqui . Fun\u00e7\u00e3o Poder Considere o procedimento de teste \\delta que rejeite H_0 se V \\ge F_{m-1,n-1}^{-1}(1 - \\alpha_0) . Assim: \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) = 1 - F_{m-1,n-1}(\\frac{\\sigma_2^2}{\\sigma_1^2}c) \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) = \\alpha_0 , quando \\sigma_1^2 = \\sigma^2_2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) < \\alpha_0 , quando \\sigma_1^2 < \\sigma_2^2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) > \\alpha_0 , quando \\sigma^2_1 > \\sigma_2^2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) \\to 0 , quando \\sigma_1^2/\\sigma_2^2 \\to 0 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) \\to 1 , quando \\sigma_1^2/\\sigma_2^2 \\to \\infty . Al\u00e9m do mais o teste \u00e9 n\u00e3o enviesado. P-valor Depois de termos observado as amostras, seja v a estat\u00edstica observada de V . O p-valor da hip\u00f3tese \u00e9 1 - F_{m-1,n-1}(v) .","title":"Teste de Hip\u00f3teses II"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#teste-de-hipoteses-ii","text":"Nesse notebook veremos: Teste de Hip\u00f3teses Simples Hip\u00f3tese Alternativa Bilateral Teste T Comparando m\u00e9dias de duas Normais Comparando vari\u00e2ncias de duas Normais","title":"Teste de Hip\u00f3teses II"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#teste-de-hipotese-simples","text":"O objetivo \u00e9 considerar se um vetor de observa\u00e7\u00f5es vem de uma entre duas observa\u00e7\u00f5es. Nesse caso o espa\u00e7o \\Omega \u00e9 formado por dois pontos, e n\u00e3o \u00e9 um espa\u00e7o de par\u00e2metros, mas espa\u00e7o de distribui\u00e7\u00f5es, em particular dessas duas distribui\u00e7\u00f5es. Isto \u00e9, vamos assumir que X = (X_1, ..., X_n) vem de f_0(x) ou f_1(x) . Assim \\Omega = \\{\\theta_0, \\theta_1\\} e \\theta = \\theta_i se os dados tem distribui\u00e7\u00e3o f_i(x), i = 0,1 . Vamos denotar: \\alpha(\\delta) = P(\\text{Rejeitar} H_0|\\theta = \\theta_0) = P(\\text{Erro I}) \\beta(\\delta) = P(\\text{N\u00e3o rejeitar} H_0|\\theta = \\theta_1) = P(\\text{Erro II})","title":"Teste de Hip\u00f3tese Simples"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#teorema","text":"Seja \\delta^* o procedimento de teste que n\u00e3o rejeita H_0 se af_0(x) > bf_1(x) e rejeita se af_0(x) < bf_1(x) . Ent\u00e3o, para todo outro procedimento de teste \\delta , a\\alpha(\\delta^*) + b\\beta(\\delta^*) \\le a\\alpha(\\delta) + b\\beta(\\delta) Queremos escolher um teste que minimize essa combina\u00e7\u00e3o linear a\\alpha(\\delta) + b\\beta(\\delta) . Claro que seria \u00f3timo ter esse erro zerado, mas sabemos que existe uma esp\u00e9cie de trade off entre esses erros. Esse teorema d\u00e1 o teste necess\u00e1rio para que isso aconten\u00e7a.","title":"Teorema"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#corolario","text":"Considere as hip\u00f3teses do teorema anterior, a > 0 e b > 0 . Defina estat\u00edstica de teste raz\u00e3o de verossimilhan\u00e7a : \\Lambda(x) = \\begin{cases} \\frac{f_0(x)}{f_1(x)}, \\text{ se } f_0(x) \\le f_1(x) \\\\ 1, \\text{ caso contr\u00e1rio }. \\end{cases} Defina o procedimento de teste \\delta : Rejeita H_0 se \\Lambda(x) > a/b . Ent\u00e3o o valor de af_0(x) + bf_1(x) \u00e9 m\u00ednimo.","title":"Corol\u00e1rio"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#lema-nayman-pearson","text":"Suponha que \\delta ' tem a seguinte forma, para algum k > 0 : H_0 n\u00e3o \u00e9 rejeitada se f_1(x) < kf_0(x) e o \u00e9 quando f_1(x) > kf_0(x). Se \\delta \u00e9 outro procedimento de teste tal que \\alpha(\\delta) \\le \\alpha(\\delta ') , ent\u00e3o \\beta(\\delta) \\ge \\beta(\\delta ').","title":"Lema Nayman-Pearson"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#implementacao","text":"Vamos fazer uma simples implementa\u00e7\u00e3o de uso para esse tipo de problema. import numpy as np from scipy.stats import bernoulli , binom from scipy.optimize import brute Nesse caso, vamos fazer uma simples simula\u00e7\u00e3o, onde um par\u00e2metro de uma distribui\u00e7\u00e3o de Bernoulli pode ser p = 0.4 ou p = 0.6 . Vamos gerar essa amostra, mas sem de fato conhecer p verdadeiro. ro = np . random . RandomState ( 1000000 ) #random state p = ro . choice ([ 0.4 , 0.6 ]) Teremos uma amostra de tamanho n . n = 20 X = ro . binomial ( 1 , p , size = n ) Vamos utilizar o Lema Nayman-Pearson. O objetivo \u00e9 testar as seguintes hip\u00f3teses: H_0: p = 0.4 H_1: p = 0.6 Vamos fixar \\alpha_0 = 0.05 o tamanho do teste. Temos que, se y = \\sum_{i=1}^n x_i \\sim Binomial(n,p) , \\frac{f_1(x)}{f_0(x)} = \\frac{0.6^y0.4^{n-y}}{0.4^y0.6^{n-y}} = \\left(\\frac{3}{2}\\right)^y\\left(\\frac{2}{3}\\right)^{n-y} = \\left(\\frac{3}{2}\\right)^{2y - n} Assim: \\begin{split} 0.05 &= P(f_1(x) > kf_0(x)|p = 0.4) = P\\left(\\left(\\frac{3}{2}\\right)^{2y - n} > k\\right) \\\\ &= P\\left(2y - n > \\frac{\\log(k)}{\\log(3/2)}\\right) \\\\ &= P\\left(y > \\frac{\\log(k)}{2\\log(3/2)} + \\frac{n}{2}\\right), y \\sim Binomial(n,0.4) \\end{split} Isto \u00e9, preciso escolher k que satisfa\u00e7a essa rela\u00e7\u00e3o. Vamos calcular k numericamente utilizando um m\u00e9todo de otimiza\u00e7\u00e3o por bruta for\u00e7a (s\u00e3o poucas as op\u00e7\u00f5es). Como n\u00e3o queremos que seja marior do que 0.05, precisamos colocar peso para que n\u00e3o seja. Veja que existem v\u00e1rios valores de k que satisfazem isso. alpha0 = 0.05 Y = binom ( n = n , p = 0.4 ) func = lambda k , n : np . abs ( 0.95 - Y . cdf (( 1 / 2 ) * np . log ( k ) / np . log ( 3 / 2 ) + n / 2 )) + \\ 10 * ( 0.95 > Y . cdf (( 1 / 2 ) * np . log ( k ) / np . log ( 3 / 2 ) + n / 2 )) k = brute ( func , ranges = ( slice ( 1 , 20 , 1 ),), args = ( n ,))[ 0 ] k 6.0 Por esse motivo, vamos tomar k=6 . Pela Lema de Neyman Pearson, esse teste \u00e9 o que minimiza o Erro do Tipo II. Vamos ver se rejeitamos ou n\u00e3o a hip\u00f3tese nula baseado nos dados obtidos. f0 = lambda x : 0.4 ** ( sum ( x )) * 0.6 ** ( len ( x ) - sum ( x )) f1 = lambda x : 0.6 ** ( sum ( x )) * 0.4 ** ( len ( x ) - sum ( x )) if f1 ( X ) > k * f0 ( X ): print ( r 'Rejeitamos H0.' ) else : print ( r 'N\u00e3o rejeitamos H0.' ) N\u00e3o rejeitamos H0. Vamos ver quem \u00e9 p , ent\u00e3o. print ( 'O valor de p \u00e9 .... ' ) print ( p ) O valor de p \u00e9 .... 0.4 Fizemos bem em n\u00e3o rejeitar a hip\u00f3tese nula!","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#hipotese-alternativa-bilateral","text":"Seja X = (X_1, ..., X_n) uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o normal com m\u00e9dia \\mu desconhecida e vari\u00e2ncia \\sigma^2 conhecida e queremos testar a hip\u00f3tese H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0 Como \\bar{X}_n \u00e9 um estimador consistente de \\mu , faz sentido rejeitar a hip\u00f3tese nula quando a m\u00e9dia amostral se afasta de \\mu_0 . Para isso, vamos escolher c_1, c_2 de forma que P(\\bar{X}_n \\leq c_1|\\mu = \\mu_0) + P(\\bar{X}_n \\geq c_2|\\mu = \\mu_0) = \\alpha_0 \\Rightarrow P\\left(Z \\leq \\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) + P\\left(Z \\geq \\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = \\alpha_0 \\Rightarrow \\Phi\\left(\\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) + 1 - \\Phi\\left(\\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = \\alpha_0 \\Rightarrow \\Phi\\left(\\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) = \\alpha_1 \\text{ e } \\Phi\\left(\\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = 1 - \\alpha_2, \\text{ com } \\alpha_1 + \\alpha_2 = \\alpha_0 Observa\u00e7\u00e3o: \\bar{X}_n \\sim N(\\mu, \\sigma^2/n) \\Rightarrow Z = \\sqrt{n}\\frac{\\bar{X}_n - \\mu}{\\sigma} \\sim N(0,1) Observa\u00e7\u00e3o 2: No c\u00e1lculo substituimos \\mu por \\mu_0 , porque estamos \"condicionando\" no conhecimento deles serem iguais. Isto \u00e9, queremos que o tamanho do teste seja \\alpha_0 , lembrando que o tamanho do teste \u00e9 o supremo das probabilidades de se rejeitar a hip\u00f3tese nula quando ela \u00e9 verdadeira.","title":"Hip\u00f3tese Alternativa Bilateral"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#teste-t","text":"Suponha que (X_1,...,X_n) \u00e9 uma amostra aleat\u00f3ria da distribui\u00e7\u00e3o N(\\mu,\\sigma^2) , com par\u00e2metros desconhecidos e queremos testar a hip\u00f3tese: H_0: \\mu \\le \\mu_0 \\implies \\Omega_0 = \\{(x,y) \\in \\mathbb{R}^2 | x \\le \\mu_0 \\text{ e } y > 0\\} H_1: \\mu > \\mu_0 \\implies \\Omega_1 = \\{(x,y) \\in \\mathbb{R}^2 | x > \\mu_0 \\text{ e } y > 0\\} Sabemos que U = n^{1/2}\\frac{\\bar{X}_n - \\mu_0}{\\sigma '} \u00e9 uma boa estat\u00edstica de teste e rejeitamos H_0 se U \\ge c . Essa estat\u00edstica \u00e9 interessante porque quando \\mu = \\mu_0, U \\sim t(n-1) . Por isso chamamos de testes t quando baseados na estat\u00edstica U . Podemos tamb\u00e9m inverter os sinais de desigualdade e rejeitar H_0 quando U \\le c . from pandas import DataFrame from scipy.stats import t import matplotlib.pyplot as plt import seaborn as sns sns . set () % matplotlib notebook mu0 = 10 # Vamos escolher mu e sigma de forma aleat\u00f3ria, mas n\u00e3o significa que \u00e9 uma vari\u00e1vel aleat\u00f3ria. n = 20","title":"Teste t"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#distibuicao-de-u","text":"Vamos gerar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o de U para um determinado \\mu . U_values = {} for i in range ( 6 ): mu = ro . normal ( mu0 , 1 ) if i < 5 else 10 sigma = ro . exponential ( mu0 ) key = 'mu = {}, sigma = {}' . format ( np . round ( mu , 2 ), np . round ( sigma , 2 )) U_values [ key ] = np . zeros ( 10000 ) for j in range ( 10000 ): X = ro . normal ( mu , sigma , size = n ) U = np . sqrt ( n ) * ( np . mean ( X ) - mu0 ) / np . std ( X , ddof = 1 ) U_values [ key ][ j ] = U U_values = DataFrame ( U_values ) fig , ax = plt . subplots ( figsize = ( 10 , 6 )) sns . kdeplot ( data = U_values , ax = ax ) ax . set_title ( 'Distribui\u00e7\u00e3o aproximada de U' ) plt . show ()","title":"Distibui\u00e7\u00e3o de U"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#teorema_1","text":"Seja c o 1 - \\alpha_0 quartil da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Ent\u00e3o, segundo o teste citado acima, a fun\u00e7\u00e3o poder tem as seguintes propriedades: \\pi(\\mu, \\sigma^2|\\delta) = \\alpha_0 , quando \\mu = \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) < \\alpha_0 , quando \\mu < \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) > \\alpha_0 , quando \\mu > \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) \\to 0 , quando \\mu \\to -\\infty . \\pi(\\mu, \\sigma^2|\\delta) \\to 1 , quando \\mu \\to \\infty . O teste tamb\u00e9m \u00e9 n\u00e3o enviesado como consequ\u00eancia.","title":"Teorema"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#p-valores-para-testes-t","text":"Seja u a estat\u00edstica U quando observada. Seja T_{n-1}(\\cdot) a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Ent\u00e3o o p-valor para H_0: \\mu \\leq \\mu_0 \u00e9 1 - T_{n-1}(u) , enquanto o p-valor para H_0: \\mu \\ge \\mu_0 \u00e9 T_{n-1}(u) .","title":"P-valores para testes t"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#distribuicao-t-nao-central","text":"O objetivo \u00e9 encontrar a distribui\u00e7\u00e3o de U mesmo quando \\mu \\neq \\mu_0 . Seja W e Y vari\u00e1veis alet\u00f3rias independentes com distribui\u00e7\u00e3o N(\\psi, 1) e \\chi^2(m) , respectivamente. Ent\u00e3o X = \\frac{W}{\\left(\\frac{Y}{m}\\right)^{1/2}} tem distribui\u00e7\u00e3o t n\u00e3o central com m graus de liberdade e n\u00e3o centralidade \\psi . Denotaremos T_m(x|\\psi) a cdf dessa distribui\u00e7\u00e3o.","title":"Distribui\u00e7\u00e3o t n\u00e3o central"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#teorema-funcao-poder","text":"Seja X_1, ..., X_n amostra aleat\u00f3ria de N(\\mu,\\sigma^2) . A distribui\u00e7\u00e3o de U \u00e9 t n\u00e3o central com n-1 graus de liberdade e par\u00e2metro de n\u00e3o centralidade \\psi = n^{1/2}(\\mu - \\mu_0)/\\sigma ( Observe que isso ocorre porque dividimos o numerador e o denominador por \\sigma . Al\u00e9m disso, note que X n\u00e3o \u00e9 uma quantidade pivotal, dado que sua distribui\u00e7\u00e3o depende de par\u00e2metros desconhecidos ) Suponha que o procedimento \\delta rejeita H_0: \\mu \\le \\mu_0 se U \\ge c . Ent\u00e3o a fun\u00e7\u00e3o poder \u00e9 \\pi(\\mu,\\sigma^2|\\delta) = 1 - T_{n-1}(c,\\psi) Se \\delta ' rejeita H_0: \\mu \\ge \\mu_0 se U \\le c . Ent\u00e3o a fun\u00e7\u00e3o poder \u00e9 \\pi(\\mu,\\sigma^2|\\delta) = T_{n-1}(c,\\psi) from scipy.stats import nct #noncentral t dsitribution from matplotlib import animation from IPython.display import HTML import warnings warnings . filterwarnings ( 'ignore' ) n = 10 mu0 = 5 sigma = 2 psi = lambda mu : np . sqrt ( n ) * ( mu - mu0 ) / sigma X = nct ( df = n - 1 , nc = psi ( - 20 )) Vamos ver o que acontece quando variamos \\mu . Nesse caso -20 \\leq \\mu \\geq 20 . fig , ax = plt . subplots () x = np . linspace ( X . ppf ( 0.01 ), X . ppf ( 0.99 ), 100 ) line , = ax . plot ( x , X . pdf ( x ), 'r-' , lw = 5 , alpha = 0.6 ) ax . set_xlim (( - 60 , 60 )) ax . set_ylim (( 0 , 0.3 )) ax . set_title ( 't n\u00e3o central' ) def animate ( i , n ): x = np . linspace ( - 60 , 60 , 100 ) line . set_data ( x , nct . pdf ( x , df = n - 1 , nc = psi ( i - 20 ))) return line , HTML ( animation . FuncAnimation ( fig , animate , frames = 40 , interval = 100 , fargs = ( n ,), repeat = False ) . to_html5_video ()) Your browser does not support the video tag.","title":"Teorema  (Fun\u00e7\u00e3o Poder)"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#alternativa-bilateral","text":"Tome agora a hip\u00f3tese H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0 Podemos usar a mesma estat\u00edstica U , mas agora que temos dois lados, vamos fazer o seguinte processo (vou construir de forma intuitiva, no livro tem uma formaliza\u00e7\u00e3o): O procedimento de teste \u00e9 do tipo: Rejeitamos H_0 se U \\le c_1 ou U \\ge c_2 . Vamos considerar c_1 = -c e c_2 = c , para simplificar. Seja \\alpha_0 o tamanho do teste, isto \u00e9, a probabilidade de rejeitarmos a hip\u00f3tese nula quando \\mu = \\mu_0 . Quando \\mu = \\mu_0 , U tem distribui\u00e7\u00e3o t com n-1 graus de liberdade. Assim: P(|U| \\ge c|\\mu = \\mu_0) = \\alpha_0 = P(U \\le -c) + P(U \\ge c) \\overset{simetria}{=} 2P( U \\ge c) = 2(1 - P(U \\le c)) n = 20 alpha0 = 0.05 c = t . ppf ( df = n - 1 , q = 1 - alpha0 / 2 ) X = t ( df = n - 1 ) x = np . arange ( - 5 , 5 , 0.1 ) plt . plot ( x , X . pdf ( x )) plt . fill_between ( x [( x < - c )], X . pdf ( x [( x < - c )]), color = 'blue' ) plt . fill_between ( x [( x > c )], X . pdf ( x [( x > c )]), color = 'blue' ) plt . title ( 'Distribui\u00e7\u00e3o de U e Regi\u00e3o de Rejei\u00e7\u00e3o' ) plt . show ()","title":"Alternativa Bilateral"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#funcao-poder","text":"\\pi(\\mu,\\sigma^2,|\\delta) = T_{n-1}(-x|\\psi) + 1 - T_{n-1}(c|\\psi)","title":"Fun\u00e7\u00e3o Poder"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#p-valor","text":"Seja u o valor observado da vari\u00e1vel U . Vamos lembrar que o p-valor \u00e9 o menor tamanho \\alpha_0 tal que se rejeita a hip\u00f3tese com esse valor observado. Como s\u00f3 rejeitamos se: |u| \\ge c = T_{n-1}^{-1}(1 - \\alpha_0/2) \\implies \\alpha_0 \\ge 2 - 2T_{n-1}(|u|) Logo o p-valor \u00e9 2 - 2T_{n-1}(|u|) .","title":"P-valor"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#comparando-medias-de-duas-normais","text":"Assumimos que X = (X_1,...,X_m) \u00e9 uma amostra da distribui\u00e7\u00e3o normal com m\u00e9dia \\mu_1 e vari\u00e2ncia \\sigma^2 , enquanto Y = (Y_1, ..., Y_n) \u00e9 normal com m\u00e9dia \\mu_2 e vari\u00e2ncia \\sigma^2 . Estamos interessados no teste H_0: \\mu_1 \\le \\mu_2 H_1: \\mu_1 > \\mu_2 A fun\u00e7\u00e3o poder \u00e9 dada por \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) . A discuss\u00e3o quando as normais tem diferentes normais ser\u00e1 postergada. Defina S_x = \\sum_{i=1}^m (X_i - \\bar{X}_m)^2 S_y = \\sum_{i=1}^n (Y_i - \\bar{Y}_n)^2 U = \\frac{(m + n - 2)^{1/2}(\\bar{X}_m - \\bar{Y}_n)}{\\left(\\frac{1}{n} + \\frac{1}{m}\\right)^{1/2}(S_x^2 + S_y^2)^{1/2}} A distribui\u00e7\u00e3o: U \\sim t com m + n - 2 graus de liberdade, com par\u00e2metro de n\u00e3o centralidade \\psi= \\frac{\\mu_1 - \\mu_2}{\\sigma\\left(\\frac{1}{m} + \\frac{1}{n}\\right)^{1/2}} Note que se \\mu_1 = \\mu_2 , U \u00e9 uma distribui\u00e7\u00e3o t padr\u00e3o.","title":"Comparando m\u00e9dias de duas normais"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#funcao-poder_1","text":"Considere o procedimento de teste \\delta que rejeite H_0 se U \\ge T_{m+n-2}^{-1}(1 - \\alpha_0) . Assim: \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) = \\alpha_0 , quando \\mu_1 = \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) < \\alpha_0 , quando \\mu_1 < \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) > \\alpha_0 , quando \\mu_1 > \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) \\to 0 , quando \\mu_1 - \\mu_2 \\to -\\infty . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) \\to 1 , quando \\mu_1 - \\mu_2 \\to \\infty . Al\u00e9m do mais o teste \u00e9 n\u00e3o enviesado.","title":"Fun\u00e7\u00e3o Poder"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#p-valor_1","text":"Depois de termos observado as amostras, seja u a estat\u00edstica observada de U . O p-valor da hip\u00f3tese \u00e9 1 - T_{m+n-2}(u) . Equivalentemente com o teste t do item 3, podemos expressar tudo com a hip\u00f3tese bilateral e s\u00f3 altera o graude liberade quando comparado com o teste t anterior.","title":"P-valor"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#variancias-diferentes","text":"","title":"Vari\u00e2ncias diferentes"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#razao-entre-as-variancias-e-conhecida","text":"Suponha que se as vari\u00e2ncias de X e Y s\u00e3o \\sigma_1^2 e \\sigma_2^2 e que \\sigma^2_2 = k\\sigma^2_1, k > 0 . Ent\u00e3o podemos usar a estat\u00edstica U = \\frac{(m + n - 2)^{1/2}(\\bar{X}_m - \\bar{Y}_n)}{\\left(\\frac{1}{n} + \\frac{k}{m}\\right)^{1/2}(S_x^2 + \\frac{S_y^2}{k})^{1/2}}","title":"Raz\u00e3o entre as vari\u00e2ncias \u00e9 conhecida"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#problema-de-behrens-fisher","text":"Quando os 4 par\u00e2metros das normais s\u00e3o desconhecidos, t\u00e3o pouco a raz\u00e3o de vari\u00e2ncias, nem a estat\u00edstica de raz\u00e3o de verossimilhan\u00e7a tem distribui\u00e7\u00e3o conhecida. Algumas tentativas j\u00e1 foram feitas, como Welch e outros .","title":"Problema de Behrens-Fisher"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#comparando-variancias-de-duas-normais","text":"Assumimos que X = (X_1,...,X_m) \u00e9 uma amostra da distribui\u00e7\u00e3o normal com m\u00e9dia \\mu_1 e vari\u00e2ncia \\sigma^2 , enquanto Y = (Y_1, ..., Y_n) \u00e9 normal com m\u00e9dia \\mu_2 e vari\u00e2ncia \\sigma^2 . Estamos interessados no teste H_0: \\sigma_1^2 \\le \\sigma_2^2 H_1: \\sigma_1^2 > \\sigma_2^2 A fun\u00e7\u00e3o poder \u00e9 dada por \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) . COnsidere S_x^2 e S_y^2 definidos anteriormente. Ent\u00e3o temos que S_x^2/(m-1) \u00e9 estimador para \\sigma_1^2 , enquanto S_y^2/(n-1) \u00e9 estimador para \\sigma_2^2 . Defina V = \\frac{S_x^2/(m-1)}{S_y^2/(n-1)} Rejeitaremos X_0 se V \\ge c , onde c ser\u00e1 escolhido para que esse teste tenha n\u00edvel de signific\u00e2ncia \\alpha_0 . Esse teste \u00e9 chamado de teste F, pois a distribui\u00e7\u00e3o de (\\sigma_1^2/\\sigma_2^2)V \u00e9 F com par\u00e2metros m-1 e n-1 . Em particular se \\sigma_1^2 = \\sigma_2^2 , V tem distribui\u00e7\u00e3o F. Onde a distribui\u00e7\u00e3o F \u00e9 descrita aqui .","title":"Comparando vari\u00e2ncias de duas Normais"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#funcao-poder_2","text":"Considere o procedimento de teste \\delta que rejeite H_0 se V \\ge F_{m-1,n-1}^{-1}(1 - \\alpha_0) . Assim: \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) = 1 - F_{m-1,n-1}(\\frac{\\sigma_2^2}{\\sigma_1^2}c) \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) = \\alpha_0 , quando \\sigma_1^2 = \\sigma^2_2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) < \\alpha_0 , quando \\sigma_1^2 < \\sigma_2^2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) > \\alpha_0 , quando \\sigma^2_1 > \\sigma_2^2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) \\to 0 , quando \\sigma_1^2/\\sigma_2^2 \\to 0 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) \\to 1 , quando \\sigma_1^2/\\sigma_2^2 \\to \\infty . Al\u00e9m do mais o teste \u00e9 n\u00e3o enviesado.","title":"Fun\u00e7\u00e3o Poder"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#p-valor_2","text":"Depois de termos observado as amostras, seja v a estat\u00edstica observada de V . O p-valor da hip\u00f3tese \u00e9 1 - F_{m-1,n-1}(v) .","title":"P-valor"},{"location":"infestatistica/TestsUniformlyPoweful/TestsUniformlyPoweful/","text":"Testes Uniformemente mais Poderosos Estamos lidando com um teste de hip\u00f3teses com as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n de uma distribui\u00e7\u00e3o parametrizada em \\theta desconhecido. H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Assumiremos que \\Omega_1 n\u00e3o \u00e9 conjunto unit\u00e1rio. Tamb\u00e9m suponha que o n\u00edvel de signific\u00e2ncia do teste seja \\alpha_0 , isto \u00e9 \\pi(\\theta|\\delta) \\le \\alpha_0, \\forall \\theta \\in \\Omega_0 . Segundo essas condi\u00e7\u00f5es, queremos encontrar o procedimento de teste \\delta que tem a menor probabilidade de erro do tipo II. TODO (Descrever os principais resultados)","title":"Testes Uniformemente mais Poderosos"},{"location":"infestatistica/TestsUniformlyPoweful/TestsUniformlyPoweful/#testes-uniformemente-mais-poderosos","text":"Estamos lidando com um teste de hip\u00f3teses com as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n de uma distribui\u00e7\u00e3o parametrizada em \\theta desconhecido. H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Assumiremos que \\Omega_1 n\u00e3o \u00e9 conjunto unit\u00e1rio. Tamb\u00e9m suponha que o n\u00edvel de signific\u00e2ncia do teste seja \\alpha_0 , isto \u00e9 \\pi(\\theta|\\delta) \\le \\alpha_0, \\forall \\theta \\in \\Omega_0 . Segundo essas condi\u00e7\u00f5es, queremos encontrar o procedimento de teste \\delta que tem a menor probabilidade de erro do tipo II. TODO (Descrever os principais resultados)","title":"Testes Uniformemente mais Poderosos"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/","text":"Estimadores n\u00e3o enviesados Defini\u00e7\u00e3o Um estimador \\delta(X) \u00e9 dito n\u00e3o enviesado para g(\\theta) se E_{\\theta}[\\delta(X)] = g(\\theta) para todo valor de \\theta . O vi\u00e9s do estimador \u00e9 definido por E_{\\theta}[\\delta(X)] - g(\\theta) . Se \\delta \u00e9 um estimador com vari\u00e2ncia finita, ent\u00e3o: R(\\theta, \\delta) = Var(\\delta) + Vi\u00e9s(\\delta)^2 Estimador n\u00e3o enviesado para vari\u00e2ncia s^2 = \\hat{\\sigma}_1^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 # Importando bibliotecas import numpy as np import pandas as pd from IPython.display import display , Math # Display latex import matplotlib.pyplot as plt % matplotlib inline np . random . seed ( 1000 ) # Garantindo reprodutibilidade Nota: Por que garantir reprodutibilidade? Reprodutibilidade \u00e9 a ideia de tornar o processo que foi feito por voc\u00ea reprodut\u00edvel por qualquer outra pessoa, para que ela possa obter os mesmos resultados seguindo os mesmos passos que voc\u00ea. Quando escolhemos um n\u00famero aleat\u00f3rio ( pseudoaleat\u00f3rio na verdade), ele muda de tempos em tempos. Mas isso vai tirar a ideia de \"garantir os mesmos resultados\". O resultado pode ser parecido, mas n\u00e3o exatamente igual. Isso \u00e9 muito importante no meio cient\u00edfico. Exemplo Vamos ver como se comporta esse estimador n\u00e3o viesado em uma popula\u00e7\u00e3o que representa o Brasil todo! Veja que eu n\u00e3o peguei dados online, porque quero TODA a popula\u00e7\u00e3o. Por isso vamos fazer uma simula\u00e7\u00e3o. A m\u00e9dia verdadeira da distribui\u00e7\u00e3o \u00e9 161,1cm e o desvio padr\u00e3o \u00e9 10cm. # Tamanho da popula\u00e7\u00e3o N = int ( 200e5 ) # Popula\u00e7\u00e3o gerada por simula\u00e7\u00e3o, usando a distribui\u00e7\u00e3o normal. population_height = pd . Series ( np . random . normal ( loc = 161.1 , scale = 10 , size = N )) Podemos ver a m\u00e9dia dessa popula\u00e7\u00e3o. population_height . mean () 161.10097546939974 O que a fun\u00e7\u00e3o var do pandas faz? Vamos comparar com o estimador trivial. ddof = 1 # Se ddof = 0, teremos a divis\u00e3o por N population_height . var ( ddof = ddof ) 100.02715920849081 Divindindo por N sigma_square_hat = (( population_height - population_height . mean ()) ** 2 ) . sum () / N sigma1_square_hat = (( population_height - population_height . mean ()) ** 2 ) . sum () / ( N - 1 ) display ( Math ( r '\\hat\\sigma^2 = {}' . format ( sigma_square_hat ))) display ( Math ( r '\\hat\\sigma_1^2 = {}' . format ( sigma1_square_hat ))) \\displaystyle \\hat\\sigma^2 = 100.02715420713488 \\displaystyle \\hat\\sigma_1^2 = 100.02715920849283 Estima\u00e7\u00e3o dos Par\u00e2metros Vamos supor que n\u00e3o conhecemos os par\u00e2metros da nossa popula\u00e7\u00e3o e podemos conhecer apenas uma amostra aleat\u00f3ria dela. Vamos fazer 500 dessas simula\u00e7\u00f5es number_simulations = 500 sample_size = 30 sample = pd . DataFrame ( population_height . sample ( n = number_simulations * sample_size , replace = True , random_state = 100 ), columns = [ 'height' ]) reshape = sample . to_numpy () . reshape (( - 1 , 30 )) samples = pd . DataFrame ( reshape , columns = range ( 0 , 30 )) Vamos estimar a m\u00e9dia com a m\u00e9dia amostral que \u00e9 n\u00e3o viesada tamb\u00e9m! Al\u00e9m disso ela \u00e9 o MLE. Estamos estimado para cada amostra a m\u00e9dia! Se fizermos uma m\u00e9dia das m\u00e9dias, veremos que ela chegar\u00e1 pr\u00f3ximo a m\u00e9dia verdadeira. estimated_mean = samples . mean ( axis = 1 ) #Axis = 1 faz a m\u00e9dia por linha. estimated_mean_of_means = estimated_mean . expanding () . mean () fig , ax = plt . subplots ( figsize = ( 8 , 5 )) ax . plot ( estimated_mean_of_means , label = 'Valor estimado' ) ax . hlines ( population_height . mean (), xmin = 0 , xmax = number_simulations , color = 'grey' , linestyle = '--' , alpha = 0.8 , label = 'Valor verdadeiro' ) ax . grid ( alpha = 0.4 ) ax . set_title ( 'Estimado a m\u00e9dia verdadeira' ) plt . show () Vamos comparar os estimadores para a vari\u00e2ncia, o viesado e o n\u00e3o viesado. df = pd . DataFrame ({ 'enviesado (dividido por n)' : samples . var ( ddof = 0 , axis = 1 ) . expanding () . mean (), 'nao_viesado (dividido por n - 1)' : samples . var ( ddof = 1 , axis = 1 ) . expanding () . mean (), 'verdadeiro' : pd . Series ( population_height . var ( ddof = 0 ), index = samples . index )}) ax = df . plot () ax . set_title ( 'Estimadores para a vari\u00e2ncia' ) ax . grid ( alpha = 0.4 ) plt . show () # Comparar com Conscist\u00eancia","title":"Estimadores n\u00e3o enviesados"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#estimadores-nao-enviesados","text":"","title":"Estimadores n\u00e3o enviesados"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#definicao","text":"Um estimador \\delta(X) \u00e9 dito n\u00e3o enviesado para g(\\theta) se E_{\\theta}[\\delta(X)] = g(\\theta) para todo valor de \\theta . O vi\u00e9s do estimador \u00e9 definido por E_{\\theta}[\\delta(X)] - g(\\theta) . Se \\delta \u00e9 um estimador com vari\u00e2ncia finita, ent\u00e3o: R(\\theta, \\delta) = Var(\\delta) + Vi\u00e9s(\\delta)^2","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#estimador-nao-enviesado-para-variancia","text":"s^2 = \\hat{\\sigma}_1^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 # Importando bibliotecas import numpy as np import pandas as pd from IPython.display import display , Math # Display latex import matplotlib.pyplot as plt % matplotlib inline np . random . seed ( 1000 ) # Garantindo reprodutibilidade","title":"Estimador n\u00e3o enviesado para vari\u00e2ncia"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#nota-por-que-garantir-reprodutibilidade","text":"Reprodutibilidade \u00e9 a ideia de tornar o processo que foi feito por voc\u00ea reprodut\u00edvel por qualquer outra pessoa, para que ela possa obter os mesmos resultados seguindo os mesmos passos que voc\u00ea. Quando escolhemos um n\u00famero aleat\u00f3rio ( pseudoaleat\u00f3rio na verdade), ele muda de tempos em tempos. Mas isso vai tirar a ideia de \"garantir os mesmos resultados\". O resultado pode ser parecido, mas n\u00e3o exatamente igual. Isso \u00e9 muito importante no meio cient\u00edfico.","title":"Nota: Por que garantir reprodutibilidade?"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#exemplo","text":"Vamos ver como se comporta esse estimador n\u00e3o viesado em uma popula\u00e7\u00e3o que representa o Brasil todo! Veja que eu n\u00e3o peguei dados online, porque quero TODA a popula\u00e7\u00e3o. Por isso vamos fazer uma simula\u00e7\u00e3o. A m\u00e9dia verdadeira da distribui\u00e7\u00e3o \u00e9 161,1cm e o desvio padr\u00e3o \u00e9 10cm. # Tamanho da popula\u00e7\u00e3o N = int ( 200e5 ) # Popula\u00e7\u00e3o gerada por simula\u00e7\u00e3o, usando a distribui\u00e7\u00e3o normal. population_height = pd . Series ( np . random . normal ( loc = 161.1 , scale = 10 , size = N )) Podemos ver a m\u00e9dia dessa popula\u00e7\u00e3o. population_height . mean () 161.10097546939974 O que a fun\u00e7\u00e3o var do pandas faz? Vamos comparar com o estimador trivial. ddof = 1 # Se ddof = 0, teremos a divis\u00e3o por N population_height . var ( ddof = ddof ) 100.02715920849081 Divindindo por N sigma_square_hat = (( population_height - population_height . mean ()) ** 2 ) . sum () / N sigma1_square_hat = (( population_height - population_height . mean ()) ** 2 ) . sum () / ( N - 1 ) display ( Math ( r '\\hat\\sigma^2 = {}' . format ( sigma_square_hat ))) display ( Math ( r '\\hat\\sigma_1^2 = {}' . format ( sigma1_square_hat ))) \\displaystyle \\hat\\sigma^2 = 100.02715420713488 \\displaystyle \\hat\\sigma_1^2 = 100.02715920849283","title":"Exemplo"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#estimacao-dos-parametros","text":"Vamos supor que n\u00e3o conhecemos os par\u00e2metros da nossa popula\u00e7\u00e3o e podemos conhecer apenas uma amostra aleat\u00f3ria dela. Vamos fazer 500 dessas simula\u00e7\u00f5es number_simulations = 500 sample_size = 30 sample = pd . DataFrame ( population_height . sample ( n = number_simulations * sample_size , replace = True , random_state = 100 ), columns = [ 'height' ]) reshape = sample . to_numpy () . reshape (( - 1 , 30 )) samples = pd . DataFrame ( reshape , columns = range ( 0 , 30 )) Vamos estimar a m\u00e9dia com a m\u00e9dia amostral que \u00e9 n\u00e3o viesada tamb\u00e9m! Al\u00e9m disso ela \u00e9 o MLE. Estamos estimado para cada amostra a m\u00e9dia! Se fizermos uma m\u00e9dia das m\u00e9dias, veremos que ela chegar\u00e1 pr\u00f3ximo a m\u00e9dia verdadeira. estimated_mean = samples . mean ( axis = 1 ) #Axis = 1 faz a m\u00e9dia por linha. estimated_mean_of_means = estimated_mean . expanding () . mean () fig , ax = plt . subplots ( figsize = ( 8 , 5 )) ax . plot ( estimated_mean_of_means , label = 'Valor estimado' ) ax . hlines ( population_height . mean (), xmin = 0 , xmax = number_simulations , color = 'grey' , linestyle = '--' , alpha = 0.8 , label = 'Valor verdadeiro' ) ax . grid ( alpha = 0.4 ) ax . set_title ( 'Estimado a m\u00e9dia verdadeira' ) plt . show () Vamos comparar os estimadores para a vari\u00e2ncia, o viesado e o n\u00e3o viesado. df = pd . DataFrame ({ 'enviesado (dividido por n)' : samples . var ( ddof = 0 , axis = 1 ) . expanding () . mean (), 'nao_viesado (dividido por n - 1)' : samples . var ( ddof = 1 , axis = 1 ) . expanding () . mean (), 'verdadeiro' : pd . Series ( population_height . var ( ddof = 0 ), index = samples . index )}) ax = df . plot () ax . set_title ( 'Estimadores para a vari\u00e2ncia' ) ax . grid ( alpha = 0.4 ) plt . show () # Comparar com Conscist\u00eancia","title":"Estima\u00e7\u00e3o dos Par\u00e2metros"}]}