{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Monitorias Este website foi desenvolvido para concentrar as informa\u00e7\u00f5es sobre as monitorias que dei ao longo da minha vida acad\u00eamica. Em particular durante a Gradua\u00e7\u00e3o na Escola de Matem\u00e1tica Aplicada, Funda\u00e7\u00e3o Getulio Vargas (EMAp/FGV). Eu espero que a informa\u00e7\u00e3o aqui contida seja de interesse! T\u00f3picos abordados \u00c1lgebra Linear (2019.2) Professor Eduardo Wagner Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias (2020.1) Professora Maria Izabel Camacho Infer\u00eancia \u00e0 Estat\u00edstica (2020.2) Professor Luiz Max de Carvalho Refer\u00eancias interessantes de matem\u00e1tica SageMath : software matem\u00e1tico que facilita c\u00e1lculos. \u00c9 uma possibilidade alternativa ao WolframAlpha , por\u00e9m com c\u00f3digo em Python. Manin : ferramenta para criar v\u00eddeos matem\u00e1ticos explicativos com anima\u00e7\u00e3o program\u00e1tica, uma biblioteca para Python.","title":"Home"},{"location":"#monitorias","text":"Este website foi desenvolvido para concentrar as informa\u00e7\u00f5es sobre as monitorias que dei ao longo da minha vida acad\u00eamica. Em particular durante a Gradua\u00e7\u00e3o na Escola de Matem\u00e1tica Aplicada, Funda\u00e7\u00e3o Getulio Vargas (EMAp/FGV). Eu espero que a informa\u00e7\u00e3o aqui contida seja de interesse!","title":"Monitorias"},{"location":"#topicos-abordados","text":"\u00c1lgebra Linear (2019.2) Professor Eduardo Wagner Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias (2020.1) Professora Maria Izabel Camacho Infer\u00eancia \u00e0 Estat\u00edstica (2020.2) Professor Luiz Max de Carvalho","title":"T\u00f3picos abordados"},{"location":"#referencias-interessantes-de-matematica","text":"SageMath : software matem\u00e1tico que facilita c\u00e1lculos. \u00c9 uma possibilidade alternativa ao WolframAlpha , por\u00e9m com c\u00f3digo em Python. Manin : ferramenta para criar v\u00eddeos matem\u00e1ticos explicativos com anima\u00e7\u00e3o program\u00e1tica, uma biblioteca para Python.","title":"Refer\u00eancias interessantes de matem\u00e1tica"},{"location":"alglin/info/","text":"Informa\u00e7\u00f5es Gerais Este t\u00f3pico ainda est\u00e1 em desenvolvimento. Monitoriais","title":"\u00c1lgebra Linear"},{"location":"alglin/info/#informacoes-gerais","text":"Este t\u00f3pico ainda est\u00e1 em desenvolvimento. Monitoriais","title":"Informa\u00e7\u00f5es Gerais"},{"location":"edo/info/","text":"Informa\u00e7\u00f5es Gerais Este t\u00f3pico ainda est\u00e1 em desenvolvimento. Monitoria","title":"Equa\u00e7\u00f5es Diferenciais"},{"location":"edo/info/#informacoes-gerais","text":"Este t\u00f3pico ainda est\u00e1 em desenvolvimento. Monitoria","title":"Informa\u00e7\u00f5es Gerais"},{"location":"infestatistica/SufficientStatistics/","text":"Estat\u00edsticas Suficientes A ideia por tra\u015b da estat\u00edstica \u00e9, como o nome diz, ser suficiente. Uma estat\u00edstica \u00e9 uma fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias, como, por exemplo, T = r(X_1, ..., X_n) . M\u00e9dia amostral, vari\u00e2ncia amostral, valor m\u00e1ximo, s\u00e3o todos exemplos. Imagine que temos um problema como o seguinte: Vamos imaginar que um estat\u00edstico d\u00e1 um trabalho para seu estagi\u00e1rio para organizar os dados de forma mais eficiente poss\u00edvel, enquanto ele pensa no modelo. O estagi\u00e1rio de forma muito ing\u00eanua cria uma lista em seu Jupyter Notebook e salva o notebook com os dados na sua lista. Depois ele salva num arquivo .txt e vai para casa tranquilo que o trabalho acabou mais cedo. Ser\u00e1 que era necess\u00e1rio ter salvo todos os dados? O estat\u00edstico no dia seuinte diz que n\u00e3o! E manda o estagi\u00e1rio estudar novamente estat\u00edstica. Ele disse para estudar Estat\u00edstica Suficientes . Defini\u00e7\u00e3o Unidimensional Seja X_1, ..., X_n uma amostra aleat\u00f3ria de distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta . Suponha que para todo valor que \\theta assume e para todo valor que T assume (vamos chamar de t = r(x_1, ..., x_n) , nesse caso j\u00e1 observamos o processo e calculamos t ), a distribui\u00e7\u00e3o conjunta condicional de X_1, ...., X_n dado T=t e \\theta , isto \u00e9, dado que voc\u00ea observou uma estat\u00edstica (a m\u00e9dia de temperaturas, por exemplo) depende apenas de t , mas n\u00e3o de \\theta . Isso significa que a distribui\u00e7\u00e3o \u00e9 constante para todos os valores de \\theta . Chamaremos essa estat\u00edstica T de suficiente para \\theta . Obs.: Para quem estudou fun\u00e7\u00f5es mensur\u00e1veis, podemos definir estat\u00edstica como fun\u00e7\u00e3o mensur\u00e1vel dos dados. Seja (\\mathbb{T}, \\mathbb{C}) um espa\u00e7o mensur\u00e1vel tal que \\mathbb{C} cont\u00e9m todos os conjuntos unit\u00e1rios. Se T : \\mathbb{X} \\to \\mathbb{T} \u00e9 mensu\u00e1vel, ent\u00e3o \u00e9 uma estat\u00edstica. Seja \\mathbb{P}_0 uma familia param\u00e9trica de distribui\u00e7\u00f5es em (\\mathbb{X}, \\mathbb{B}) . Seja (\\Omega, \\tau) um espa\u00e7o dos par\u00e2metros e \\Theta: \\mathbb{P}_0 \\to \\Omega um par\u00e2metro. Seja T uma estat\u00edstica. Ela \u00e9 suficiente para \\Theta se para toda priori \\mu_{\\Theta} , existem vers\u00f5es da posteriori \\mu_{\\Theta|X} e \\mu_{\\Theta|T} tal que \\forall B \\in \\tau, \\mu_{\\Theta|X}(B|x) = \\mu_{\\Theta|T}(B|T(x)) , quase certamente convergente para [\\mu_X] onde \\mu_X \u00e9 distribui\u00e7\u00e3o marginal de X . Crit\u00e9rio de Fatoriza\u00e7\u00e3o Teorema atribu\u00eddo a Neyman-Fisher. X_1,...X_n amostra aleat\u00f3ria com pdf ou pmf f(x|\\theta) , onde \\theta \u00e9 desconhecido. Uma estat\u00edstica T = r(X) para \\theta \u00e9 suficiente se, e somente se, a distribui\u00e7\u00e3o conjunta f_n(x|\\theta) pode ser fatorada para todo valor x \\in \\mathbb{R}^n da seguinte forma: f_n(x|\\theta) = u(x)v[r(x), \\theta] Onde u e v s\u00e3o n\u00e3o negativas, u n\u00e3o depende de \\theta e v s\u00f3 depende dos dados atrav\u00e9s da estat\u00edstica. Isto \u00e9, n\u00e3o adianta voc\u00ea encontrar qualquer fun\u00e7\u00e3o de x , tem que encontrar a estat\u00edstica T em v . Estat\u00edsticas Conjuntas Suficientes Suponha que para cada \\theta , vetor, e cada valor das estat\u00edsticas (T_1, ..., T_k) = (t_1, ..., t_k) a distribui\u00e7\u00e3o conjunta condicional dos dados dadas as estat\u00edsticas n\u00e3o depende de \\theta . Veja que nesse caso, a diferen\u00e7a \u00e9 que condiciono em k estat\u00edsticas, k \\geq 1 . Crit\u00e9rio de Fatoriza\u00e7\u00e3o Sejam r_1, ..., r_k fun\u00e7\u00f5es de n vari\u00e1veis. A estat\u00edsticas T_i = r_i(X) s\u00e3o estat\u00edsticas suficientes conjuntas para \\theta se, e somene se, a pdf conjunta f_n(x|\\theta) pode ser fatorado como f_n(x|\\theta) = u(x)v[r_1(x), ..., r_k(x),\\theta], para todos os valores x \\in \\mathbb{R}^n e \\theta \\in \\Omega Obs.: Podemos mostrar que qualquer fun\u00e7\u00e3o injetiva de uma estat\u00edstica suficiente \u00e9 uma estat\u00edstica suficiente. Estat\u00edstica Suficiente M\u00ednima Estat\u00edstica de Ordem Considere uma amostra aleat\u00f3ria e a ordene. Diremos que a nova amostra, ordenada, \u00e9 uma estat\u00edstica de ordem. Observe que ela funciona como uma matrix de \"shifts\" que opera trocando as linhas do vetor de lugar. Por isso ela \u00e9 uma fun\u00e7\u00e3o. Essa estat\u00edstica \u00e9 sufciente conjunta para \\theta . O interessante que podemos ver isso dado que o produt\u00f3rio n\u00e3o importa a ordem. Estat\u00edstica Suficiente M\u00ednima \u00c9 uma estat\u00edstica T suficiente e, al\u00e9m disso, \u00e9 fun\u00e7\u00e3o de todas as outras estat\u00edsticas suficientes. MLE e Estat\u00edstica Suficiente Seja T uma estat\u00edstica suficiente para \\theta . Ent\u00e3o o estimador de m\u00e1xima verossimilhan\u00e7a \\hat{\\theta} depende das observa\u00e7\u00f5es somente atrav\u00e9s da estat\u00edstica T . Al\u00e9m disso, se \\hat{\\theta} \u00e9 suficiente, ent\u00e3o \u00e9 m\u00ednimo. Estat\u00edsticas Suficientes e Estimador de Bayes T = r(X) estat\u00edstica suficiente para \\theta . Ent\u00e3o todo estimador de Bayes \\hat{\\theta} depende nas observa\u00e7\u00f5es X_1, ..., X_n apenas atrav\u00e9s da estat\u00edstica T . Al\u00e9m do mais, se for suficiente, ser\u00e1 suficiente m\u00ednimo. Defini\u00e7\u00f5es Adicionais Considere uma amostra aleat\u00f3ria X_1,...,X_n Estat\u00edstica Completa Seja t = T(X) estat\u00edstica. Se E[g(T(X))|\\theta] = 0, \\forall \\theta \\implies P[g(T(X)) = 0] = 1, ent\u00e3o ela \u00e9 dita completa. Estat\u00edstica Ancillary Suponha que queremos estimar \\theta e f_n(x|\\theta) seja a pdf conjunta. Seja A(X) uma estat\u00edstica. Se a sua distribui\u00e7\u00e3o n\u00e3o depende de \\theta , ent\u00e3o ser\u00e1 uma estat\u00edstica ancillary (auxiliar?) Por exemplo, se X_1, X_2 \\sim N(\\mu, \\sigma^2) e \\mu \u00e9 desconhecido, temos que X_1 - X_2 \\sim N(0, 2\\sigma^2) \u00e9 uma estat\u00edstica auxiliar. Melhorando um Estimador Suponha que temos uma amostra aleat\u00f3ria X = (X_1, ..., X_n) cuja pdf \u00e9 f(x|\\theta) e \\theta \\in \\Omega desconhecido, tal que queremos estimar h(\\theta) para alguma fun\u00e7\u00e3o h . Seja Z = g(X_1, ..., X_n) . E_{\\theta}(Z) = \\int_{-\\infty}^{\\infty}...\\int_{-\\infty}^{\\infty} g(x)f_n(x|\\theta)dx_1, ..., dx_n Para cada estimado \\delta(X) e para todo valor de \\theta , definimos o MSE (Erro M\u00e9dio Quadr\u00e1tico) R(\\theta, \\delta) = E_{\\theta}\\{[\\delta(X) - h(\\theta)]^2\\} Quando n\u00e3o atribu\u00edmos uma priori para \\theta , ent\u00e3o queremos encontrar um estimador para que o MSE seja pequeno para v\u00e1rios valores de \\theta . Seja T uma estat\u00edstica suficiente conhecida. Definimos \\delta_0(T) = E_{\\theta}\\{\\delta(X)|T\\} \\overset{1}{=} E\\{\\delta(X)|T\\} (1) Agora, por que podemos chamar \\delta_0 de estimador se depende de \\theta ? Como T \u00e9 uma estat\u00edstica suficiente, a distribui\u00e7\u00e3o condicionada em T e em \\theta da amostra X_1, ..., X_n n\u00e3o depende de \\theta !!! Em particular o valor esperado do estimador \\delta(T) . Logo, como esse valor esperado n\u00e3o depende de \\theta , podemos dizer sim que ele \u00e9 um estimador. Teorema Rao - Blackwell Teorema 7.9.1 do livro. Seja \\delta(X) um estimador e T uma estat\u00edstica suficiente para \\theta . O estimador \\delta_0(T) definido acima, para todo valor \\theta \\in \\Omega \u00e9: R(\\theta, \\delta_0) \\leq R(\\theta, \\delta), isto \u00e9, \u00e9 um estimador com menor erro quadr\u00e1tico m\u00e9dio (MSE). Em particular se R(\\theta, \\delta) < \\infty , a desigualdade se torna estrita, a menos que \\delta(X) seja um afun\u00e7\u00e3o de T , isto \u00e9, se \\delta(X) n\u00e3o for fun\u00e7\u00e3o de T , ent\u00e3o a desigualdade ser\u00e1 estrita. Por desigualdade estrita entenda < . Obs.: Chamamos o processo de melhorar um estimador com esse teorema de \"Rao-Blackwelliation\". Obs.2: Podemos generalizar um pouco mais. Para isso, pesquise sobre Conjuntos Convexos e sobre Fun\u00e7\u00f5es Convexas . Em um conjunto convexo, se a nossa fun\u00e7\u00e3o de perda n\u00e3o for o MSE, mas for uma fun\u00e7\u00e3o convexa, o teorema tamb\u00e9m valer\u00e1. Uma suposi\u00e7\u00e3o interessante que o Livro n\u00e3o imp\u00f5e \u00e9 que E[||\\delta(X)||) < \\infty . Inadmissibilidade Suponha que R(\\theta, \\delta) \u00e9 MSE. O estimador \\delta \u00e9 inadimiss\u00edvel se existe outro estimador \\delta_0 tal que R(\\theta, \\delta_0) \\leq R(\\theta, \\delta) para todo valor de \\theta e existe a desigualdade estrita em, pelo menos um valor de \\theta . Dizemos nesse caso que \\delta_0 domina o estimador \\delta . Um estimador \\delta_0 \u00e9 admiss\u00edvel se n\u00e3o existe outro estimador que o domine.","title":"Estat\u00edsticas Suficientes"},{"location":"infestatistica/SufficientStatistics/#estatisticas-suficientes","text":"A ideia por tra\u015b da estat\u00edstica \u00e9, como o nome diz, ser suficiente. Uma estat\u00edstica \u00e9 uma fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias, como, por exemplo, T = r(X_1, ..., X_n) . M\u00e9dia amostral, vari\u00e2ncia amostral, valor m\u00e1ximo, s\u00e3o todos exemplos. Imagine que temos um problema como o seguinte: Vamos imaginar que um estat\u00edstico d\u00e1 um trabalho para seu estagi\u00e1rio para organizar os dados de forma mais eficiente poss\u00edvel, enquanto ele pensa no modelo. O estagi\u00e1rio de forma muito ing\u00eanua cria uma lista em seu Jupyter Notebook e salva o notebook com os dados na sua lista. Depois ele salva num arquivo .txt e vai para casa tranquilo que o trabalho acabou mais cedo. Ser\u00e1 que era necess\u00e1rio ter salvo todos os dados? O estat\u00edstico no dia seuinte diz que n\u00e3o! E manda o estagi\u00e1rio estudar novamente estat\u00edstica. Ele disse para estudar Estat\u00edstica Suficientes .","title":"Estat\u00edsticas Suficientes"},{"location":"infestatistica/SufficientStatistics/#definicao-unidimensional","text":"Seja X_1, ..., X_n uma amostra aleat\u00f3ria de distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta . Suponha que para todo valor que \\theta assume e para todo valor que T assume (vamos chamar de t = r(x_1, ..., x_n) , nesse caso j\u00e1 observamos o processo e calculamos t ), a distribui\u00e7\u00e3o conjunta condicional de X_1, ...., X_n dado T=t e \\theta , isto \u00e9, dado que voc\u00ea observou uma estat\u00edstica (a m\u00e9dia de temperaturas, por exemplo) depende apenas de t , mas n\u00e3o de \\theta . Isso significa que a distribui\u00e7\u00e3o \u00e9 constante para todos os valores de \\theta . Chamaremos essa estat\u00edstica T de suficiente para \\theta . Obs.: Para quem estudou fun\u00e7\u00f5es mensur\u00e1veis, podemos definir estat\u00edstica como fun\u00e7\u00e3o mensur\u00e1vel dos dados. Seja (\\mathbb{T}, \\mathbb{C}) um espa\u00e7o mensur\u00e1vel tal que \\mathbb{C} cont\u00e9m todos os conjuntos unit\u00e1rios. Se T : \\mathbb{X} \\to \\mathbb{T} \u00e9 mensu\u00e1vel, ent\u00e3o \u00e9 uma estat\u00edstica. Seja \\mathbb{P}_0 uma familia param\u00e9trica de distribui\u00e7\u00f5es em (\\mathbb{X}, \\mathbb{B}) . Seja (\\Omega, \\tau) um espa\u00e7o dos par\u00e2metros e \\Theta: \\mathbb{P}_0 \\to \\Omega um par\u00e2metro. Seja T uma estat\u00edstica. Ela \u00e9 suficiente para \\Theta se para toda priori \\mu_{\\Theta} , existem vers\u00f5es da posteriori \\mu_{\\Theta|X} e \\mu_{\\Theta|T} tal que \\forall B \\in \\tau, \\mu_{\\Theta|X}(B|x) = \\mu_{\\Theta|T}(B|T(x)) , quase certamente convergente para [\\mu_X] onde \\mu_X \u00e9 distribui\u00e7\u00e3o marginal de X .","title":"Defini\u00e7\u00e3o Unidimensional"},{"location":"infestatistica/SufficientStatistics/#criterio-de-fatorizacao","text":"Teorema atribu\u00eddo a Neyman-Fisher. X_1,...X_n amostra aleat\u00f3ria com pdf ou pmf f(x|\\theta) , onde \\theta \u00e9 desconhecido. Uma estat\u00edstica T = r(X) para \\theta \u00e9 suficiente se, e somente se, a distribui\u00e7\u00e3o conjunta f_n(x|\\theta) pode ser fatorada para todo valor x \\in \\mathbb{R}^n da seguinte forma: f_n(x|\\theta) = u(x)v[r(x), \\theta] Onde u e v s\u00e3o n\u00e3o negativas, u n\u00e3o depende de \\theta e v s\u00f3 depende dos dados atrav\u00e9s da estat\u00edstica. Isto \u00e9, n\u00e3o adianta voc\u00ea encontrar qualquer fun\u00e7\u00e3o de x , tem que encontrar a estat\u00edstica T em v .","title":"Crit\u00e9rio de Fatoriza\u00e7\u00e3o"},{"location":"infestatistica/SufficientStatistics/#estatisticas-conjuntas-suficientes","text":"Suponha que para cada \\theta , vetor, e cada valor das estat\u00edsticas (T_1, ..., T_k) = (t_1, ..., t_k) a distribui\u00e7\u00e3o conjunta condicional dos dados dadas as estat\u00edsticas n\u00e3o depende de \\theta . Veja que nesse caso, a diferen\u00e7a \u00e9 que condiciono em k estat\u00edsticas, k \\geq 1 .","title":"Estat\u00edsticas Conjuntas Suficientes"},{"location":"infestatistica/SufficientStatistics/#criterio-de-fatorizacao_1","text":"Sejam r_1, ..., r_k fun\u00e7\u00f5es de n vari\u00e1veis. A estat\u00edsticas T_i = r_i(X) s\u00e3o estat\u00edsticas suficientes conjuntas para \\theta se, e somene se, a pdf conjunta f_n(x|\\theta) pode ser fatorado como f_n(x|\\theta) = u(x)v[r_1(x), ..., r_k(x),\\theta], para todos os valores x \\in \\mathbb{R}^n e \\theta \\in \\Omega Obs.: Podemos mostrar que qualquer fun\u00e7\u00e3o injetiva de uma estat\u00edstica suficiente \u00e9 uma estat\u00edstica suficiente.","title":"Crit\u00e9rio de Fatoriza\u00e7\u00e3o"},{"location":"infestatistica/SufficientStatistics/#estatistica-suficiente-minima","text":"","title":"Estat\u00edstica Suficiente M\u00ednima"},{"location":"infestatistica/SufficientStatistics/#estatistica-de-ordem","text":"Considere uma amostra aleat\u00f3ria e a ordene. Diremos que a nova amostra, ordenada, \u00e9 uma estat\u00edstica de ordem. Observe que ela funciona como uma matrix de \"shifts\" que opera trocando as linhas do vetor de lugar. Por isso ela \u00e9 uma fun\u00e7\u00e3o. Essa estat\u00edstica \u00e9 sufciente conjunta para \\theta . O interessante que podemos ver isso dado que o produt\u00f3rio n\u00e3o importa a ordem.","title":"Estat\u00edstica de Ordem"},{"location":"infestatistica/SufficientStatistics/#estatistica-suficiente-minima_1","text":"\u00c9 uma estat\u00edstica T suficiente e, al\u00e9m disso, \u00e9 fun\u00e7\u00e3o de todas as outras estat\u00edsticas suficientes.","title":"Estat\u00edstica Suficiente M\u00ednima"},{"location":"infestatistica/SufficientStatistics/#mle-e-estatistica-suficiente","text":"Seja T uma estat\u00edstica suficiente para \\theta . Ent\u00e3o o estimador de m\u00e1xima verossimilhan\u00e7a \\hat{\\theta} depende das observa\u00e7\u00f5es somente atrav\u00e9s da estat\u00edstica T . Al\u00e9m disso, se \\hat{\\theta} \u00e9 suficiente, ent\u00e3o \u00e9 m\u00ednimo.","title":"MLE e Estat\u00edstica Suficiente"},{"location":"infestatistica/SufficientStatistics/#estatisticas-suficientes-e-estimador-de-bayes","text":"T = r(X) estat\u00edstica suficiente para \\theta . Ent\u00e3o todo estimador de Bayes \\hat{\\theta} depende nas observa\u00e7\u00f5es X_1, ..., X_n apenas atrav\u00e9s da estat\u00edstica T . Al\u00e9m do mais, se for suficiente, ser\u00e1 suficiente m\u00ednimo.","title":"Estat\u00edsticas Suficientes e Estimador de Bayes"},{"location":"infestatistica/SufficientStatistics/#definicoes-adicionais","text":"Considere uma amostra aleat\u00f3ria X_1,...,X_n","title":"Defini\u00e7\u00f5es Adicionais"},{"location":"infestatistica/SufficientStatistics/#estatistica-completa","text":"Seja t = T(X) estat\u00edstica. Se E[g(T(X))|\\theta] = 0, \\forall \\theta \\implies P[g(T(X)) = 0] = 1, ent\u00e3o ela \u00e9 dita completa.","title":"Estat\u00edstica Completa"},{"location":"infestatistica/SufficientStatistics/#estatistica-ancillary","text":"Suponha que queremos estimar \\theta e f_n(x|\\theta) seja a pdf conjunta. Seja A(X) uma estat\u00edstica. Se a sua distribui\u00e7\u00e3o n\u00e3o depende de \\theta , ent\u00e3o ser\u00e1 uma estat\u00edstica ancillary (auxiliar?) Por exemplo, se X_1, X_2 \\sim N(\\mu, \\sigma^2) e \\mu \u00e9 desconhecido, temos que X_1 - X_2 \\sim N(0, 2\\sigma^2) \u00e9 uma estat\u00edstica auxiliar.","title":"Estat\u00edstica Ancillary"},{"location":"infestatistica/SufficientStatistics/#melhorando-um-estimador","text":"Suponha que temos uma amostra aleat\u00f3ria X = (X_1, ..., X_n) cuja pdf \u00e9 f(x|\\theta) e \\theta \\in \\Omega desconhecido, tal que queremos estimar h(\\theta) para alguma fun\u00e7\u00e3o h . Seja Z = g(X_1, ..., X_n) . E_{\\theta}(Z) = \\int_{-\\infty}^{\\infty}...\\int_{-\\infty}^{\\infty} g(x)f_n(x|\\theta)dx_1, ..., dx_n Para cada estimado \\delta(X) e para todo valor de \\theta , definimos o MSE (Erro M\u00e9dio Quadr\u00e1tico) R(\\theta, \\delta) = E_{\\theta}\\{[\\delta(X) - h(\\theta)]^2\\} Quando n\u00e3o atribu\u00edmos uma priori para \\theta , ent\u00e3o queremos encontrar um estimador para que o MSE seja pequeno para v\u00e1rios valores de \\theta . Seja T uma estat\u00edstica suficiente conhecida. Definimos \\delta_0(T) = E_{\\theta}\\{\\delta(X)|T\\} \\overset{1}{=} E\\{\\delta(X)|T\\} (1) Agora, por que podemos chamar \\delta_0 de estimador se depende de \\theta ? Como T \u00e9 uma estat\u00edstica suficiente, a distribui\u00e7\u00e3o condicionada em T e em \\theta da amostra X_1, ..., X_n n\u00e3o depende de \\theta !!! Em particular o valor esperado do estimador \\delta(T) . Logo, como esse valor esperado n\u00e3o depende de \\theta , podemos dizer sim que ele \u00e9 um estimador.","title":"Melhorando um Estimador"},{"location":"infestatistica/SufficientStatistics/#teorema-rao-blackwell","text":"Teorema 7.9.1 do livro. Seja \\delta(X) um estimador e T uma estat\u00edstica suficiente para \\theta . O estimador \\delta_0(T) definido acima, para todo valor \\theta \\in \\Omega \u00e9: R(\\theta, \\delta_0) \\leq R(\\theta, \\delta), isto \u00e9, \u00e9 um estimador com menor erro quadr\u00e1tico m\u00e9dio (MSE). Em particular se R(\\theta, \\delta) < \\infty , a desigualdade se torna estrita, a menos que \\delta(X) seja um afun\u00e7\u00e3o de T , isto \u00e9, se \\delta(X) n\u00e3o for fun\u00e7\u00e3o de T , ent\u00e3o a desigualdade ser\u00e1 estrita. Por desigualdade estrita entenda < . Obs.: Chamamos o processo de melhorar um estimador com esse teorema de \"Rao-Blackwelliation\". Obs.2: Podemos generalizar um pouco mais. Para isso, pesquise sobre Conjuntos Convexos e sobre Fun\u00e7\u00f5es Convexas . Em um conjunto convexo, se a nossa fun\u00e7\u00e3o de perda n\u00e3o for o MSE, mas for uma fun\u00e7\u00e3o convexa, o teorema tamb\u00e9m valer\u00e1. Uma suposi\u00e7\u00e3o interessante que o Livro n\u00e3o imp\u00f5e \u00e9 que E[||\\delta(X)||) < \\infty .","title":"Teorema Rao - Blackwell"},{"location":"infestatistica/SufficientStatistics/#inadmissibilidade","text":"Suponha que R(\\theta, \\delta) \u00e9 MSE. O estimador \\delta \u00e9 inadimiss\u00edvel se existe outro estimador \\delta_0 tal que R(\\theta, \\delta_0) \\leq R(\\theta, \\delta) para todo valor de \\theta e existe a desigualdade estrita em, pelo menos um valor de \\theta . Dizemos nesse caso que \\delta_0 domina o estimador \\delta . Um estimador \\delta_0 \u00e9 admiss\u00edvel se n\u00e3o existe outro estimador que o domine.","title":"Inadmissibilidade"},{"location":"infestatistica/info/","text":"Informa\u00e7\u00f5es Gerais As monitorias ocorrem todas as quinta-feiras, \u00e0s 16h, atrav\u00e9s da plataforma Zoom. Link para acesso Monitorias gravadas Notebooks : Voc\u00ea pode rodar esses notebooks tamb\u00e9m! T\u00f3picos e Exerc\u00edcios Os exerc\u00edcios resolvidos dispon\u00edveis est\u00e3o linkados ao t\u00edtulo da se\u00e7\u00e3o. A1 Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios Resolvido em Monitoria 7.2 Priori e Posteriori 2,3,10 Sim 7.3 Fam\u00edlias Conjugadas 2,17,19,21 Sim 7.4 Estimador de Bayes 2,4,7,11,14 Sim 7.5 Estimador de M\u00e1xima Verossimilhan\u00e7a 1,4,9,10 Sim 7.6 Propriedades EMV 3,5,11,20,22,23 Sim 7.7 Estat\u00edstica Suficiente 4,7,13,16 Sim 7.8 Estat\u00edstica Suficiente Conjunta 3,8,12,16 Sim 7.9 Melhorando um Estimador 2,3,6,9,10 Sim 8.7 Estimadores n\u00e3o viesados 4,6,11,13 Sim 8.8 Informa\u00e7\u00e3o de Fisher 5,7,10 Sim 8.1 Estimadores de Distribui\u00e7\u00f5es Amostrais 1,2,3,9 Sim 8.2 Distribui\u00e7\u00e3o Chi-Quadrada 4,7,10,13 Sim - Exerc\u00edcios de Revis\u00e3o - Sim A2 Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios Resolvido em Monitoria 8.3 Distribui\u00e7\u00e3o conjunta da m\u00e9dia e vari\u00e2ncia amostral 8 N\u00e3o 8.4 A distribui\u00e7\u00e3o t - - 8.5 Resumos Probability and Statistics (Morris H. DeGroot) Cap\u00edtulo 7 Cap\u00edtulo 8 - parte 1 Cap\u00edtulo 8 - parte 2 Cap\u00edtulo 9 Documentos Adicionais Theory of Statistical Estimation (Ronald Fisher) : problema da estima\u00e7\u00e3o \u00e9 abordado e as t\u00e9cnicas apresentadas por Ronald Fisher. Ele se debru\u00e7a sobre estat\u00edsticas eficientes e suficientes. Mathematical Foudations Statistics (Ronald Fisher) : refer\u00eancia em estat\u00edstica com principais conceitos da mat\u00e9ria. Digital TextBook Statlect","title":"Informa\u00e7\u00f5es Gerais"},{"location":"infestatistica/info/#informacoes-gerais","text":"As monitorias ocorrem todas as quinta-feiras, \u00e0s 16h, atrav\u00e9s da plataforma Zoom. Link para acesso Monitorias gravadas Notebooks : Voc\u00ea pode rodar esses notebooks tamb\u00e9m!","title":"Informa\u00e7\u00f5es Gerais"},{"location":"infestatistica/info/#topicos-e-exercicios","text":"Os exerc\u00edcios resolvidos dispon\u00edveis est\u00e3o linkados ao t\u00edtulo da se\u00e7\u00e3o.","title":"T\u00f3picos e Exerc\u00edcios"},{"location":"infestatistica/info/#a1","text":"Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios Resolvido em Monitoria 7.2 Priori e Posteriori 2,3,10 Sim 7.3 Fam\u00edlias Conjugadas 2,17,19,21 Sim 7.4 Estimador de Bayes 2,4,7,11,14 Sim 7.5 Estimador de M\u00e1xima Verossimilhan\u00e7a 1,4,9,10 Sim 7.6 Propriedades EMV 3,5,11,20,22,23 Sim 7.7 Estat\u00edstica Suficiente 4,7,13,16 Sim 7.8 Estat\u00edstica Suficiente Conjunta 3,8,12,16 Sim 7.9 Melhorando um Estimador 2,3,6,9,10 Sim 8.7 Estimadores n\u00e3o viesados 4,6,11,13 Sim 8.8 Informa\u00e7\u00e3o de Fisher 5,7,10 Sim 8.1 Estimadores de Distribui\u00e7\u00f5es Amostrais 1,2,3,9 Sim 8.2 Distribui\u00e7\u00e3o Chi-Quadrada 4,7,10,13 Sim - Exerc\u00edcios de Revis\u00e3o - Sim","title":"A1"},{"location":"infestatistica/info/#a2","text":"Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios Resolvido em Monitoria 8.3 Distribui\u00e7\u00e3o conjunta da m\u00e9dia e vari\u00e2ncia amostral 8 N\u00e3o 8.4 A distribui\u00e7\u00e3o t - - 8.5","title":"A2"},{"location":"infestatistica/info/#resumos","text":"Probability and Statistics (Morris H. DeGroot) Cap\u00edtulo 7 Cap\u00edtulo 8 - parte 1 Cap\u00edtulo 8 - parte 2 Cap\u00edtulo 9","title":"Resumos"},{"location":"infestatistica/info/#documentos-adicionais","text":"Theory of Statistical Estimation (Ronald Fisher) : problema da estima\u00e7\u00e3o \u00e9 abordado e as t\u00e9cnicas apresentadas por Ronald Fisher. Ele se debru\u00e7a sobre estat\u00edsticas eficientes e suficientes. Mathematical Foudations Statistics (Ronald Fisher) : refer\u00eancia em estat\u00edstica com principais conceitos da mat\u00e9ria. Digital TextBook Statlect","title":"Documentos Adicionais"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/","text":"Distribui\u00e7\u00f5es de Prioris Conjugadas Se a distribui\u00e7\u00e3o a priori \u00e9 membro de uma fam\u00edlia e a distribui\u00e7\u00e3o a posteriori tamb\u00e9m pertence a mesma a fam\u00edlia, essa fam\u00edlia de distribui\u00e7\u00f5es \u00e9 chamada de fam\u00edlia conjugada . A principal consequ\u00eancia de usar prioris de uma fam\u00edlia conjugada \u00e9 que as contas ficam muito mais simples. Principais Fam\u00edlias Conjugadas Teorema Suponha que X_1, ..., X_n \\overset{iid}{\\sim} \\text{Bernoulli}(\\theta) , 0 < \\theta < 1 desconhecido. Suponha que \\theta \\sim \\text{Beta}(\\alpha, \\beta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori de \\theta \u00e9 a distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n - \\sum_{i=1}^n x_i . Teorema Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Poisson}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n . Teorema Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Normal}(\\theta, \\sigma^2) , onde \\theta \u00e9 desconhecido e \\sigma \u00e9 conhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma normal com m\u00e9dia \\mu_0 e vari\u00e2ncia v_0^2 . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 normal com m\u00e9dia \\mu_1 e vari\u00e2ncia v_1^2 , onde: \\mu_1 = \\frac{\\sigma^2\\mu_0 + nv_0^2\\bar{x}_n}{\\sigma^2 + nv_0^2} v_1^2 = \\frac{\\sigma^2v_0^2}{\\sigma^2 + nv_0^2} Teorema Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + n e \\beta + \\sum_{i=1}^n x_i . import numpy as np from scipy import stats import matplotlib.pyplot as plt from matplotlib import animation , cm from IPython.display import HTML % matplotlib inline Suponha que \\theta seja a probabilidade de um item ser defeituoso em uma s\u00e9rie de items. Suponha que nossa priori em \\theta \u00e9 uma distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha e \\beta . S\u00e3o selecionados n items por vez para o teste. Sabemos que nossa posteriori ser\u00e1 uma Beta com os par\u00e2metros acima. Vejamos graficamente esse processo. theta_real = 0.1 n = 100 np . random . seed ( 10 ) XMIN = 0 XMAX = 1 YMIN = 0 YMAX = 5 alpha = [ 1 ] beta = [ 1 ] x = np . linspace ( 0.001 , 1 , 1000 ) # Definindo cores cmap = cm . autumn # Esta fun\u00e7\u00e3o permite plotar o backgroud def init (): line . set_data ([], []) return ( line ,) # Definindo o espa\u00e7o da imagem fig , ax = plt . subplots () # Definindo caracter\u00edsticas do background ax . set_xlim (( XMIN , XMAX )) ax . set_ylim (( YMIN , YMAX )) ax . set_title ( 'Evolu\u00e7\u00e3o da posteriori a cada itera\u00e7\u00e3o' ) ax . vlines ( theta_real , ymin = YMIN , ymax = YMAX , linestyle = '--' , color = 'grey' ) # Definindo plots vari\u00e1veis line , = ax . plot ([], [], lw = 2 ) line2 , _ = ax . plot ( XMIN , XMAX , YMIN , YMAX , linestyle = ':' ) def animate ( i , alpha , beta , x , n ): # Amostro da distribui\u00e7\u00e3o sample = np . random . binomial ( 1 , p = theta_real ) # Junto a lista que guarda os alphas e betas de cada itera\u00e7\u00e3o alpha . append ( alpha [ - 1 ] + sample ) beta . append ( beta [ - 1 ] + 1 - sample ) # Calculo a posteriori posteriori = stats . beta ( a = alpha [ - 1 ], b = beta [ - 1 ]) line . set_data ( x , posteriori . pdf ( x )) line . set_color ( cmap ( 1 - i / n )) line2 . set_data ([ posteriori . mean (), posteriori . mean ()], [ YMIN , YMAX ]) line2 . set_color ( cmap ( 1 - i / n )) return ( line , line2 ) anim = animation . FuncAnimation ( fig , animate , frames = n , init_func = init , interval = 100 , blit = True , fargs = ( alpha , beta , x , n ), repeat = False ) HTML ( anim . to_html5_video ()) Your browser does not support the video tag. Hiperpar\u00e2metros Seja \\Psi uma fam\u00edlia de distribui\u00e7\u00f5es poss\u00edveis sobre um espa\u00e7o de par\u00eamtros \\Omega . Suponha que independente da distribui\u00e7\u00e3o a priori dessa fam\u00edlia e n\u00e3o importando as observa\u00e7\u00f5es (quais s\u00e3o ou quantas s\u00e3o), a distribui\u00e7\u00e3o a posteriori seja da mesma fam\u00edlia. Chamamos \\Psi de fam\u00edlia conjunda de distribui\u00e7\u00f5es a priori. Os par\u00e2metros associados a essa fam\u00edlia s\u00e3o chamados de hiperpar\u00e2metros. Distribui\u00e7\u00f5es a Priori Impr\u00f3prias Seja \\xi uma fun\u00e7\u00e3o n\u00e3o negativa tal que \\Omega \u00e9 subconjunto de seu dom\u00ednio. Suponha que \\int \\xi(\\theta) d\\theta = \\infty . Se \\xi(\\theta) \u00e9 priori de \\theta , ela \u00e9 chamada de prioori impr\u00f3pria. Podemos gerar limites de distribui\u00e7\u00f5es, como, por exemplo, a distribui\u00e7\u00e3o uniforme [0,1] com intervalo sendo a reta, agora. Estimador de Bayes Estimador Seja X_1, ..., X_n dados observador cuja distribui\u00e7\u00e3o conjunta \u00e9 inndexada pelo par\u00e2metro \\theta . Um estimador do par\u00e2metro \\theta \u00e9 uma fun\u00e7\u00e3o real X_1, ..., X_n \\mapsto \\delta(X_1, ..., X_n) . Se X_i = x_i \u00e9 observado, \\delta(x_1,...,x_n) \u00e9 uma estimativa. Fun\u00e7\u00e3o de Perda \u00c9 uma fun\u00e7\u00e3o real L(\\theta, a) onde \\theta \\in \\Omega e a \\in \\mathbb{R} . Essa fun\u00e7\u00e3o procura indicar, para cada escoolha de \\theta , a perda do estat\u00edstico. Seja \\xi(\\theta) priori de \\theta . O valor esperado da perda \u00e9 dado por: E[L(\\theta, a)] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta) d\\theta, \\text{ a priori} E[L(\\theta, a)|x] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta|x) d\\theta, \\text{ a posteriori} Estimador de Bayes Seja L(\\theta, a) fun\u00e7\u00e3o de perda. Seja \\delta^*(x) o valor de a tal que E[L(\\theta, a)|x] \u00e9 minimizado. Ent\u00e3o \\delta^* \u00e9 o estimador de Bayes de \\theta . E[L(\\theta, \\delta^*(x))|x] = \\min_{a \\in \\mathbb{R}}E[L(\\theta, a)|x] Fun\u00e7\u00f5es de Perda: Exemplos Erro quadr\u00e1tico: L(\\theta, a) = (\\theta - a)^2 Queremos minimizar E[(\\theta - a)^2|x] \\delta^*(X) = E(\\theta| X) P\u00e1gina 260 (DeGroot) Erro absoluto: L(\\theta, a) = |\\theta - a| Queremos minimizar E[|\\theta - a||x] \\delta^*(X) = \\text{mediana (quartil 0.5)} P\u00e1gina 245 (DeGroot) Estimador Consistente Uma sequ\u00eancia de estimadores que converge em probabilidade para um valor desconhecido de um par\u00e2metro a ser estimafo \u00e9 chamado de sequ\u00eancia consistente de estimadores. Essa consist\u00eancia fala que em grandes amostras, o estimador estar\u00e1 pr\u00f3ximo o suficiente do valor desconhecido de \\theta . O estimador de Bayes, sob algumas condi\u00e7\u00f5es, forma uma sequ\u00eancia de estimadores consistentes. Limita\u00e7\u00f5es De acordo com a teoria Bayesiana , esse estimador \u00e9 o \u00fanico coerente que pode ser constru\u00eddo. \u00c9 importante que tenha-se definido uma fun\u00e7\u00e3o de perda e uma distribui\u00e7\u00e3o a priori para os par\u00e2metros. Quando \\theta \u00e9 um vetor, precisamos definit uma priori multivariada, mesmo que n\u00e3o queiramos estimar todos os par\u00e2metros. Exemplo 7.4.7 Quetelet reportou medidas do peito de 5732 homens militares. Os dados foram retirados desse site . import requests from bs4 import BeautifulSoup import pandas as pd Obtendo os dados direto do site Eu uso essas tr\u00eas bibliotecas, onde as duas primeiras s\u00e3o usadas para retirar informa\u00e7\u00e3o do site desejado. Veja que n\u00e3o coloco verifica\u00e7\u00e3o, pois o site tem esse problema. Depois eu coloco numa estrutura chamada DataFrame que \u00e9 basicamente uma tabela onde tem cada item nas linhas e cada caracter\u00edstica nas colunas. website = requests . get ( 'https://www.stat.cmu.edu/StatDat/Datafiles/MilitiamenChests.html' , verify = False ) soup = BeautifulSoup ( website . content ) data = soup . pre . text . strip () . split ( ' \\n ' ) chest_data = { 'Chest' : [], 'Count' : []} for item in data [ 1 :]: co , ch = item . split ( ' \\t ' ) chest_data [ 'Chest' ] . append ( int ( ch )) chest_data [ 'Count' ] . append ( int ( co )) chest_df = pd . DataFrame ( chest_data ) chest_df . head () /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Chest Count 0 33 3 1 34 18 2 35 81 3 36 185 4 37 420 plt . bar ( chest_df [ 'Chest' ], chest_df [ 'Count' ]) mean = sum ( chest_df [ 'Chest' ] * chest_df [ 'Count' ]) / chest_df [ 'Count' ] . sum () plt . vlines ( mean , ymin = 0 , ymax = 1200 , color = 'black' , linestyle = '--' , label = 'M\u00e9dia={:.2f}' . format ( mean )) plt . title ( 'Histograma do tamanho do ppeito de militares escoseses' ) plt . xlabel ( 'Medida de peitorais' ) plt . legend () plt . show () Vamos modelar as medidas do peitoral, como uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal com m\u00e9dia \\theta e vari\u00e2ncia \\sigma^2 , conhecido. Temos que a m\u00e9dia \u00e9 39.83 das amostras. Se \\theta \\sim N(\\mu_0, v_0^2) \u00e9 uma priori para \\theta , podemos calcular o estimador de Bayes a posteriori. Sabemos que a posteriori ser\u00e1 uma normal (conjugada) com m\u00e9dia e vari\u00e2ncia: \\mu_1 = \\frac{\\sigma^2 + 5732\\cdot v_0^2 \\cdot 39.83}{\\sigma^2 + 5732\\cdot v_0^2} v_1^2 = \\frac{\\sigma^2 v_0^2}{\\sigma^2 + 5732\\cdot v_0^2} O estimador de Bayes, segundo a perda quadr\u00e1tica, \u00e9 a m\u00e9dia a posteriori, portanto \\delta(x) = \\mu_1 Priori para \\theta \\mu_0 = 39.83 v_0^2 = 4","title":"Distribui\u00e7\u00f5es Conjugada e Estimador de Bayes"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#distribuicoes-de-prioris-conjugadas","text":"Se a distribui\u00e7\u00e3o a priori \u00e9 membro de uma fam\u00edlia e a distribui\u00e7\u00e3o a posteriori tamb\u00e9m pertence a mesma a fam\u00edlia, essa fam\u00edlia de distribui\u00e7\u00f5es \u00e9 chamada de fam\u00edlia conjugada . A principal consequ\u00eancia de usar prioris de uma fam\u00edlia conjugada \u00e9 que as contas ficam muito mais simples.","title":"Distribui\u00e7\u00f5es de Prioris Conjugadas"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#principais-familias-conjugadas","text":"","title":"Principais Fam\u00edlias Conjugadas"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#teorema","text":"Suponha que X_1, ..., X_n \\overset{iid}{\\sim} \\text{Bernoulli}(\\theta) , 0 < \\theta < 1 desconhecido. Suponha que \\theta \\sim \\text{Beta}(\\alpha, \\beta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori de \\theta \u00e9 a distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n - \\sum_{i=1}^n x_i .","title":"Teorema"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#teorema_1","text":"Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Poisson}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n .","title":"Teorema"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#teorema_2","text":"Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Normal}(\\theta, \\sigma^2) , onde \\theta \u00e9 desconhecido e \\sigma \u00e9 conhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma normal com m\u00e9dia \\mu_0 e vari\u00e2ncia v_0^2 . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 normal com m\u00e9dia \\mu_1 e vari\u00e2ncia v_1^2 , onde: \\mu_1 = \\frac{\\sigma^2\\mu_0 + nv_0^2\\bar{x}_n}{\\sigma^2 + nv_0^2} v_1^2 = \\frac{\\sigma^2v_0^2}{\\sigma^2 + nv_0^2}","title":"Teorema"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#teorema_3","text":"Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + n e \\beta + \\sum_{i=1}^n x_i . import numpy as np from scipy import stats import matplotlib.pyplot as plt from matplotlib import animation , cm from IPython.display import HTML % matplotlib inline Suponha que \\theta seja a probabilidade de um item ser defeituoso em uma s\u00e9rie de items. Suponha que nossa priori em \\theta \u00e9 uma distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha e \\beta . S\u00e3o selecionados n items por vez para o teste. Sabemos que nossa posteriori ser\u00e1 uma Beta com os par\u00e2metros acima. Vejamos graficamente esse processo. theta_real = 0.1 n = 100 np . random . seed ( 10 ) XMIN = 0 XMAX = 1 YMIN = 0 YMAX = 5 alpha = [ 1 ] beta = [ 1 ] x = np . linspace ( 0.001 , 1 , 1000 ) # Definindo cores cmap = cm . autumn # Esta fun\u00e7\u00e3o permite plotar o backgroud def init (): line . set_data ([], []) return ( line ,) # Definindo o espa\u00e7o da imagem fig , ax = plt . subplots () # Definindo caracter\u00edsticas do background ax . set_xlim (( XMIN , XMAX )) ax . set_ylim (( YMIN , YMAX )) ax . set_title ( 'Evolu\u00e7\u00e3o da posteriori a cada itera\u00e7\u00e3o' ) ax . vlines ( theta_real , ymin = YMIN , ymax = YMAX , linestyle = '--' , color = 'grey' ) # Definindo plots vari\u00e1veis line , = ax . plot ([], [], lw = 2 ) line2 , _ = ax . plot ( XMIN , XMAX , YMIN , YMAX , linestyle = ':' ) def animate ( i , alpha , beta , x , n ): # Amostro da distribui\u00e7\u00e3o sample = np . random . binomial ( 1 , p = theta_real ) # Junto a lista que guarda os alphas e betas de cada itera\u00e7\u00e3o alpha . append ( alpha [ - 1 ] + sample ) beta . append ( beta [ - 1 ] + 1 - sample ) # Calculo a posteriori posteriori = stats . beta ( a = alpha [ - 1 ], b = beta [ - 1 ]) line . set_data ( x , posteriori . pdf ( x )) line . set_color ( cmap ( 1 - i / n )) line2 . set_data ([ posteriori . mean (), posteriori . mean ()], [ YMIN , YMAX ]) line2 . set_color ( cmap ( 1 - i / n )) return ( line , line2 ) anim = animation . FuncAnimation ( fig , animate , frames = n , init_func = init , interval = 100 , blit = True , fargs = ( alpha , beta , x , n ), repeat = False ) HTML ( anim . to_html5_video ()) Your browser does not support the video tag.","title":"Teorema"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#hiperparametros","text":"Seja \\Psi uma fam\u00edlia de distribui\u00e7\u00f5es poss\u00edveis sobre um espa\u00e7o de par\u00eamtros \\Omega . Suponha que independente da distribui\u00e7\u00e3o a priori dessa fam\u00edlia e n\u00e3o importando as observa\u00e7\u00f5es (quais s\u00e3o ou quantas s\u00e3o), a distribui\u00e7\u00e3o a posteriori seja da mesma fam\u00edlia. Chamamos \\Psi de fam\u00edlia conjunda de distribui\u00e7\u00f5es a priori. Os par\u00e2metros associados a essa fam\u00edlia s\u00e3o chamados de hiperpar\u00e2metros.","title":"Hiperpar\u00e2metros"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#distribuicoes-a-priori-improprias","text":"Seja \\xi uma fun\u00e7\u00e3o n\u00e3o negativa tal que \\Omega \u00e9 subconjunto de seu dom\u00ednio. Suponha que \\int \\xi(\\theta) d\\theta = \\infty . Se \\xi(\\theta) \u00e9 priori de \\theta , ela \u00e9 chamada de prioori impr\u00f3pria. Podemos gerar limites de distribui\u00e7\u00f5es, como, por exemplo, a distribui\u00e7\u00e3o uniforme [0,1] com intervalo sendo a reta, agora.","title":"Distribui\u00e7\u00f5es a Priori Impr\u00f3prias"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#estimador-de-bayes","text":"","title":"Estimador de Bayes"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#estimador","text":"Seja X_1, ..., X_n dados observador cuja distribui\u00e7\u00e3o conjunta \u00e9 inndexada pelo par\u00e2metro \\theta . Um estimador do par\u00e2metro \\theta \u00e9 uma fun\u00e7\u00e3o real X_1, ..., X_n \\mapsto \\delta(X_1, ..., X_n) . Se X_i = x_i \u00e9 observado, \\delta(x_1,...,x_n) \u00e9 uma estimativa.","title":"Estimador"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#funcao-de-perda","text":"\u00c9 uma fun\u00e7\u00e3o real L(\\theta, a) onde \\theta \\in \\Omega e a \\in \\mathbb{R} . Essa fun\u00e7\u00e3o procura indicar, para cada escoolha de \\theta , a perda do estat\u00edstico. Seja \\xi(\\theta) priori de \\theta . O valor esperado da perda \u00e9 dado por: E[L(\\theta, a)] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta) d\\theta, \\text{ a priori} E[L(\\theta, a)|x] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta|x) d\\theta, \\text{ a posteriori}","title":"Fun\u00e7\u00e3o de Perda"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#estimador-de-bayes_1","text":"Seja L(\\theta, a) fun\u00e7\u00e3o de perda. Seja \\delta^*(x) o valor de a tal que E[L(\\theta, a)|x] \u00e9 minimizado. Ent\u00e3o \\delta^* \u00e9 o estimador de Bayes de \\theta . E[L(\\theta, \\delta^*(x))|x] = \\min_{a \\in \\mathbb{R}}E[L(\\theta, a)|x]","title":"Estimador de Bayes"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#funcoes-de-perda-exemplos","text":"Erro quadr\u00e1tico: L(\\theta, a) = (\\theta - a)^2 Queremos minimizar E[(\\theta - a)^2|x] \\delta^*(X) = E(\\theta| X) P\u00e1gina 260 (DeGroot) Erro absoluto: L(\\theta, a) = |\\theta - a| Queremos minimizar E[|\\theta - a||x] \\delta^*(X) = \\text{mediana (quartil 0.5)} P\u00e1gina 245 (DeGroot)","title":"Fun\u00e7\u00f5es de Perda: Exemplos"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#estimador-consistente","text":"Uma sequ\u00eancia de estimadores que converge em probabilidade para um valor desconhecido de um par\u00e2metro a ser estimafo \u00e9 chamado de sequ\u00eancia consistente de estimadores. Essa consist\u00eancia fala que em grandes amostras, o estimador estar\u00e1 pr\u00f3ximo o suficiente do valor desconhecido de \\theta . O estimador de Bayes, sob algumas condi\u00e7\u00f5es, forma uma sequ\u00eancia de estimadores consistentes.","title":"Estimador Consistente"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#limitacoes","text":"De acordo com a teoria Bayesiana , esse estimador \u00e9 o \u00fanico coerente que pode ser constru\u00eddo. \u00c9 importante que tenha-se definido uma fun\u00e7\u00e3o de perda e uma distribui\u00e7\u00e3o a priori para os par\u00e2metros. Quando \\theta \u00e9 um vetor, precisamos definit uma priori multivariada, mesmo que n\u00e3o queiramos estimar todos os par\u00e2metros.","title":"Limita\u00e7\u00f5es"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#exemplo-747","text":"Quetelet reportou medidas do peito de 5732 homens militares. Os dados foram retirados desse site . import requests from bs4 import BeautifulSoup import pandas as pd","title":"Exemplo 7.4.7"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#obtendo-os-dados-direto-do-site","text":"Eu uso essas tr\u00eas bibliotecas, onde as duas primeiras s\u00e3o usadas para retirar informa\u00e7\u00e3o do site desejado. Veja que n\u00e3o coloco verifica\u00e7\u00e3o, pois o site tem esse problema. Depois eu coloco numa estrutura chamada DataFrame que \u00e9 basicamente uma tabela onde tem cada item nas linhas e cada caracter\u00edstica nas colunas. website = requests . get ( 'https://www.stat.cmu.edu/StatDat/Datafiles/MilitiamenChests.html' , verify = False ) soup = BeautifulSoup ( website . content ) data = soup . pre . text . strip () . split ( ' \\n ' ) chest_data = { 'Chest' : [], 'Count' : []} for item in data [ 1 :]: co , ch = item . split ( ' \\t ' ) chest_data [ 'Chest' ] . append ( int ( ch )) chest_data [ 'Count' ] . append ( int ( co )) chest_df = pd . DataFrame ( chest_data ) chest_df . head () /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Chest Count 0 33 3 1 34 18 2 35 81 3 36 185 4 37 420 plt . bar ( chest_df [ 'Chest' ], chest_df [ 'Count' ]) mean = sum ( chest_df [ 'Chest' ] * chest_df [ 'Count' ]) / chest_df [ 'Count' ] . sum () plt . vlines ( mean , ymin = 0 , ymax = 1200 , color = 'black' , linestyle = '--' , label = 'M\u00e9dia={:.2f}' . format ( mean )) plt . title ( 'Histograma do tamanho do ppeito de militares escoseses' ) plt . xlabel ( 'Medida de peitorais' ) plt . legend () plt . show () Vamos modelar as medidas do peitoral, como uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal com m\u00e9dia \\theta e vari\u00e2ncia \\sigma^2 , conhecido. Temos que a m\u00e9dia \u00e9 39.83 das amostras. Se \\theta \\sim N(\\mu_0, v_0^2) \u00e9 uma priori para \\theta , podemos calcular o estimador de Bayes a posteriori. Sabemos que a posteriori ser\u00e1 uma normal (conjugada) com m\u00e9dia e vari\u00e2ncia: \\mu_1 = \\frac{\\sigma^2 + 5732\\cdot v_0^2 \\cdot 39.83}{\\sigma^2 + 5732\\cdot v_0^2} v_1^2 = \\frac{\\sigma^2 v_0^2}{\\sigma^2 + 5732\\cdot v_0^2} O estimador de Bayes, segundo a perda quadr\u00e1tica, \u00e9 a m\u00e9dia a posteriori, portanto \\delta(x) = \\mu_1","title":"Obtendo os dados direto do site"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#priori-para-theta","text":"\\mu_0 = 39.83 v_0^2 = 4","title":"Priori para \\theta"},{"location":"infestatistica/FisherInformation/FisherInformation/","text":"Informa\u00e7\u00e3o de Fisher Seja X uma amostra aleat\u00f3ria cuja distribui\u00e7\u00e3o depende de \\theta e tem valores em (a,b) \\subset \\mathbb{R} . Seja f_n(x|\\theta) a pdf conjunta de X . Assuma que S = {x | f(x|\\theta) > 0} \u00e9 o mesmo para todo \\theta . E \\lambda_n(x|\\theta) = \\log f_n(x|\\theta) \u00e9 duas vezes diferenci\u00e1vel em \\theta . A informa\u00e7\u00e3o \u00e9: I_n(\\theta) = E_{\\theta}\\{[\\lambda_n '(X|\\theta)]^2\\} Agora assuma que duas derivadas de \\int_S f_n(x|\\theta)dx com respeito a \\theta podemos inverter a ordem de integra\u00e7\u00e3o e diferencia\u00e7\u00e3o . Ent\u00e3o: I_n(\\theta) = - E_{\\theta}[\\lambda_n ''(X|\\theta)] Teorema I_n(\\theta) = nI(\\theta) Obs.: Estamos tratando da informa\u00e7\u00e3o de Fisher para o caso unidimensional. Para o caso em que temos \\Omega \\subset \\mathbb{R}^k , a informa\u00e7\u00e3o de Fisher ser\u00e1 uma matriz de tamanho k \\times k onde I_{n,i,j} = Cov_{\\theta}\\left[\\frac{\\partial}{\\partial \\theta_i}\\lambda_n'(X|\\theta), \\frac{\\partial}{\\partial \\theta_j}\\lambda_n'(X|\\theta)\\right] import numpy as np from scipy.stats import norm from scipy.misc import derivative from scipy.optimize import curve_fit import matplotlib.pyplot as plt from seaborn import violinplot import inspect Exemplo Construtivo Vamos pensar num caso bem simples: amostra aleat\u00f3ria X_1, ..., X_n \\sim \\text{Normal}(\\mu, \\sigma^2) , onde o par\u00e2metro \\sigma^2 \u00e9 conhecido e \\mu n\u00e3o. De forma direta, poder\u00edamos perguntar qual a Informa\u00e7\u00e3o de Fisher (ou Informa\u00e7\u00e3o Diferencial) da amostra aleat\u00f3ria sobre o par\u00e2metro desconhecido \\mu . Vamos encontrar a distribui\u00e7\u00e3o conjunta: f(x|\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2}\\right] \\begin{split} f_n(x|\\mu) &= \\prod_{i=1}^n f(x_i|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i^2 - 2x_i\\mu + \\mu^2)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n x_i^2 - 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\end{split} Vamos encontrar a verossimilhan\u00e7a: \u00e9 a distribui\u00e7\u00e3o conjunta como fun\u00e7\u00e3o do par\u00e2metro! f_n(x|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] Vamos comparar para \\sigma = 1 e \\sigma = 5 loglikelihood = lambda mu , sigma , x : np . sum ( np . log ([ norm ( loc = mu , scale = sigma ) . pdf ( xi ) for xi in x ]), axis = 0 ) sigmas = [ 1 , 3 , 5 , 10 ] mu_true = 5 mu_range = np . linspace ( 0 , 10 , 1000 ) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Log-verossimilhan\u00e7as da Distribui\u00e7\u00e3o Normal' ) def generate_curves ( sigma , ax , n = 20 , n_times = 50 ): for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) logvalues = loglikelihood ( mu_range , sigma , x ) ax . plot ( mu_range , logvalues , color = 'blue' , alpha = 0.2 ) ax . vlines ( mu_true , ymin = ax . get_ylim ()[ 0 ], ymax = ax . get_ylim ()[ 1 ], linestyle = '--' ) ax . set_title ( r '$\\sigma =$ {}' . format ( sigma )) ax . set_xlabel ( r '$\\mu$' ) generate_curves ( sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_curves ( sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_curves ( sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_curves ( sigmas [ 3 ], ax [ 1 ][ 1 ]) Vamos ver como se comporta derivada. Esse \u00e9 o score: \\lambda '_n(y|\\mu) = \\frac{1}{\\sigma^2}\\left(n\\bar{x}_n - \\mu\\right) score = lambda mu , sigma , x : derivative ( loglikelihood , mu , dx = 1e-5 , args = ( sigma , x )) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Scores da Distribui\u00e7\u00e3o Normal' ) def generate_curves ( sigma , ax , n = 20 , n_times = 50 ): for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) scorevalues = score ( mu_range , sigma , x ) ax . plot ( mu_range , scorevalues , color = 'blue' , alpha = 0.2 ) ax . vlines ( mu_true , ymin = ax . get_ylim ()[ 0 ], ymax = ax . get_ylim ()[ 1 ], linestyle = '--' ) ax . set_title ( r '$\\sigma =$ {}' . format ( sigma )) ax . set_xlabel ( r '$\\mu$' ) ax . set_ylim (( - 10 , 10 )) generate_curves ( sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_curves ( sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_curves ( sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_curves ( sigmas [ 3 ], ax [ 1 ][ 1 ]) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Histogramas dos Scores para mu' ) def generate_histograms ( mu , sigma , ax , n = 15 , n_times = 100 ): scorevalues = [] for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) scorevalues . append ( score ( mu , sigma , x )) violinplot ( scorevalues , ax = ax ) ax . set_title ( r '$\\sigma =$ {}' . format ( sigma )) ax . set_xlabel ( 'score' ) generate_histograms ( 5 , sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_histograms ( 5 , sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_histograms ( 5 , sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_histograms ( 5 , sigmas [ 3 ], ax [ 1 ][ 1 ]) A informa\u00e7\u00e3o de Fisher \u00e9 a Vari\u00e2ncia da fun\u00e7\u00e3o score em X , isto \u00e9: \\begin{split} I_n(\\mu) &= Var(\\lambda '_n(x|p)) = E[(\\lambda '_n(x|p))^2] - E[\\lambda '_n(x|p)]^2\\\\ &= \\frac{1}{\\sigma^4}Var\\left[n\\bar{x}_n - \\mu\\right] \\\\ &= \\frac{n^2}{\\sigma^4}Var(\\bar{x}_n) \\\\ &= \\frac{n^2\\sigma^2}{n\\sigma^4} \\\\ &= \\frac{n}{\\sigma^2} \\end{split} Desigualdade de Cram\u00e9r-Rao Seja X uma amostra aleat\u00f3ria com pdf f(x| \\theta) . Suponha as hip\u00f3teses acima acerca dessa distribui\u00e7\u00e3o. Seja T = r(X) com vari\u00e2ncia finita e m(\\theta) = E_{\\theta}(T) \u00e9 diferenci\u00e1vel. Assim: Var_{\\theta}(T) \\geq \\frac{[m'(\\theta)]^2}{nI(\\theta)} A igualdade vale se, e somente se, existem fun\u00e7\u00f5es u(\\theta) e v(\\theta) que podem depender em \\theta mas n\u00e3o de X tal que: T = u(\\theta)\\lambda_n'(X|\\theta) + v(\\theta) Se T for n\u00e3o enviesado m(\\theta) = \\theta \\implies m'(\\theta) = 1 Exemplo Num\u00e9rico do limite de Cram\u00e9r-Rao Refer\u00eancia Considere um sinal (como uma m\u00fasica) com tr\u00eas par\u00e2metros, amplitude, frequ\u00eancia e fase inicia. Saberemos o n\u00famero de amostras que sera 100Hz com n\u00edvel de ru\u00eddo de 0.1 s = lambda t , a , f , ph : a * np . sin ( 2 * np . pi * f * t + ph ) # fun\u00e7\u00e3o que representa o sinal p0 = [ 2 , 8 , 0 ] # Amplitude, frequ\u00eancia e fase inicial para testar noise = 0.1 T = np . linspace ( 0 , 1 , 100 ) #100 valores entre 0 e 1 igualmente espa\u00e7ados plt . plot ( T , s ( T , * p0 ), '.-k' ) plt . xlabel ( 'Tempo (s)' ) plt . title ( 'Sinal' ) plt . show () Vamos usar inspect para nos ajudar a pegar labels das fun\u00e7\u00f5es, isto \u00e9, os par\u00e2metros necess\u00e1rios das fun\u00e7\u00f5es. Essa biblioteca fornece v\u00e1rias fun\u00e7\u00f5es de ajuda desse tipo. D\u00ea uma olhada. parameters = str ( inspect . signature ( s )) . strip ( '()' ) . replace ( ' ' , '' ) . split ( ',' )[ 1 :] p0dict = dict ( zip ( parameters , p0 )) p0dict {'a': 2, 'f': 8, 'ph': 0} No caso geral, calcular a Matriz de Informa\u00e7\u00e3o de Fisher n\u00e3o \u00e9 trivial. Por isso, vamos calcular para o caso em que as medi\u00e7\u00f5es s\u00e3o de uma amostra com distribui\u00e7\u00e3o multivariada normal, isto \u00e9, \u00e9 uma distribui\u00e7\u00e3o normal, s\u00f3 que em mais dimens\u00f5es, em particular, 441 dimens\u00f5es (n\u00famero de pontos no tempo) Se calcularmos a informa\u00e7\u00e3o de Fisher, podemos ver que: \\mathcal{I}_{mn} = \\frac{1}{\\sigma^2} \\frac{\\partial \\mu^\\mathrm{T}}{\\partial \\theta_m} \\frac{\\partial \\mu}{\\partial \\theta_n} = \\frac{1}{\\sigma^2} \\sum_k \\frac{\\partial \\mu_k}{\\partial \\theta_m} \\frac{\\partial \\mu_k}{\\partial \\theta_n} onde \\theta = [a,f,ph]^T , \\mu = \\mu(\\theta) \u00e9 o vetor m\u00e9dia da normal multivariada e \\sigma^2 \u00e9 a vari\u00e2ncia de cada marginal da normal. N\u00e3o se assuste. Na multivariada, temos uma matriz para indicar as vari\u00e2ncias (ela se chama Matriz de Covari\u00e2ncias, na verdade). O que estou dizendo \u00e9 que ela \u00e9 \\sigma^2 vezes a identidade. \u00c9 bom conhecer essa distribui\u00e7\u00e3o! Por enquando acredite em mim! Ou no Wikipedia . Vou chamar D_{ik} = \\frac{\\partial \\mu_k}{\\partial \\theta_i} # Usamos ** para desempacotar elementos de um dicion\u00e1rio. string = \"a: {a} f: {f} ph: {ph}\" . format ( ** p0dict ) print ( string ) a: 2 f: 8 ph: 0 D = np . zeros (( len ( p0 ), len ( T ))) # para cada par\u00e2metro for i , parameter in enumerate ( parameters ): # para cada ponto no tempo for k , t in enumerate ( T ): func = lambda x : s ( t , ** dict ( p0dict , ** { parameter : x })) # Calculamos a derivada com respeito a x, que nesse caso \u00e9 o valor do parametro D [ i , k ] = derivative ( func , p0dict [ parameter ], dx = 1e-4 ) Veja que o tamanho de D \u00e9 o seguinte: D . shape (3, 100) plt . plot ( T , s ( T , * p0 ), '--k' , lw = 2 , label = 'Sinal' ) for Di , parameter in zip ( D , parameters ): # Estamos acessando Di = linha_i(D) plt . plot ( T , Di , '.-' , label = parameter ) plt . legend () plt . xlabel ( 'Tempo (s)' ) plt . show () O que D_{ik} indica? \u00c9 a derivada da k-\u00e9sima m\u00e9dia com respeito ao i-\u00e9simo par\u00e2metro. Logo indica o quanto o quando a amostra k afeta o par\u00e2metro i . Veja que quando temos picos no seno, teremos pico na amplitude,. Tamb\u00e9m vemos que a fase inicial n\u00e3o tem essa relev\u00e2ncia. Vemos tamb\u00e9m que o sinal se torna mais e mais sens\u00edvel \u00e0 frequ\u00eancia. Assim, podemos calular a informa\u00e7\u00e3o de fisher, usando einsum I = 1 / noise ** 2 * np . einsum ( 'mk,nk' , D , D ) print ( I ) [[ 4.95000000e+03 -5.64643569e+02 -3.43706036e-09] [-5.64643569e+02 2.68635205e+05 6.34601694e+04] [-3.43706036e-09 6.34601694e+04 2.01999999e+04]] Podemos calcular o limite de Cram\u00e9r-Rao para qualquer estimador n\u00e3o enviesado. Nesse caso, veja aqui para mais detalhes. Mas n\u00e3o se incomode com os detalhes, se preferir. iI = np . linalg . inv ( I ) print ( 'Cram\u00e9r-Rao Limite Inferior' ) for parameter , variance in zip ( parameters , iI . diagonal ()): print ( '{}: {:.2g}' . format ( parameter , np . sqrt ( variance ))) Cram\u00e9r-Rao Limite Inferior a: 0.014 f: 0.0038 ph: 0.014 Estimador Eficiente T \u00e9 um estimador eficiente de sua esperan\u00e7a m(\\theta) se, para todo \\theta , vale a igualdade em Cram\u00e9r-Rao. Mas nem sempre vale a igualdade, inclusive conhecemos uma consdi\u00e7\u00e3o necess\u00e1ria e suficiente para isso, que est\u00e1 logo acima. Estimadores n\u00e3o enviesados com vari\u00e2ncia m\u00ednima Suponha que T seja um estimador eficiente de sua esperan\u00e7a m(\\theta) e T_1 outro estimador n\u00e3o enviesado. Ent\u00e3o para todo valor \\theta \\in \\Omega , Var_{\\theta}(T) ser\u00e1 igual ao limite inferior de Cram\u00e9r-Rao e Var_{\\theta}(T_1) ser\u00e1 pelo menos maior ou igual. Portanto Var_{\\theta}(T) \\leq Var_{\\theta}(T_1), \\forall \\theta . Isto \u00e9, um estimado eficiente de m(\\theta) ter\u00e1 menor vari\u00e2ncia. Distribui\u00e7\u00e3o assint\u00f3tica de um estimador eficiente Assuma as hip\u00f3teses do teorema de Cram\u00e9r-Rao. Seja T um estimador eficiente para a sua m\u00e9dia m(\\theta) e m'(\\theta) \\neq 0 . Ent\u00e3o: \\frac{[nI(\\theta)]^{1/2}}{m'(\\theta)}[T - m(\\theta)] \\overset{d}{\\to} N(0,1) Distribui\u00e7\u00e3o assint\u00f3tica do MLE Suponha que obtemos \\hat{\\theta}_n resolvendo a equa\u00e7\u00e3o \\lambda_n'(x|\\theta) = 0 , isto \u00e9, maximizando a log-verossimilhan\u00e7a (MLE). E suponha que \\lambda_n'' e \\lambda_n''' existem e satisfazem certas condi\u00e7\u00f5es de regularidade. Ent\u00e3o [nI(\\theta)]^{1/2}(\\hat{\\theta}_n - \\theta) \\overset{d}{\\to} N(0,1) Como o MLE \u00e9 n\u00e3o enviesado, ent\u00e3o se ele for Eficiente, j\u00e1 sabemos que esse teorema \u00e9 verdade pelo anterior. (se ele \u00e9 n\u00e3o enviesado) Bayesiano Suponha que adotamos uma priori para \\theta com uma pdf diferenci\u00e1vel no intervalo. Sobre condi\u00e7\u00f5es de regularidade similares \u00e0quelas que garantem normalidade assint\u00f3tica para \\hat{\\theta}_n , pode-se mostrar que que a distribui\u00e7\u00e3o a posteriori de \\theta vai se aproximadamente uma normal com m\u00e9dia \\hat{\\theta}_n e vari\u00e2ncia 1/[nI(\\hat{\\theta}_n)] , onde \\hat{\\theta}_n \u00e9 o MLE.","title":"Informa\u00e7\u00e3o de Fisher"},{"location":"infestatistica/FisherInformation/FisherInformation/#informacao-de-fisher","text":"Seja X uma amostra aleat\u00f3ria cuja distribui\u00e7\u00e3o depende de \\theta e tem valores em (a,b) \\subset \\mathbb{R} . Seja f_n(x|\\theta) a pdf conjunta de X . Assuma que S = {x | f(x|\\theta) > 0} \u00e9 o mesmo para todo \\theta . E \\lambda_n(x|\\theta) = \\log f_n(x|\\theta) \u00e9 duas vezes diferenci\u00e1vel em \\theta . A informa\u00e7\u00e3o \u00e9: I_n(\\theta) = E_{\\theta}\\{[\\lambda_n '(X|\\theta)]^2\\} Agora assuma que duas derivadas de \\int_S f_n(x|\\theta)dx com respeito a \\theta podemos inverter a ordem de integra\u00e7\u00e3o e diferencia\u00e7\u00e3o . Ent\u00e3o: I_n(\\theta) = - E_{\\theta}[\\lambda_n ''(X|\\theta)]","title":"Informa\u00e7\u00e3o de Fisher"},{"location":"infestatistica/FisherInformation/FisherInformation/#teorema","text":"I_n(\\theta) = nI(\\theta) Obs.: Estamos tratando da informa\u00e7\u00e3o de Fisher para o caso unidimensional. Para o caso em que temos \\Omega \\subset \\mathbb{R}^k , a informa\u00e7\u00e3o de Fisher ser\u00e1 uma matriz de tamanho k \\times k onde I_{n,i,j} = Cov_{\\theta}\\left[\\frac{\\partial}{\\partial \\theta_i}\\lambda_n'(X|\\theta), \\frac{\\partial}{\\partial \\theta_j}\\lambda_n'(X|\\theta)\\right] import numpy as np from scipy.stats import norm from scipy.misc import derivative from scipy.optimize import curve_fit import matplotlib.pyplot as plt from seaborn import violinplot import inspect","title":"Teorema"},{"location":"infestatistica/FisherInformation/FisherInformation/#exemplo-construtivo","text":"Vamos pensar num caso bem simples: amostra aleat\u00f3ria X_1, ..., X_n \\sim \\text{Normal}(\\mu, \\sigma^2) , onde o par\u00e2metro \\sigma^2 \u00e9 conhecido e \\mu n\u00e3o. De forma direta, poder\u00edamos perguntar qual a Informa\u00e7\u00e3o de Fisher (ou Informa\u00e7\u00e3o Diferencial) da amostra aleat\u00f3ria sobre o par\u00e2metro desconhecido \\mu . Vamos encontrar a distribui\u00e7\u00e3o conjunta: f(x|\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2}\\right] \\begin{split} f_n(x|\\mu) &= \\prod_{i=1}^n f(x_i|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i^2 - 2x_i\\mu + \\mu^2)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n x_i^2 - 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\end{split} Vamos encontrar a verossimilhan\u00e7a: \u00e9 a distribui\u00e7\u00e3o conjunta como fun\u00e7\u00e3o do par\u00e2metro! f_n(x|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] Vamos comparar para \\sigma = 1 e \\sigma = 5 loglikelihood = lambda mu , sigma , x : np . sum ( np . log ([ norm ( loc = mu , scale = sigma ) . pdf ( xi ) for xi in x ]), axis = 0 ) sigmas = [ 1 , 3 , 5 , 10 ] mu_true = 5 mu_range = np . linspace ( 0 , 10 , 1000 ) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Log-verossimilhan\u00e7as da Distribui\u00e7\u00e3o Normal' ) def generate_curves ( sigma , ax , n = 20 , n_times = 50 ): for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) logvalues = loglikelihood ( mu_range , sigma , x ) ax . plot ( mu_range , logvalues , color = 'blue' , alpha = 0.2 ) ax . vlines ( mu_true , ymin = ax . get_ylim ()[ 0 ], ymax = ax . get_ylim ()[ 1 ], linestyle = '--' ) ax . set_title ( r '$\\sigma =$ {}' . format ( sigma )) ax . set_xlabel ( r '$\\mu$' ) generate_curves ( sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_curves ( sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_curves ( sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_curves ( sigmas [ 3 ], ax [ 1 ][ 1 ]) Vamos ver como se comporta derivada. Esse \u00e9 o score: \\lambda '_n(y|\\mu) = \\frac{1}{\\sigma^2}\\left(n\\bar{x}_n - \\mu\\right) score = lambda mu , sigma , x : derivative ( loglikelihood , mu , dx = 1e-5 , args = ( sigma , x )) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Scores da Distribui\u00e7\u00e3o Normal' ) def generate_curves ( sigma , ax , n = 20 , n_times = 50 ): for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) scorevalues = score ( mu_range , sigma , x ) ax . plot ( mu_range , scorevalues , color = 'blue' , alpha = 0.2 ) ax . vlines ( mu_true , ymin = ax . get_ylim ()[ 0 ], ymax = ax . get_ylim ()[ 1 ], linestyle = '--' ) ax . set_title ( r '$\\sigma =$ {}' . format ( sigma )) ax . set_xlabel ( r '$\\mu$' ) ax . set_ylim (( - 10 , 10 )) generate_curves ( sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_curves ( sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_curves ( sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_curves ( sigmas [ 3 ], ax [ 1 ][ 1 ]) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Histogramas dos Scores para mu' ) def generate_histograms ( mu , sigma , ax , n = 15 , n_times = 100 ): scorevalues = [] for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) scorevalues . append ( score ( mu , sigma , x )) violinplot ( scorevalues , ax = ax ) ax . set_title ( r '$\\sigma =$ {}' . format ( sigma )) ax . set_xlabel ( 'score' ) generate_histograms ( 5 , sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_histograms ( 5 , sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_histograms ( 5 , sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_histograms ( 5 , sigmas [ 3 ], ax [ 1 ][ 1 ]) A informa\u00e7\u00e3o de Fisher \u00e9 a Vari\u00e2ncia da fun\u00e7\u00e3o score em X , isto \u00e9: \\begin{split} I_n(\\mu) &= Var(\\lambda '_n(x|p)) = E[(\\lambda '_n(x|p))^2] - E[\\lambda '_n(x|p)]^2\\\\ &= \\frac{1}{\\sigma^4}Var\\left[n\\bar{x}_n - \\mu\\right] \\\\ &= \\frac{n^2}{\\sigma^4}Var(\\bar{x}_n) \\\\ &= \\frac{n^2\\sigma^2}{n\\sigma^4} \\\\ &= \\frac{n}{\\sigma^2} \\end{split}","title":"Exemplo Construtivo"},{"location":"infestatistica/FisherInformation/FisherInformation/#desigualdade-de-cramer-rao","text":"Seja X uma amostra aleat\u00f3ria com pdf f(x| \\theta) . Suponha as hip\u00f3teses acima acerca dessa distribui\u00e7\u00e3o. Seja T = r(X) com vari\u00e2ncia finita e m(\\theta) = E_{\\theta}(T) \u00e9 diferenci\u00e1vel. Assim: Var_{\\theta}(T) \\geq \\frac{[m'(\\theta)]^2}{nI(\\theta)} A igualdade vale se, e somente se, existem fun\u00e7\u00f5es u(\\theta) e v(\\theta) que podem depender em \\theta mas n\u00e3o de X tal que: T = u(\\theta)\\lambda_n'(X|\\theta) + v(\\theta) Se T for n\u00e3o enviesado m(\\theta) = \\theta \\implies m'(\\theta) = 1","title":"Desigualdade de Cram\u00e9r-Rao"},{"location":"infestatistica/FisherInformation/FisherInformation/#exemplo-numerico-do-limite-de-cramer-rao","text":"Refer\u00eancia Considere um sinal (como uma m\u00fasica) com tr\u00eas par\u00e2metros, amplitude, frequ\u00eancia e fase inicia. Saberemos o n\u00famero de amostras que sera 100Hz com n\u00edvel de ru\u00eddo de 0.1 s = lambda t , a , f , ph : a * np . sin ( 2 * np . pi * f * t + ph ) # fun\u00e7\u00e3o que representa o sinal p0 = [ 2 , 8 , 0 ] # Amplitude, frequ\u00eancia e fase inicial para testar noise = 0.1 T = np . linspace ( 0 , 1 , 100 ) #100 valores entre 0 e 1 igualmente espa\u00e7ados plt . plot ( T , s ( T , * p0 ), '.-k' ) plt . xlabel ( 'Tempo (s)' ) plt . title ( 'Sinal' ) plt . show () Vamos usar inspect para nos ajudar a pegar labels das fun\u00e7\u00f5es, isto \u00e9, os par\u00e2metros necess\u00e1rios das fun\u00e7\u00f5es. Essa biblioteca fornece v\u00e1rias fun\u00e7\u00f5es de ajuda desse tipo. D\u00ea uma olhada. parameters = str ( inspect . signature ( s )) . strip ( '()' ) . replace ( ' ' , '' ) . split ( ',' )[ 1 :] p0dict = dict ( zip ( parameters , p0 )) p0dict {'a': 2, 'f': 8, 'ph': 0} No caso geral, calcular a Matriz de Informa\u00e7\u00e3o de Fisher n\u00e3o \u00e9 trivial. Por isso, vamos calcular para o caso em que as medi\u00e7\u00f5es s\u00e3o de uma amostra com distribui\u00e7\u00e3o multivariada normal, isto \u00e9, \u00e9 uma distribui\u00e7\u00e3o normal, s\u00f3 que em mais dimens\u00f5es, em particular, 441 dimens\u00f5es (n\u00famero de pontos no tempo) Se calcularmos a informa\u00e7\u00e3o de Fisher, podemos ver que: \\mathcal{I}_{mn} = \\frac{1}{\\sigma^2} \\frac{\\partial \\mu^\\mathrm{T}}{\\partial \\theta_m} \\frac{\\partial \\mu}{\\partial \\theta_n} = \\frac{1}{\\sigma^2} \\sum_k \\frac{\\partial \\mu_k}{\\partial \\theta_m} \\frac{\\partial \\mu_k}{\\partial \\theta_n} onde \\theta = [a,f,ph]^T , \\mu = \\mu(\\theta) \u00e9 o vetor m\u00e9dia da normal multivariada e \\sigma^2 \u00e9 a vari\u00e2ncia de cada marginal da normal. N\u00e3o se assuste. Na multivariada, temos uma matriz para indicar as vari\u00e2ncias (ela se chama Matriz de Covari\u00e2ncias, na verdade). O que estou dizendo \u00e9 que ela \u00e9 \\sigma^2 vezes a identidade. \u00c9 bom conhecer essa distribui\u00e7\u00e3o! Por enquando acredite em mim! Ou no Wikipedia . Vou chamar D_{ik} = \\frac{\\partial \\mu_k}{\\partial \\theta_i} # Usamos ** para desempacotar elementos de um dicion\u00e1rio. string = \"a: {a} f: {f} ph: {ph}\" . format ( ** p0dict ) print ( string ) a: 2 f: 8 ph: 0 D = np . zeros (( len ( p0 ), len ( T ))) # para cada par\u00e2metro for i , parameter in enumerate ( parameters ): # para cada ponto no tempo for k , t in enumerate ( T ): func = lambda x : s ( t , ** dict ( p0dict , ** { parameter : x })) # Calculamos a derivada com respeito a x, que nesse caso \u00e9 o valor do parametro D [ i , k ] = derivative ( func , p0dict [ parameter ], dx = 1e-4 ) Veja que o tamanho de D \u00e9 o seguinte: D . shape (3, 100) plt . plot ( T , s ( T , * p0 ), '--k' , lw = 2 , label = 'Sinal' ) for Di , parameter in zip ( D , parameters ): # Estamos acessando Di = linha_i(D) plt . plot ( T , Di , '.-' , label = parameter ) plt . legend () plt . xlabel ( 'Tempo (s)' ) plt . show () O que D_{ik} indica? \u00c9 a derivada da k-\u00e9sima m\u00e9dia com respeito ao i-\u00e9simo par\u00e2metro. Logo indica o quanto o quando a amostra k afeta o par\u00e2metro i . Veja que quando temos picos no seno, teremos pico na amplitude,. Tamb\u00e9m vemos que a fase inicial n\u00e3o tem essa relev\u00e2ncia. Vemos tamb\u00e9m que o sinal se torna mais e mais sens\u00edvel \u00e0 frequ\u00eancia. Assim, podemos calular a informa\u00e7\u00e3o de fisher, usando einsum I = 1 / noise ** 2 * np . einsum ( 'mk,nk' , D , D ) print ( I ) [[ 4.95000000e+03 -5.64643569e+02 -3.43706036e-09] [-5.64643569e+02 2.68635205e+05 6.34601694e+04] [-3.43706036e-09 6.34601694e+04 2.01999999e+04]] Podemos calcular o limite de Cram\u00e9r-Rao para qualquer estimador n\u00e3o enviesado. Nesse caso, veja aqui para mais detalhes. Mas n\u00e3o se incomode com os detalhes, se preferir. iI = np . linalg . inv ( I ) print ( 'Cram\u00e9r-Rao Limite Inferior' ) for parameter , variance in zip ( parameters , iI . diagonal ()): print ( '{}: {:.2g}' . format ( parameter , np . sqrt ( variance ))) Cram\u00e9r-Rao Limite Inferior a: 0.014 f: 0.0038 ph: 0.014","title":"Exemplo Num\u00e9rico do limite de Cram\u00e9r-Rao"},{"location":"infestatistica/FisherInformation/FisherInformation/#estimador-eficiente","text":"T \u00e9 um estimador eficiente de sua esperan\u00e7a m(\\theta) se, para todo \\theta , vale a igualdade em Cram\u00e9r-Rao. Mas nem sempre vale a igualdade, inclusive conhecemos uma consdi\u00e7\u00e3o necess\u00e1ria e suficiente para isso, que est\u00e1 logo acima.","title":"Estimador Eficiente"},{"location":"infestatistica/FisherInformation/FisherInformation/#estimadores-nao-enviesados-com-variancia-minima","text":"Suponha que T seja um estimador eficiente de sua esperan\u00e7a m(\\theta) e T_1 outro estimador n\u00e3o enviesado. Ent\u00e3o para todo valor \\theta \\in \\Omega , Var_{\\theta}(T) ser\u00e1 igual ao limite inferior de Cram\u00e9r-Rao e Var_{\\theta}(T_1) ser\u00e1 pelo menos maior ou igual. Portanto Var_{\\theta}(T) \\leq Var_{\\theta}(T_1), \\forall \\theta . Isto \u00e9, um estimado eficiente de m(\\theta) ter\u00e1 menor vari\u00e2ncia.","title":"Estimadores n\u00e3o enviesados com vari\u00e2ncia m\u00ednima"},{"location":"infestatistica/FisherInformation/FisherInformation/#distribuicao-assintotica-de-um-estimador-eficiente","text":"Assuma as hip\u00f3teses do teorema de Cram\u00e9r-Rao. Seja T um estimador eficiente para a sua m\u00e9dia m(\\theta) e m'(\\theta) \\neq 0 . Ent\u00e3o: \\frac{[nI(\\theta)]^{1/2}}{m'(\\theta)}[T - m(\\theta)] \\overset{d}{\\to} N(0,1)","title":"Distribui\u00e7\u00e3o assint\u00f3tica de um estimador eficiente"},{"location":"infestatistica/FisherInformation/FisherInformation/#distribuicao-assintotica-do-mle","text":"Suponha que obtemos \\hat{\\theta}_n resolvendo a equa\u00e7\u00e3o \\lambda_n'(x|\\theta) = 0 , isto \u00e9, maximizando a log-verossimilhan\u00e7a (MLE). E suponha que \\lambda_n'' e \\lambda_n''' existem e satisfazem certas condi\u00e7\u00f5es de regularidade. Ent\u00e3o [nI(\\theta)]^{1/2}(\\hat{\\theta}_n - \\theta) \\overset{d}{\\to} N(0,1) Como o MLE \u00e9 n\u00e3o enviesado, ent\u00e3o se ele for Eficiente, j\u00e1 sabemos que esse teorema \u00e9 verdade pelo anterior. (se ele \u00e9 n\u00e3o enviesado)","title":"Distribui\u00e7\u00e3o assint\u00f3tica do MLE"},{"location":"infestatistica/FisherInformation/FisherInformation/#bayesiano","text":"Suponha que adotamos uma priori para \\theta com uma pdf diferenci\u00e1vel no intervalo. Sobre condi\u00e7\u00f5es de regularidade similares \u00e0quelas que garantem normalidade assint\u00f3tica para \\hat{\\theta}_n , pode-se mostrar que que a distribui\u00e7\u00e3o a posteriori de \\theta vai se aproximadamente uma normal com m\u00e9dia \\hat{\\theta}_n e vari\u00e2ncia 1/[nI(\\hat{\\theta}_n)] , onde \\hat{\\theta}_n \u00e9 o MLE.","title":"Bayesiano"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/","text":"Grandes Amostras Desigualdade de Markov P(X \\geq t) \\leq \\frac{E[X^n]}{t^n}, ~dado~que~P(X \\geq 0) = 1, t > 0 Desigualdade de Chebyshev Seja X uma vari\u00e1vel aleat\u00f3ria em que o segundo momento \u00e9 finito. Ent\u00e3o, \\forall t > 0 . P(|X - E[X]| \\geq t) \\leq \\frac{Var[X]}{t^2} Propriedades Importantes X_1, ..., X_n amostra aleat\u00f3ria (por defini\u00e7\u00e3o mesma distribui\u00e7\u00e3o e independentes), com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 . Ent\u00e3o E[\\bar{X_n}] = \\mu e Var[\\bar{X_n}] = \\sigma^2/n . Importando bibliotecas import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set () Exemplo 6.2.2 Um engenheiro ambiental acredita que existam dois contaminantes na \u00e1gua: ars\u00eanico e chumbo. Suponha que ambas s\u00e3o vari\u00e1veis aleat\u00f3rias independentes X e Y , medidas na mesma unidade. O engenheiro est\u00e1 interessado em saber a propor\u00e7\u00e3o de contamina\u00e7\u00e3o por chumbo em m\u00e9dia, isto \u00e9, E[R] = E[Y/(X + Y)] . Como nem sempre conhecemos a distribui\u00e7\u00e3o de R, podemos aproximar o valor esperado atrav\u00e9s de uma m\u00e9dia amostral de R , atrav\u00e9s de observa\u00e7\u00f5es (X_1, Y_1), ..., (X_n, Y_n) . Usando a desigualdade de Chebyshev, (tente ver que Var[R] \\leq 1 ). P(|\\bar{R_n} - E[R]| \\geq \\epsilon) \\leq \\frac{1}{n\\epsilon^2} # Usando apenas Chebyshev: epsilon = 0.0005 prob = 0.95 # probabilidade m\u00ednima de que a diferen\u00e7a entre a # m\u00e9dia amostral e o valor esperado seja menor do que epsilon def get_number_simulations ( epsilon , prob ): assert prob >= 0 assert prob <= 1 # Queremos que P <= 1 - prob-> 1/(n*eps**2) <= 1 - prob min_n = 1 / (( 1 - prob ) * ( epsilon ** 2 )) min_n = np . ceil ( min_n ) print ( '----------------------------------------------------------------------' ) print ( 'In order to have the sample mean at least {} close with probability {}: ' . format ( epsilon , prob )) print ( 'The minimum number of simulations are: {}' . format ( int ( min_n ))) print ( '----------------------------------------------------------------------' ) return min_n def get_epsilon ( prob , n ): assert prob >= 0 assert prob <= 1 # Queremos encontrar epsilon para que 1/(n*eps**2) = 1 - prob eps = np . sqrt ( 1 / ( n * ( 1 - prob ))) return eps _ = get_number_simulations ( epsilon , prob ) ---------------------------------------------------------------------- In order to have the sample mean at least 0.0005 close with probability 0.95: The minimum number of simulations are: 80000000 ---------------------------------------------------------------------- # Testando com distribui\u00e7\u00e3o uniforme (X e Y tem distribui\u00e7\u00f5es uniformes) # Nesse caso, podemos provar que E[R] = 0.5 probs = [ 0.6 , 0.75 , 0.9 , 0.95 , 0.99 ] n_range = np . array ([ j * 10 ** i for i in [ 2 , 3 , 4 , 5 , 6 , 7 ] for j in [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]]) E_R = [] for n in n_range : X = np . random . uniform ( 0 , 1 , size = int ( n )) #np = numpy Y = np . random . uniform ( 0 , 1 , size = int ( n )) R = Y / ( X + Y ) E_R . append ( np . mean ( R )) # E[R] = 0.5 chebyshev_interval = np . empty ( shape = ( len ( probs ), len ( n_range ))) for i , prob in enumerate ( probs ): chebyshev_interval [ i , :] = get_epsilon ( prob , n_range ) # Plotando fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) for i in [ 0 , 1 ]: ax [ i ] . plot ( n_range , E_R , color = 'darkred' ) ax [ i ] . hlines ( 0.5 , xmin = min ( n_range ), xmax = max ( n_range ), linestyle = '--' , alpha = 0.4 , color = 'black' ) ax [ i ] . set_xscale ( 'log' ) colors = [ 'black' , 'red' , 'green' , 'blue' , 'pink' ] for i in range ( len ( probs )): ax [ 1 ] . fill_between ( x = n_range , y1 = 0.5 + chebyshev_interval [ i ,:], y2 = 0.5 - chebyshev_interval [ i ,:], color = colors [ i ], alpha = 0.3 + 0.5 * ( len ( probs ) - i ) / ( len ( probs )), label = probs [ i ]) ax [ 1 ] . legend () ax [ 0 ] . set_title ( 'Different mean samples' , fontsize = 15 ) ax [ 1 ] . set_title ( 'Chebyshev bound with prob' , fontsize = 15 ) plt . show () Lei dos Grandes N\u00fameros Converg\u00eancia em Probabilidade \\forall \\epsilon > 0, \\lim_{n\\to\\infty} P[|Z_n - b| < \\epsilon] = 1 \\iff Z_n \\overset{p}{\\to} b Converg\u00eancia quase certa (Implica a anterior) P[\\lim_{n_\\to\\infty} Z_n = b] = 1 Vers\u00e3o Fraca X_1, \\dots, X_n amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o com m\u00e9dia \\mu e vari\u00e2ncia vinita. Se \\bar{X}_n \u00e9 a m\u00e9dia amostral. Ent\u00e3o \\bar{X}_n \\overset{p}{\\to} \\mu . Vers\u00e3o Forte P[\\lim_{n\\to\\infty} \\bar{X}_n = \\mu] = 1 Histogramas S\u00e3o usados para aproximar uma fun\u00e7\u00e3o de densidade de probabilidade de forma discreta. Seja X_1, X_2, \\dots vari\u00e1veis aleat\u00f3rias iid. Seja c_1 < c_2 constantes. Seja Y_i uma indicadora para c_1\\leq X_i < c_2 . Ent\u00e3o \\bar{Y}_n (propor\u00e7\u00e3o de valores X_1, ..., X_n no intervalo [c_1, c_2) e \\bar{Y}_n \\overset{p}{\\to} P[c_1 \\leq X_1 < c_2] . # Exemplo 6.2.4 lamda = 0.5 # N\u00e3o posso usar lambda beta = 1 / lamda # Numpy usa esse par\u00e2metro t = np . arange ( 0.0001 , 15 , 0.01 ) X_true = lamda * np . exp ( - lamda * t ) fig , ax = plt . subplots ( figsize = ( 14 , 5 )) for n in [ 1 , 10 , 100 , 1000 , 10000 ]: X = np . random . exponential ( scale = beta , size = n ) sns . distplot ( X , ax = ax , kde = False , norm_hist = True , label = 'n=' + str ( n )) # area = 1 sns . lineplot ( t , X_true , ax = ax , lw = 3 ) ax . set_title ( 'Histograma da Distribui\u00e7\u00e3o Exponencial' ) ax . legend () plt . show () Teorema Central do Limite Se as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n formam uma amostra aleat\u00f3ria de tamanho n para uma dada distribui\u00e7\u00e3o de m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 finita, ent\u00e3o para cada n\u00famero x , lim_{n\\to\\infty} P[\\frac{\\bar{X}_n - \\mu}{\\sigma/n^{1/2}} \\leq x] = \\Phi(x), onde \\Phi \u00e9 a fun\u00e7\u00e3o de densidade acumulada da distribui\u00e7\u00e3o normal!!! coffee_df = pd . read_csv ( '../data/CoffeeAndCode.csv' ) display ( coffee_df . head ()) display ( coffee_df . shape ) display ( coffee_df . describe ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay CoffeeTime CodingWithoutCoffee CoffeeType CoffeeSolveBugs Gender Country AgeRange 0 8 2 Before coding Yes Caff\u00e8 latte Sometimes Female Lebanon 18 to 29 1 3 2 Before coding Yes Americano Yes Female Lebanon 30 to 39 2 5 3 While coding No Nescafe Yes Female Lebanon 18 to 29 3 8 2 Before coding No Nescafe Yes Male Lebanon NaN 4 10 3 While coding Sometimes Turkish No Male Lebanon 18 to 29 (100, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay count 100.000000 100.000000 mean 6.410000 2.890000 std 2.644205 1.613673 min 1.000000 1.000000 25% 4.000000 2.000000 50% 7.000000 2.500000 75% 8.000000 4.000000 max 10.000000 8.000000 # Plotting fig , ax = plt . subplots ( figsize = ( 7 , 5 )) sns . distplot ( coffee_df . CoffeeCupsPerDay , ax = ax ) ax . vlines ( coffee_df . CoffeeCupsPerDay . mean (), ymin = 0 , ymax = 1 , linestyle = '--' , color = 'black' , label = 'black' ) ax . annotate ( 'M\u00e9dia:' + str ( coffee_df . CoffeeCupsPerDay . mean ()), ( coffee_df . CoffeeCupsPerDay . mean () + 0.5 , 0.4 )) ax . set_title ( 'Histrograma de copos de caf\u00e9 bebidos por programadores' ) ax . set_ylabel ( 'Frequ\u00eancia' ) ax . set_ylim (( 0 , 0.45 )) plt . show () # Generating sample means samples = [ 10 , 50 , 150 , 300 , 500 , 1000 ] n_experiments = 500 experiments_coffe_cups = np . empty (( n_experiments , len ( samples ))) for j , sample_size in enumerate ( samples ): sample = coffee_df . CoffeeCupsPerDay . sample ( n = sample_size * n_experiments , replace = True ) matrix = np . array ( sample ) . reshape (( n_experiments , sample_size )) experiments_coffe_cups [:, j ] = matrix . mean ( axis = 1 ) experiments_coffe_cups_df = pd . DataFrame ( experiments_coffe_cups , columns = samples ) fig , ax = plt . subplots ( 2 , 3 , figsize = ( 20 , 10 )) for index , column in enumerate ( experiments_coffe_cups_df . columns ): i = int ( index / 3 ) j = index % 3 sns . distplot ( experiments_coffe_cups_df [ column ], ax = ax [ i ][ j ]) ax [ i ][ j ] . set_title ( 'M\u00e9dia amostral com {} amostras' . format ( column )) ax [ i ][ j ] . set_ylabel ( 'Frequ\u00eancia' ) fig . suptitle ( 'Histogramas com diferentes n\u00fameros de amostras' ) plt . show () M\u00e9todo Delta Seja Y_1, Y_2, \\dots uma sequ\u00eancia de v.a. e F uma fun\u00e7\u00e3o de densidade acumulada cont\u00ednua. Sejam \\theta \\in \\mathbb{R} e \\{a_n\\}_{n\\in\\mathbb{N}} que tende ao \\infty . Suponha que a_n(Y_n - \\theta) converge para F . Seja \\alpha uma fun\u00e7\u00e3o com derivada cont\u00ednua, tal que \\alpha '(\\theta) \\neq 0 . Ent\u00e3o a_n[\\alpha(Y_n) - \\alpha(\\theta)]/\\alpha '(\\theta) converge para a distribui\u00e7\u00e3o F . Teorema de Slutsky \\begin{align*} {X}^{(n)}& \\overset{d}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{d}{\\to} X,\\\\ {X}^{(n)}& \\overset{p}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{p}{\\to} X,\\\\ {X}^{(n)}& \\overset{as}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{as}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{as}{\\to} X. \\end{align*} Corol\u00e1rio Se f \u00e9 uma fun\u00e7\u00e3o cont\u00ednua: {X}^{(n)}\\overset{d}{\\to} X \\quad \\text{ e }\\quad {Y}^{(n)}\\overset{p}{\\to} c\\quad \\text{implica}\\quad f ({X}^{(n)},{Y}^{(n)}) \\overset{d}{\\to} f(X,c). Aproxima\u00e7\u00e3o de Taylor e M\u00e9todo Delta Refer\u00eancia de Probabilidade","title":"Grandes Amostras"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#grandes-amostras","text":"","title":"Grandes Amostras"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#desigualdade-de-markov","text":"P(X \\geq t) \\leq \\frac{E[X^n]}{t^n}, ~dado~que~P(X \\geq 0) = 1, t > 0","title":"Desigualdade de Markov"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#desigualdade-de-chebyshev","text":"Seja X uma vari\u00e1vel aleat\u00f3ria em que o segundo momento \u00e9 finito. Ent\u00e3o, \\forall t > 0 . P(|X - E[X]| \\geq t) \\leq \\frac{Var[X]}{t^2}","title":"Desigualdade de Chebyshev"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#propriedades-importantes","text":"X_1, ..., X_n amostra aleat\u00f3ria (por defini\u00e7\u00e3o mesma distribui\u00e7\u00e3o e independentes), com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 . Ent\u00e3o E[\\bar{X_n}] = \\mu e Var[\\bar{X_n}] = \\sigma^2/n .","title":"Propriedades Importantes"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#importando-bibliotecas","text":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set ()","title":"Importando bibliotecas"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#exemplo-622","text":"Um engenheiro ambiental acredita que existam dois contaminantes na \u00e1gua: ars\u00eanico e chumbo. Suponha que ambas s\u00e3o vari\u00e1veis aleat\u00f3rias independentes X e Y , medidas na mesma unidade. O engenheiro est\u00e1 interessado em saber a propor\u00e7\u00e3o de contamina\u00e7\u00e3o por chumbo em m\u00e9dia, isto \u00e9, E[R] = E[Y/(X + Y)] . Como nem sempre conhecemos a distribui\u00e7\u00e3o de R, podemos aproximar o valor esperado atrav\u00e9s de uma m\u00e9dia amostral de R , atrav\u00e9s de observa\u00e7\u00f5es (X_1, Y_1), ..., (X_n, Y_n) . Usando a desigualdade de Chebyshev, (tente ver que Var[R] \\leq 1 ). P(|\\bar{R_n} - E[R]| \\geq \\epsilon) \\leq \\frac{1}{n\\epsilon^2} # Usando apenas Chebyshev: epsilon = 0.0005 prob = 0.95 # probabilidade m\u00ednima de que a diferen\u00e7a entre a # m\u00e9dia amostral e o valor esperado seja menor do que epsilon def get_number_simulations ( epsilon , prob ): assert prob >= 0 assert prob <= 1 # Queremos que P <= 1 - prob-> 1/(n*eps**2) <= 1 - prob min_n = 1 / (( 1 - prob ) * ( epsilon ** 2 )) min_n = np . ceil ( min_n ) print ( '----------------------------------------------------------------------' ) print ( 'In order to have the sample mean at least {} close with probability {}: ' . format ( epsilon , prob )) print ( 'The minimum number of simulations are: {}' . format ( int ( min_n ))) print ( '----------------------------------------------------------------------' ) return min_n def get_epsilon ( prob , n ): assert prob >= 0 assert prob <= 1 # Queremos encontrar epsilon para que 1/(n*eps**2) = 1 - prob eps = np . sqrt ( 1 / ( n * ( 1 - prob ))) return eps _ = get_number_simulations ( epsilon , prob ) ---------------------------------------------------------------------- In order to have the sample mean at least 0.0005 close with probability 0.95: The minimum number of simulations are: 80000000 ---------------------------------------------------------------------- # Testando com distribui\u00e7\u00e3o uniforme (X e Y tem distribui\u00e7\u00f5es uniformes) # Nesse caso, podemos provar que E[R] = 0.5 probs = [ 0.6 , 0.75 , 0.9 , 0.95 , 0.99 ] n_range = np . array ([ j * 10 ** i for i in [ 2 , 3 , 4 , 5 , 6 , 7 ] for j in [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]]) E_R = [] for n in n_range : X = np . random . uniform ( 0 , 1 , size = int ( n )) #np = numpy Y = np . random . uniform ( 0 , 1 , size = int ( n )) R = Y / ( X + Y ) E_R . append ( np . mean ( R )) # E[R] = 0.5 chebyshev_interval = np . empty ( shape = ( len ( probs ), len ( n_range ))) for i , prob in enumerate ( probs ): chebyshev_interval [ i , :] = get_epsilon ( prob , n_range ) # Plotando fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) for i in [ 0 , 1 ]: ax [ i ] . plot ( n_range , E_R , color = 'darkred' ) ax [ i ] . hlines ( 0.5 , xmin = min ( n_range ), xmax = max ( n_range ), linestyle = '--' , alpha = 0.4 , color = 'black' ) ax [ i ] . set_xscale ( 'log' ) colors = [ 'black' , 'red' , 'green' , 'blue' , 'pink' ] for i in range ( len ( probs )): ax [ 1 ] . fill_between ( x = n_range , y1 = 0.5 + chebyshev_interval [ i ,:], y2 = 0.5 - chebyshev_interval [ i ,:], color = colors [ i ], alpha = 0.3 + 0.5 * ( len ( probs ) - i ) / ( len ( probs )), label = probs [ i ]) ax [ 1 ] . legend () ax [ 0 ] . set_title ( 'Different mean samples' , fontsize = 15 ) ax [ 1 ] . set_title ( 'Chebyshev bound with prob' , fontsize = 15 ) plt . show ()","title":"Exemplo 6.2.2"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#lei-dos-grandes-numeros","text":"","title":"Lei dos Grandes N\u00fameros"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#convergencia-em-probabilidade","text":"\\forall \\epsilon > 0, \\lim_{n\\to\\infty} P[|Z_n - b| < \\epsilon] = 1 \\iff Z_n \\overset{p}{\\to} b","title":"Converg\u00eancia em Probabilidade"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#convergencia-quase-certa-implica-a-anterior","text":"P[\\lim_{n_\\to\\infty} Z_n = b] = 1","title":"Converg\u00eancia quase certa (Implica a anterior)"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#versao-fraca","text":"X_1, \\dots, X_n amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o com m\u00e9dia \\mu e vari\u00e2ncia vinita. Se \\bar{X}_n \u00e9 a m\u00e9dia amostral. Ent\u00e3o \\bar{X}_n \\overset{p}{\\to} \\mu .","title":"Vers\u00e3o Fraca"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#versao-forte","text":"P[\\lim_{n\\to\\infty} \\bar{X}_n = \\mu] = 1","title":"Vers\u00e3o Forte"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#histogramas","text":"S\u00e3o usados para aproximar uma fun\u00e7\u00e3o de densidade de probabilidade de forma discreta. Seja X_1, X_2, \\dots vari\u00e1veis aleat\u00f3rias iid. Seja c_1 < c_2 constantes. Seja Y_i uma indicadora para c_1\\leq X_i < c_2 . Ent\u00e3o \\bar{Y}_n (propor\u00e7\u00e3o de valores X_1, ..., X_n no intervalo [c_1, c_2) e \\bar{Y}_n \\overset{p}{\\to} P[c_1 \\leq X_1 < c_2] . # Exemplo 6.2.4 lamda = 0.5 # N\u00e3o posso usar lambda beta = 1 / lamda # Numpy usa esse par\u00e2metro t = np . arange ( 0.0001 , 15 , 0.01 ) X_true = lamda * np . exp ( - lamda * t ) fig , ax = plt . subplots ( figsize = ( 14 , 5 )) for n in [ 1 , 10 , 100 , 1000 , 10000 ]: X = np . random . exponential ( scale = beta , size = n ) sns . distplot ( X , ax = ax , kde = False , norm_hist = True , label = 'n=' + str ( n )) # area = 1 sns . lineplot ( t , X_true , ax = ax , lw = 3 ) ax . set_title ( 'Histograma da Distribui\u00e7\u00e3o Exponencial' ) ax . legend () plt . show ()","title":"Histogramas"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#teorema-central-do-limite","text":"Se as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n formam uma amostra aleat\u00f3ria de tamanho n para uma dada distribui\u00e7\u00e3o de m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 finita, ent\u00e3o para cada n\u00famero x , lim_{n\\to\\infty} P[\\frac{\\bar{X}_n - \\mu}{\\sigma/n^{1/2}} \\leq x] = \\Phi(x), onde \\Phi \u00e9 a fun\u00e7\u00e3o de densidade acumulada da distribui\u00e7\u00e3o normal!!! coffee_df = pd . read_csv ( '../data/CoffeeAndCode.csv' ) display ( coffee_df . head ()) display ( coffee_df . shape ) display ( coffee_df . describe ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay CoffeeTime CodingWithoutCoffee CoffeeType CoffeeSolveBugs Gender Country AgeRange 0 8 2 Before coding Yes Caff\u00e8 latte Sometimes Female Lebanon 18 to 29 1 3 2 Before coding Yes Americano Yes Female Lebanon 30 to 39 2 5 3 While coding No Nescafe Yes Female Lebanon 18 to 29 3 8 2 Before coding No Nescafe Yes Male Lebanon NaN 4 10 3 While coding Sometimes Turkish No Male Lebanon 18 to 29 (100, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay count 100.000000 100.000000 mean 6.410000 2.890000 std 2.644205 1.613673 min 1.000000 1.000000 25% 4.000000 2.000000 50% 7.000000 2.500000 75% 8.000000 4.000000 max 10.000000 8.000000 # Plotting fig , ax = plt . subplots ( figsize = ( 7 , 5 )) sns . distplot ( coffee_df . CoffeeCupsPerDay , ax = ax ) ax . vlines ( coffee_df . CoffeeCupsPerDay . mean (), ymin = 0 , ymax = 1 , linestyle = '--' , color = 'black' , label = 'black' ) ax . annotate ( 'M\u00e9dia:' + str ( coffee_df . CoffeeCupsPerDay . mean ()), ( coffee_df . CoffeeCupsPerDay . mean () + 0.5 , 0.4 )) ax . set_title ( 'Histrograma de copos de caf\u00e9 bebidos por programadores' ) ax . set_ylabel ( 'Frequ\u00eancia' ) ax . set_ylim (( 0 , 0.45 )) plt . show () # Generating sample means samples = [ 10 , 50 , 150 , 300 , 500 , 1000 ] n_experiments = 500 experiments_coffe_cups = np . empty (( n_experiments , len ( samples ))) for j , sample_size in enumerate ( samples ): sample = coffee_df . CoffeeCupsPerDay . sample ( n = sample_size * n_experiments , replace = True ) matrix = np . array ( sample ) . reshape (( n_experiments , sample_size )) experiments_coffe_cups [:, j ] = matrix . mean ( axis = 1 ) experiments_coffe_cups_df = pd . DataFrame ( experiments_coffe_cups , columns = samples ) fig , ax = plt . subplots ( 2 , 3 , figsize = ( 20 , 10 )) for index , column in enumerate ( experiments_coffe_cups_df . columns ): i = int ( index / 3 ) j = index % 3 sns . distplot ( experiments_coffe_cups_df [ column ], ax = ax [ i ][ j ]) ax [ i ][ j ] . set_title ( 'M\u00e9dia amostral com {} amostras' . format ( column )) ax [ i ][ j ] . set_ylabel ( 'Frequ\u00eancia' ) fig . suptitle ( 'Histogramas com diferentes n\u00fameros de amostras' ) plt . show ()","title":"Teorema Central do Limite"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#metodo-delta","text":"Seja Y_1, Y_2, \\dots uma sequ\u00eancia de v.a. e F uma fun\u00e7\u00e3o de densidade acumulada cont\u00ednua. Sejam \\theta \\in \\mathbb{R} e \\{a_n\\}_{n\\in\\mathbb{N}} que tende ao \\infty . Suponha que a_n(Y_n - \\theta) converge para F . Seja \\alpha uma fun\u00e7\u00e3o com derivada cont\u00ednua, tal que \\alpha '(\\theta) \\neq 0 . Ent\u00e3o a_n[\\alpha(Y_n) - \\alpha(\\theta)]/\\alpha '(\\theta) converge para a distribui\u00e7\u00e3o F .","title":"M\u00e9todo Delta"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#teorema-de-slutsky","text":"\\begin{align*} {X}^{(n)}& \\overset{d}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{d}{\\to} X,\\\\ {X}^{(n)}& \\overset{p}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{p}{\\to} X,\\\\ {X}^{(n)}& \\overset{as}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{as}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{as}{\\to} X. \\end{align*}","title":"Teorema de Slutsky"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#corolario","text":"Se f \u00e9 uma fun\u00e7\u00e3o cont\u00ednua: {X}^{(n)}\\overset{d}{\\to} X \\quad \\text{ e }\\quad {Y}^{(n)}\\overset{p}{\\to} c\\quad \\text{implica}\\quad f ({X}^{(n)},{Y}^{(n)}) \\overset{d}{\\to} f(X,c). Aproxima\u00e7\u00e3o de Taylor e M\u00e9todo Delta Refer\u00eancia de Probabilidade","title":"Corol\u00e1rio"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/","text":"Estimador de M\u00e1xima Verossimilhan\u00e7a Introdu\u00e7\u00e3o \"Tradicionalmente a infer\u00eancia estat\u00edstica sobre a m\u00e9dia de uma popula\u00e7\u00e3o se apoia no Teorema Central do Limite para construir Intervalos de Confian\u00e7a ou testar hip\u00f3teses sobre o valor do par\u00e2metro. Esta abordagem da estat\u00edstica tradicional pode ser extendida para infer\u00eancias a respeito de qualquer par\u00e2metro, n\u00e3o s\u00f3 a m\u00e9dia. Da mesma forma que no caso da m\u00e9dia populacional se usa a distribui\u00e7\u00e3o t-Student ou a distribui\u00e7\u00e3o Normal Padr\u00e3o , no caso de outros par\u00e2metros se utiliza outras distribui\u00e7\u00f5es amostrais. Essas distribui\u00e7\u00f5es s\u00e3o chamadas amostrais porque representam o comportamento das estimativas baseado na repeti\u00e7\u00e3o incont\u00e1vel do processo de amostragem . Na pr\u00e1tica cient\u00edfica, no entanto, sempre se realiza uma \u00fanica amostragem , o que resulta em uma \u00fanica amostra. Assim, o conceito de distribui\u00e7\u00e3o amostral \u00e9 at\u00e9 certo ponto artificial, pois em pesquisa cient\u00edfica n\u00e3o raciocinamos em termos de repeti\u00e7\u00f5es incont\u00e1veis de experimentos ou processos de observa\u00e7\u00e3o . O resultado disto \u00e9 que o conceito de teste estat\u00edstico de hip\u00f3tese e de intervalo de confian\u00e7a s\u00e3o frequentemente mal compreendidos. O desenvolvimento da infer\u00eancia estat\u00edstica a partir do conceito de verossimilhan\u00e7a tem sido utilizado como uma alternativa \u00e0 abordagem estat\u00edstica frequentista e, segundo alguns autores (como por exemplo Royall, 1997), \u00e9 mais coerente com a pr\u00e1tica cient\u00edfica .\" (Batista, 2009) Site de Refer\u00eancia Fun\u00e7\u00e3o Verossimilhan\u00e7a Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x) Estimador de M\u00e1xima Verossimilhan\u00e7a (MLE) Para cada observa\u00e7\u00e3o x , seja \\delta(x) um valor de \\theta \\in \\Omega tal que a fun\u00e7\u00e3o de verossimilhna\u00e7a seja m\u00e1xima . Defina \\hat{\\theta} = \\delta(X) o estimador. \u00c9 importante observar que o m\u00e1ximo dessa fun\u00e7\u00e3o pode n\u00e3o estar em um ponto de \\Omega . Nesse caso, MLE n\u00e3o existe. Ele pode n\u00e3o estar unicamente definido, tamb\u00e9m. Limita\u00e7\u00f5es N\u00e3o exist\u00eancia em todos os casos, isso depende muito da fun\u00e7\u00e3o e do espa\u00e7o dos par\u00e2metros. N\u00e3o unicidade em todos os casos. N\u00e3o podemos interpretar MLE como o par\u00e2metro mais prov\u00e1vel, pois ter\u00edamos que ter um espa\u00e7o de probabilidade associado ao par\u00e2metro, o que n\u00e3o \u00e9 dado. Implementa\u00e7\u00e3o Como refer\u00eancia, estou utilizando este site . # importando bibliotecas import numpy as np , pandas as pd from matplotlib import pyplot as plt import seaborn as sns from scipy.optimize import minimize import scipy.stats as stats import pymc3 as pm3 import numdifftools as ndt import statsmodels.api as sm from statsmodels.base.model import GenericLikelihoodModel % matplotlib inline # Gerando os dados N = 100 x = np . linspace ( 0 , 20 , N ) # gerando lista igualmente espa\u00e7ada beta1 = 3 beta0 = 0 sigma = 5 error = np . random . normal ( 0 , sigma , size = N ) y = beta1 * x + beta0 + error data = pd . DataFrame ({ 'y' : y , 'x' : x }) data [ 'constant' ] = 1 sns . regplot ( 'x' , 'y' , data = data ) # Essa reta \u00e9 uma estimativa dos dados feito por seaborn plt . title ( 'Dados' ) plt . show () Y = \\beta_1 x + \\beta_0 + e Nesse exemplo, o nosso problema ser\u00e1 estimar a m\u00e9dia. Observe que os dados tem um comportamento linear. Sem nos concentrarmos muito na modelagem e os problemas que ela pode trazer, eu vou j\u00e1 supor que temos um problema de Regress\u00e3o Linear , onde os dados Y \\sim N(\\mu, \\sigma^2) , onde \\sigma^2 \u00e9 a vari\u00e2ncia do erro no processo, e \\mu = \\beta_0 + \\beta_1 x , isto \u00e9, depende de x, nesse caso. Essa \u00e9 uma dificuldade, as contas ficam mais dif\u00edceis e, por isso, vamos usar asrtif\u00edcios computacionais. Vamos supor que a vari\u00e2ncia \u00e9 conhecida . Al\u00e9m disso, vamos supor que temos uma amostra aleat\u00f3ria Y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2) Temos que a verossimilhan\u00e7a \u00e9 produto das pdfs(distribui\u00e7\u00e3o de densidade de probabilidade). Para otimizar podemos, entretanto, obter a soma dos logaritmos das pdfs . E por fim, vamos resolver um problema de minimizar o negativo desse valor. Veja que \u00e9 equivalente a maximixar a soma!! # Fun\u00e7\u00e3o de verossimilhan\u00e7a. Chamamos de Fun\u00e7\u00e3o de Perda def MLE ( params ): # Fun\u00e7\u00e3o Perda: - log-verossimilhan\u00e7a beta0 , beta1 = params [ 0 ], params [ 1 ] # Modelo Linear yhat = beta0 + beta1 * x #= mu #loc \u00e9 a m\u00e9dia e scale desvio padr\u00e3o. Note que sigma \u00e9 conhecido negLikelihood = - np . sum ( stats . norm . logpdf ( y , loc = yhat , scale = sigma )) return negLikelihood # Esse \u00e9 o chute inicial initial_guess = np . array ([ 3 , 6 ]) results = minimize ( MLE , initial_guess , method = 'Nelder-Mead' , options = { 'disp' : True }) Optimization terminated successfully. Current function value: 307.745486 Iterations: 56 Function evaluations: 107 print ( results ) final_simplex: (array([[-1.03428809, 3.11012856], [-1.0342294 , 3.110121 ], [-1.03433677, 3.11012912]]), array([293.95399071, 293.95399071, 293.95399071])) fun: 293.95399070678394 message: 'Optimization terminated successfully.' nfev: 103 nit: 55 status: 0 success: True x: array([-1.03428809, 3.11012856]) resultsdf = pd . DataFrame ({ 'coef' : results [ 'x' ]}) resultsdf . index = [ r '$\\beta_0$' , r '$\\beta_1$' ] np . round ( resultsdf . head ( 2 ), 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef $\\beta_0$ -1.0343 $\\beta_1$ 3.1101 Vamos estimar usando a biblioteca OLS. Ela faz esse processo e muito mais internamente. results_ols = sm . OLS ( data . y , data [[ 'constant' , 'x' ]]) . fit () results_ols . summary () OLS Regression Results Dep. Variable: y R-squared: 0.941 Model: OLS Adj. R-squared: 0.941 Method: Least Squares F-statistic: 1568. Date: Wed, 26 Aug 2020 Prob (F-statistic): 4.22e-62 Time: 21:20:55 Log-Likelihood: -293.06 No. Observations: 100 AIC: 590.1 Df Residuals: 98 BIC: 595.3 Df Model: 1 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] constant -1.0343 0.909 -1.138 0.258 -2.839 0.770 x 3.1101 0.079 39.599 0.000 2.954 3.266 Omnibus: 1.778 Durbin-Watson: 2.306 Prob(Omnibus): 0.411 Jarque-Bera (JB): 1.423 Skew: -0.289 Prob(JB): 0.491 Kurtosis: 3.084 Cond. No. 23.1 Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Veja que a estima\u00e7\u00e3o dos coeficientes foi a mesma! Apesar de ambas estarem erradas p para \\beta_0 . Na verdade se olharmos o intervalo de confian\u00e7a que OLS nos d\u00e1, vemos que de fato 0 est\u00e1 nele. Mas ainda n\u00e3o esta na hora de voc\u00eas verem isso! Conclus\u00e3o Podemos usar uma fun\u00e7\u00e3o de perda (que no caso ser\u00e1 menos a log-verossimilhan\u00e7a) e usar um algoritmo de otimiza\u00e7\u00e3o! Propriedades Invari\u00e2ncia Se \\hat{\\theta} \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de \\theta e g \u00e9 uma fun\u00e7\u00e3o injetiva, ent\u00e3o g(\\hat{\\theta}) \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de g(\\theta) . Na verdade, podemos retirar condi\u00e7\u00e3o de injetividade. MLE de uma Fun\u00e7\u00e3o Seja g(\\theta) uma fun\u00e7\u00e3o arbitr\u00e1ria do par\u00e2metro e G = g(\\Omega) . Para cada t \\in G , definimos G_t := \\{\\theta : g(\\theta) = t\\} e L^*(t) := \\max_{\\theta \\in G_t} log f_n(x|\\theta) Definimos a ML.E.de g(\\theta) := arg\\,max_{t\\in G} L^*(t) Teorema Seja \\hat{\\theta} MLE de \\theta e g(\\theta) fun\u00e7\u00e3o de \\theta . Ent\u00e3o uma MLE de g(\\theta) \u00e9 g(\\hat{\\theta}) . Consist\u00eancia Suponha que para uma amostra suficientemente grantde, existe um MLE \u00fanico para \\theta . Ent\u00e3o, sob algumas condi\u00e7\u00f5es, a sequ\u00eancia de MLE \u00e9 uma sequ\u00eancia consistente de estimadores de \\theta . A seuq\u00eancia convergee em probabilidade para o valor desconhecido de \\theta . O mesmo acontece com o Estimador de Bayes, dadas condi\u00e7\u00f5es de regularidade. Fun\u00e7\u00e3o Digamma: \\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)} M\u00e9todo dos Momentos Assuma que a amostra aleat\u00f3ria X_1,...,X_n vem da distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta k-dimensional. Por exemplo, a distribui\u00e7\u00e3o normal tem k = 2 . Tamb\u00e9m suponha que pelo menos os k primeiros momentos ( E[X_i^k] < \\infty ) sejam finitos. Defina \\mu_j(\\theta) = E[X_1^j|\\theta], j = 1,...k . Suponha que a fun\u00e7\u00e3o: \\begin{split} \\mu : ~&\\Omega \\to \\mathbb{R}^k \\\\ &\\theta \\mapsto \\mu(\\theta) = (\\mu_1(\\theta), ..., \\mu_k(\\theta)), \\end{split} \u00e9 injetiva em \\theta . Seja M(\\mu_1,...,\\mu_k) a fun\u00e7\u00e3o inversa, isto \u00e9, \\theta = M(\\mu_1,...,\\mu_k) O m\u00e9todo dos momentos ser\u00e1 M(m_1,...,m_j) , onde m_j = \\frac{1}{n}\\sum_{i=1}^n X_i^j, j = 1,...,k De forma mais simplificada, basta que sesolvemos o sistema: m_j = \\mu_j(\\theta), isto \u00e9, os momentos amostrais iguais aos momentos da amostra, condicionados em \\theta . Teorema Suponha que \\{X_n\\}_{n\\in\\mathbb{N}} i.i.d com distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta , k -dimensional. Suponha que os primeiros k momentos existem e s\u00e3o finitos para todo \\theta . Suponha que a inversa M definida acima \u00e9 cont\u00ednua. Ent\u00e3o a sequ\u00eancia de estimadores do m\u00e9todo de momentos em X_1,...,X_n \u00e9 consistente. M.L.E e Estimador de Bayes Se tivermos condi\u00e7\u00f5es de suavidade em f(x|\\theta) , podemos provar que quando n \\to \\infty , teremos que: L(\\theta|x) \\to c(x)\\cdot \\exp\\{-\\frac{1}{2V_n(\\theta)/n}(\\theta - \\hat{\\theta})^2\\}, onde \\hat{\\theta} \u00e9 MLE e V_n(\\theta) \u00e9 uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias convergente. No caso de termos uma priori relativamente flat, a posteriori ser\u00e1 aproximadamente uma distribui\u00e7\u00e3o normal com m\u00e9dia \\hat{\\theta} e vari\u00e2ncia V_n(\\hat{\\theta})/n . Exemplo 7.6.12 (Mortes ex\u00e9rcito pr\u00fassio) Bortkiewicz contou o n\u00famero de soldados mortos por horsekick em 14 unidades do ex\u00e9rcito em 20 anos, com 280 contagens ao total. Das contagens temos Valor 0 1 2 3 4 Total Contagem 144 91 32 11 2 280 Modelamos X_1, ..., X_{280} como uma vari\u00e1vel de contagem. Considere a distribui\u00e7\u00e3o Poisson(\\theta) . Escolhemos a distribui\u00e7\u00e3o Gamma(\\alpha,\\beta) , dada que ela pertence \u00e0 familia conjungada. Em particular, a distribui\u00e7\u00e3o a posteriori ser\u00e1 Gamma(\\alpha + \\sum X_i, \\beta + n) , onde \\sum X_i = 196 . Se assumirmos \\alpha inteiro por simplicidade, vemos que a distribui\u00e7\u00e3o Gamma pode ser vista como a soma de \\alpha + \\sum X_i distribui\u00e7\u00f5es Exponencial(\\beta + n) . Logo a soma dessas vari\u00e1veis ser\u00e1 aproximadamente normal com m\u00e9dia 196/280 e vari\u00e2ncia 196/280^2 . import numpy as np import matplotlib.pyplot as plt from scipy.stats import gamma alpha = 1 beta = 1 # Esse \u00e9 o MLE, a m\u00e9dia. Vou supor que esse \u00e9 o par\u00e2metro verdadeiro s\u00f3 para mostrar. theta = 196 / 280 sum_xi = 196 fig , ax = plt . subplots ( 2 , 3 , figsize = ( 18 , 6 )) fig . suptitle ( 'Avaliando a converg\u00eancia da distribui\u00e7\u00e3o Gamma' ) for index , n in enumerate ([ 1 , 10 , 100 , 1000 , 10000 , 280 ]): i = int ( index / 3 ) j = index % 3 X = np . random . poisson ( theta , size = n ) if n != 280 : T = X . sum () ax [ i ][ j ] . set_title ( 'n = {}' . format ( n )) else : T = sum_xi #Valor dos dados ax [ i ][ j ] . set_title ( 'Dados Oficiais: n = {}' . format ( n )) t = np . linspace ( start = 0.00001 , stop = 3 - i - 1 , num = 1000 ) posteriori = gamma ( alpha + T , scale = 1 / ( beta + n )) y = posteriori . pdf ( t ) ax [ i ][ j ] . plot ( t , y , color = 'darkblue' ) ax [ i ][ j ] . grid ( color = 'grey' , alpha = 0.6 , linestyle = '--' ) ax [ i ][ j ] . vlines ( theta , ymin = 0 , ymax = max ( y ), color = 'black' , linestyle = '--' ) Veja que com os dados reais, j\u00e1 temos uma boa aproxima\u00e7\u00e3o!","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#estimador-de-maxima-verossimilhanca","text":"","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#introducao","text":"\"Tradicionalmente a infer\u00eancia estat\u00edstica sobre a m\u00e9dia de uma popula\u00e7\u00e3o se apoia no Teorema Central do Limite para construir Intervalos de Confian\u00e7a ou testar hip\u00f3teses sobre o valor do par\u00e2metro. Esta abordagem da estat\u00edstica tradicional pode ser extendida para infer\u00eancias a respeito de qualquer par\u00e2metro, n\u00e3o s\u00f3 a m\u00e9dia. Da mesma forma que no caso da m\u00e9dia populacional se usa a distribui\u00e7\u00e3o t-Student ou a distribui\u00e7\u00e3o Normal Padr\u00e3o , no caso de outros par\u00e2metros se utiliza outras distribui\u00e7\u00f5es amostrais. Essas distribui\u00e7\u00f5es s\u00e3o chamadas amostrais porque representam o comportamento das estimativas baseado na repeti\u00e7\u00e3o incont\u00e1vel do processo de amostragem . Na pr\u00e1tica cient\u00edfica, no entanto, sempre se realiza uma \u00fanica amostragem , o que resulta em uma \u00fanica amostra. Assim, o conceito de distribui\u00e7\u00e3o amostral \u00e9 at\u00e9 certo ponto artificial, pois em pesquisa cient\u00edfica n\u00e3o raciocinamos em termos de repeti\u00e7\u00f5es incont\u00e1veis de experimentos ou processos de observa\u00e7\u00e3o . O resultado disto \u00e9 que o conceito de teste estat\u00edstico de hip\u00f3tese e de intervalo de confian\u00e7a s\u00e3o frequentemente mal compreendidos. O desenvolvimento da infer\u00eancia estat\u00edstica a partir do conceito de verossimilhan\u00e7a tem sido utilizado como uma alternativa \u00e0 abordagem estat\u00edstica frequentista e, segundo alguns autores (como por exemplo Royall, 1997), \u00e9 mais coerente com a pr\u00e1tica cient\u00edfica .\" (Batista, 2009) Site de Refer\u00eancia","title":"Introdu\u00e7\u00e3o"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#funcao-verossimilhanca","text":"Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x)","title":"Fun\u00e7\u00e3o Verossimilhan\u00e7a"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#estimador-de-maxima-verossimilhanca-mle","text":"Para cada observa\u00e7\u00e3o x , seja \\delta(x) um valor de \\theta \\in \\Omega tal que a fun\u00e7\u00e3o de verossimilhna\u00e7a seja m\u00e1xima . Defina \\hat{\\theta} = \\delta(X) o estimador. \u00c9 importante observar que o m\u00e1ximo dessa fun\u00e7\u00e3o pode n\u00e3o estar em um ponto de \\Omega . Nesse caso, MLE n\u00e3o existe. Ele pode n\u00e3o estar unicamente definido, tamb\u00e9m.","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a (MLE)"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#limitacoes","text":"N\u00e3o exist\u00eancia em todos os casos, isso depende muito da fun\u00e7\u00e3o e do espa\u00e7o dos par\u00e2metros. N\u00e3o unicidade em todos os casos. N\u00e3o podemos interpretar MLE como o par\u00e2metro mais prov\u00e1vel, pois ter\u00edamos que ter um espa\u00e7o de probabilidade associado ao par\u00e2metro, o que n\u00e3o \u00e9 dado.","title":"Limita\u00e7\u00f5es"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#implementacao","text":"Como refer\u00eancia, estou utilizando este site . # importando bibliotecas import numpy as np , pandas as pd from matplotlib import pyplot as plt import seaborn as sns from scipy.optimize import minimize import scipy.stats as stats import pymc3 as pm3 import numdifftools as ndt import statsmodels.api as sm from statsmodels.base.model import GenericLikelihoodModel % matplotlib inline # Gerando os dados N = 100 x = np . linspace ( 0 , 20 , N ) # gerando lista igualmente espa\u00e7ada beta1 = 3 beta0 = 0 sigma = 5 error = np . random . normal ( 0 , sigma , size = N ) y = beta1 * x + beta0 + error data = pd . DataFrame ({ 'y' : y , 'x' : x }) data [ 'constant' ] = 1 sns . regplot ( 'x' , 'y' , data = data ) # Essa reta \u00e9 uma estimativa dos dados feito por seaborn plt . title ( 'Dados' ) plt . show () Y = \\beta_1 x + \\beta_0 + e Nesse exemplo, o nosso problema ser\u00e1 estimar a m\u00e9dia. Observe que os dados tem um comportamento linear. Sem nos concentrarmos muito na modelagem e os problemas que ela pode trazer, eu vou j\u00e1 supor que temos um problema de Regress\u00e3o Linear , onde os dados Y \\sim N(\\mu, \\sigma^2) , onde \\sigma^2 \u00e9 a vari\u00e2ncia do erro no processo, e \\mu = \\beta_0 + \\beta_1 x , isto \u00e9, depende de x, nesse caso. Essa \u00e9 uma dificuldade, as contas ficam mais dif\u00edceis e, por isso, vamos usar asrtif\u00edcios computacionais. Vamos supor que a vari\u00e2ncia \u00e9 conhecida . Al\u00e9m disso, vamos supor que temos uma amostra aleat\u00f3ria Y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2) Temos que a verossimilhan\u00e7a \u00e9 produto das pdfs(distribui\u00e7\u00e3o de densidade de probabilidade). Para otimizar podemos, entretanto, obter a soma dos logaritmos das pdfs . E por fim, vamos resolver um problema de minimizar o negativo desse valor. Veja que \u00e9 equivalente a maximixar a soma!! # Fun\u00e7\u00e3o de verossimilhan\u00e7a. Chamamos de Fun\u00e7\u00e3o de Perda def MLE ( params ): # Fun\u00e7\u00e3o Perda: - log-verossimilhan\u00e7a beta0 , beta1 = params [ 0 ], params [ 1 ] # Modelo Linear yhat = beta0 + beta1 * x #= mu #loc \u00e9 a m\u00e9dia e scale desvio padr\u00e3o. Note que sigma \u00e9 conhecido negLikelihood = - np . sum ( stats . norm . logpdf ( y , loc = yhat , scale = sigma )) return negLikelihood # Esse \u00e9 o chute inicial initial_guess = np . array ([ 3 , 6 ]) results = minimize ( MLE , initial_guess , method = 'Nelder-Mead' , options = { 'disp' : True }) Optimization terminated successfully. Current function value: 307.745486 Iterations: 56 Function evaluations: 107 print ( results ) final_simplex: (array([[-1.03428809, 3.11012856], [-1.0342294 , 3.110121 ], [-1.03433677, 3.11012912]]), array([293.95399071, 293.95399071, 293.95399071])) fun: 293.95399070678394 message: 'Optimization terminated successfully.' nfev: 103 nit: 55 status: 0 success: True x: array([-1.03428809, 3.11012856]) resultsdf = pd . DataFrame ({ 'coef' : results [ 'x' ]}) resultsdf . index = [ r '$\\beta_0$' , r '$\\beta_1$' ] np . round ( resultsdf . head ( 2 ), 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef $\\beta_0$ -1.0343 $\\beta_1$ 3.1101 Vamos estimar usando a biblioteca OLS. Ela faz esse processo e muito mais internamente. results_ols = sm . OLS ( data . y , data [[ 'constant' , 'x' ]]) . fit () results_ols . summary () OLS Regression Results Dep. Variable: y R-squared: 0.941 Model: OLS Adj. R-squared: 0.941 Method: Least Squares F-statistic: 1568. Date: Wed, 26 Aug 2020 Prob (F-statistic): 4.22e-62 Time: 21:20:55 Log-Likelihood: -293.06 No. Observations: 100 AIC: 590.1 Df Residuals: 98 BIC: 595.3 Df Model: 1 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] constant -1.0343 0.909 -1.138 0.258 -2.839 0.770 x 3.1101 0.079 39.599 0.000 2.954 3.266 Omnibus: 1.778 Durbin-Watson: 2.306 Prob(Omnibus): 0.411 Jarque-Bera (JB): 1.423 Skew: -0.289 Prob(JB): 0.491 Kurtosis: 3.084 Cond. No. 23.1 Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Veja que a estima\u00e7\u00e3o dos coeficientes foi a mesma! Apesar de ambas estarem erradas p para \\beta_0 . Na verdade se olharmos o intervalo de confian\u00e7a que OLS nos d\u00e1, vemos que de fato 0 est\u00e1 nele. Mas ainda n\u00e3o esta na hora de voc\u00eas verem isso!","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#conclusao","text":"Podemos usar uma fun\u00e7\u00e3o de perda (que no caso ser\u00e1 menos a log-verossimilhan\u00e7a) e usar um algoritmo de otimiza\u00e7\u00e3o!","title":"Conclus\u00e3o"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#propriedades","text":"","title":"Propriedades"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#invariancia","text":"Se \\hat{\\theta} \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de \\theta e g \u00e9 uma fun\u00e7\u00e3o injetiva, ent\u00e3o g(\\hat{\\theta}) \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de g(\\theta) . Na verdade, podemos retirar condi\u00e7\u00e3o de injetividade.","title":"Invari\u00e2ncia"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#mle-de-uma-funcao","text":"Seja g(\\theta) uma fun\u00e7\u00e3o arbitr\u00e1ria do par\u00e2metro e G = g(\\Omega) . Para cada t \\in G , definimos G_t := \\{\\theta : g(\\theta) = t\\} e L^*(t) := \\max_{\\theta \\in G_t} log f_n(x|\\theta) Definimos a ML.E.de g(\\theta) := arg\\,max_{t\\in G} L^*(t)","title":"MLE de uma Fun\u00e7\u00e3o"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#teorema","text":"Seja \\hat{\\theta} MLE de \\theta e g(\\theta) fun\u00e7\u00e3o de \\theta . Ent\u00e3o uma MLE de g(\\theta) \u00e9 g(\\hat{\\theta}) .","title":"Teorema"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#consistencia","text":"Suponha que para uma amostra suficientemente grantde, existe um MLE \u00fanico para \\theta . Ent\u00e3o, sob algumas condi\u00e7\u00f5es, a sequ\u00eancia de MLE \u00e9 uma sequ\u00eancia consistente de estimadores de \\theta . A seuq\u00eancia convergee em probabilidade para o valor desconhecido de \\theta . O mesmo acontece com o Estimador de Bayes, dadas condi\u00e7\u00f5es de regularidade.","title":"Consist\u00eancia"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#funcao-digamma","text":"\\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)}","title":"Fun\u00e7\u00e3o Digamma:"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#metodo-dos-momentos","text":"Assuma que a amostra aleat\u00f3ria X_1,...,X_n vem da distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta k-dimensional. Por exemplo, a distribui\u00e7\u00e3o normal tem k = 2 . Tamb\u00e9m suponha que pelo menos os k primeiros momentos ( E[X_i^k] < \\infty ) sejam finitos. Defina \\mu_j(\\theta) = E[X_1^j|\\theta], j = 1,...k . Suponha que a fun\u00e7\u00e3o: \\begin{split} \\mu : ~&\\Omega \\to \\mathbb{R}^k \\\\ &\\theta \\mapsto \\mu(\\theta) = (\\mu_1(\\theta), ..., \\mu_k(\\theta)), \\end{split} \u00e9 injetiva em \\theta . Seja M(\\mu_1,...,\\mu_k) a fun\u00e7\u00e3o inversa, isto \u00e9, \\theta = M(\\mu_1,...,\\mu_k) O m\u00e9todo dos momentos ser\u00e1 M(m_1,...,m_j) , onde m_j = \\frac{1}{n}\\sum_{i=1}^n X_i^j, j = 1,...,k De forma mais simplificada, basta que sesolvemos o sistema: m_j = \\mu_j(\\theta), isto \u00e9, os momentos amostrais iguais aos momentos da amostra, condicionados em \\theta .","title":"M\u00e9todo dos Momentos"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#teorema_1","text":"Suponha que \\{X_n\\}_{n\\in\\mathbb{N}} i.i.d com distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta , k -dimensional. Suponha que os primeiros k momentos existem e s\u00e3o finitos para todo \\theta . Suponha que a inversa M definida acima \u00e9 cont\u00ednua. Ent\u00e3o a sequ\u00eancia de estimadores do m\u00e9todo de momentos em X_1,...,X_n \u00e9 consistente.","title":"Teorema"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#mle-e-estimador-de-bayes","text":"Se tivermos condi\u00e7\u00f5es de suavidade em f(x|\\theta) , podemos provar que quando n \\to \\infty , teremos que: L(\\theta|x) \\to c(x)\\cdot \\exp\\{-\\frac{1}{2V_n(\\theta)/n}(\\theta - \\hat{\\theta})^2\\}, onde \\hat{\\theta} \u00e9 MLE e V_n(\\theta) \u00e9 uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias convergente. No caso de termos uma priori relativamente flat, a posteriori ser\u00e1 aproximadamente uma distribui\u00e7\u00e3o normal com m\u00e9dia \\hat{\\theta} e vari\u00e2ncia V_n(\\hat{\\theta})/n .","title":"M.L.E e Estimador de Bayes"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#exemplo-7612-mortes-exercito-prussio","text":"Bortkiewicz contou o n\u00famero de soldados mortos por horsekick em 14 unidades do ex\u00e9rcito em 20 anos, com 280 contagens ao total. Das contagens temos Valor 0 1 2 3 4 Total Contagem 144 91 32 11 2 280 Modelamos X_1, ..., X_{280} como uma vari\u00e1vel de contagem. Considere a distribui\u00e7\u00e3o Poisson(\\theta) . Escolhemos a distribui\u00e7\u00e3o Gamma(\\alpha,\\beta) , dada que ela pertence \u00e0 familia conjungada. Em particular, a distribui\u00e7\u00e3o a posteriori ser\u00e1 Gamma(\\alpha + \\sum X_i, \\beta + n) , onde \\sum X_i = 196 . Se assumirmos \\alpha inteiro por simplicidade, vemos que a distribui\u00e7\u00e3o Gamma pode ser vista como a soma de \\alpha + \\sum X_i distribui\u00e7\u00f5es Exponencial(\\beta + n) . Logo a soma dessas vari\u00e1veis ser\u00e1 aproximadamente normal com m\u00e9dia 196/280 e vari\u00e2ncia 196/280^2 . import numpy as np import matplotlib.pyplot as plt from scipy.stats import gamma alpha = 1 beta = 1 # Esse \u00e9 o MLE, a m\u00e9dia. Vou supor que esse \u00e9 o par\u00e2metro verdadeiro s\u00f3 para mostrar. theta = 196 / 280 sum_xi = 196 fig , ax = plt . subplots ( 2 , 3 , figsize = ( 18 , 6 )) fig . suptitle ( 'Avaliando a converg\u00eancia da distribui\u00e7\u00e3o Gamma' ) for index , n in enumerate ([ 1 , 10 , 100 , 1000 , 10000 , 280 ]): i = int ( index / 3 ) j = index % 3 X = np . random . poisson ( theta , size = n ) if n != 280 : T = X . sum () ax [ i ][ j ] . set_title ( 'n = {}' . format ( n )) else : T = sum_xi #Valor dos dados ax [ i ][ j ] . set_title ( 'Dados Oficiais: n = {}' . format ( n )) t = np . linspace ( start = 0.00001 , stop = 3 - i - 1 , num = 1000 ) posteriori = gamma ( alpha + T , scale = 1 / ( beta + n )) y = posteriori . pdf ( t ) ax [ i ][ j ] . plot ( t , y , color = 'darkblue' ) ax [ i ][ j ] . grid ( color = 'grey' , alpha = 0.6 , linestyle = '--' ) ax [ i ][ j ] . vlines ( theta , ymin = 0 , ymax = max ( y ), color = 'black' , linestyle = '--' ) Veja que com os dados reais, j\u00e1 temos uma boa aproxima\u00e7\u00e3o!","title":"Exemplo 7.6.12 (Mortes ex\u00e9rcito pr\u00fassio)"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/","text":"Distribui\u00e7\u00f5es a Priori e a Posteriori Priori Tratamos \\theta de um modelo como uma vari\u00e1vel aleat\u00f3ria e atribuimos uma distribui\u00e7\u00e3o para esse par\u00e2metro. O nome ser\u00e1 distribui\u00e7\u00e3o a priori. Ao fazer modelagens, ela \u00e9 em geral pr\u00e9-definida pelo modelador, que \u00e9 em geral aconselhado por um especialista. Posteriori Sejam X_1, ..., X_n v.a. observadas e um par\u00e2metro \\theta desconhecido. A distribui\u00e7\u00e3o de \\theta condicionado nas vari\u00e1veis aleat\u00f3rias \u00e9 a distribui\u00e7\u00e3o a posteriori. Observe a rela\u00e7\u00e3o com o Teorema de Bayes. Teorema Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o f(x|\\theta) . Suponha que o par\u00e2metro seja desconhecido e que a distribui\u00e7\u00e3o da priori seja \\xi(\\theta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori \u00e9: \\xi(\\theta|x) = \\frac{f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta)}{g_n(\\theta)}, \\theta \\in \\Omega Onde g_n \u00e9 a distribui\u00e7\u00e3o marginal conjunta de X_1,...,X_n Observe que, essencialmente \\xi(\\theta|x) \\propto f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta) , mas que sua integral seja 1 . Queremos que essa fun\u00e7\u00e3o seja integr\u00e1vel e a integral sobre o dom\u00ednio seja 1 . Fun\u00e7\u00e3o de Verossimilhan\u00e7a Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x) Observa\u00e7\u00f5es Sequenciais e Predi\u00e7\u00f5es Nesse caso a ordem das vari\u00e1veis X_1, ..., X_n importam (como uma s\u00e9rie temporal, por exemplo). Nesse caso, podemos, iterativamente fazer: \\xi(\\theta|x_1) \\propto f(x_1|\\theta)\\xi(\\theta) \\xi(\\theta|x_1,...,x_{n+1}) \\propto f(x_n|\\theta)\\xi(\\theta|x_1,....,x_n) Isso acontece dada a independ\u00eancia das vari\u00e1veis aleat\u00f3rias. Notebook de Refer\u00eancia Frequentistas Os dados observados s\u00e3o considerados aleat\u00f3rios, realidados de um processo aleat\u00f3rio. Os par\u00e2metros do modelo s\u00e3o fixos e desconhecidos Queremos derivas estimadores para os par\u00e2metros desconhecidos. Bayesianos Os dados s\u00e3o fixos, isto \u00e9, vieram de um processo aleat\u00f3rio, mas depois eles n\u00e3o se alteram. Os par\u00e2metros s\u00e3o usualmente representados por distribui\u00e7\u00f5es, s\u00e3o vari\u00e1veis aleat\u00f3rias. F\u00f3rmula de Bayes. Simples exemplo de infer\u00eancia Bayesiana Hemofilia \u00e9 uma disordem gen\u00e9tica que prejudica a coagula\u00e7\u00e3o em resposta a rupturas em vasos sangu\u00edneos. \u00c9 recessiva ligada ao cromossomo X. Isso implica que homens com 1 gene s\u00e3o afetados, enquanto as mulheres n\u00e3o s\u00e3o afetadas, mas portadoras. Considere uma mulher cuja m\u00e3e \u00e9 portadora e tem um irm\u00e3o afetado. Ela se casa com um homem n\u00e3o afetado. A mulher tem dois filhos consecutivos que n\u00e3o s\u00e3o afetados. Ser\u00e1 que a m\u00e3e \u00e9 portadora? A pergunra \u00e9 simples. Vamos tentar usar um pouco do que sabemos. Seja W = 1 se a mulher \u00e9 portadora e W = 0 se ela n\u00e3o for portadora. Queremos saber P(W = 1|s_1 = 0, s_2 = 0) , isto \u00e9, os filhos n\u00e3o s\u00e3o afetados. Que informa\u00e7\u00e3o n\u00f3s temos ? A m\u00e3e dela \u00e9 portadora, portanto uma priori interessante \u00e9: P(W = 1) = 0.5 \\Rightarrow O(W = 1) = \\frac{P(W=1)}{P(W=0)} = 1 \\text{ chances (odds) a priori } Podemos calcular a fun\u00e7\u00e3o de verossimilhan\u00e7a: L(W = 1|s_1 = 0, s_2 = 0) = F(s_1 = 0, s_2 = 0 | W = 1) = (0.5)(0.5) = 0.25 L(W = 0|s_1 = 0, s_2 = 0) = (1)(1) = 1 import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm priori = 0.5 # P(W = 1) p = 0.5 # prob de um filho ser afetado Likelihood = lambda w , s : np . prod ([( 1 - i , p ** i * ( 1 - p ) ** ( 1 - i ))[ w ] for i in s ]) s = [ 0 , 0 ] posteriori = Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori )) print ( \"A probabilidade da m\u00e3e portar \u00e9 {} com dois filhos n\u00e3o portadores.\" . format ( posteriori )) A probabilidade da m\u00e3e portar \u00e9 0.2 com dois filhos n\u00e3o portadores. s = [ 0 ] # terceiro filho priori = posteriori posteriori = Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori )) print ( \"A probabilidade da m\u00e3e portar \u00e9 {:.3f} com tr\u00eas filhos n\u00e3o portadores.\" . format ( posteriori )) A probabilidade da m\u00e3e portar \u00e9 0.111 com tr\u00eas filhos n\u00e3o portadores. priori = 0.5 p = 0.5 s = [ 0 ] posteriori = [] for i in range ( 50 ): posteriori . append ( Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori ))) priori = posteriori [ - 1 ] priori = 0.5 posteriori2 = [] for i in range ( 50 ): posteriori2 . append ( Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori ))) priori = posteriori2 [ - 1 ] s = [ np . random . choice ([ 0 , 1 ], p = ( 0.9 , 0.1 ))] plt . plot ( range ( 50 ), posteriori , label = 'Situa\u00e7\u00e3o 1' ) plt . plot ( range ( 50 ), posteriori2 , label = 'Situa\u00e7\u00e3o 2' ) plt . legend () plt . title ( 'Probabilidade dado cada filho' ) plt . show () Princ\u00edpio de Verossimilhan\u00e7a Afirma que para uma infer\u00eancia sobre um par\u00e2metro \\theta , toda evid\u00eancia de qualquer observa\u00e7\u00e3o de uma vari\u00e1vel aleat\u00f3ria X = x com distribui\u00e7\u00e3o X \\sim f(x|\\theta) se encontra na fun\u00e7\u00e3o de verossimilhan\u00e7a L(\\theta|x) . A interpreta\u00e7\u00e3o \u00e9 de que qualquer observa\u00e7\u00e3o de X pode construir conclus\u00f5es sobre \\theta . Al\u00e9m disso, se pud\u00e9ssemos obter informa\u00e7\u00e3o de \\theta sobre outra vari\u00e1vel aleat\u00f3ria Y com verossimilhan\u00e7a \\tilde{L} , teremos que L(\\theta|x) = c\\cdot \\tilde{L}(\\theta|y) . Isto \u00e9, as conclus\u00f5es sobre o par\u00e2metro n\u00e3o dependem da observa\u00e7\u00e3o feita. Qual o problema? Jeffreys : \"An hypothesis that may be true is rejected because it has failed to predict observable results that have not occurred. \" import pymc3 as pm from pymc3 import Model , Normal , Slice from pymc3 import sample from pymc3 import traceplot from pymc3.distributions import Interpolated from scipy import stats import matplotlib as mpl plt . style . use ( 'seaborn-darkgrid' ) Gerando os dados Y = \\alpha + \\beta_0\\cdot X_1 + \\beta_1\\cdot X_2 + \\text{erro} # True parameters alpha_true = 5 beta0_true = 7 beta1_true = 13 # Size of the dataset size = 100 # Random variables np . random . seed ( 1 ) X1 = np . random . randn ( size ) X2 = np . random . randn ( size ) * 0.2 e = np . random . randn ( size ) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e Especificando o modelo Vamos fazer um modelo simples aqui, s\u00f3 para mostrar essa biblioteca nova. A ideia \u00e9 mostrar como funciona a ideia de priori e posteriori. Vamos dizer que Y \\sim N(\\mu, 1) , onde \\mu = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 model = Model () #cria um novo modelo, que em Python, \u00e9 um objeto with model : # Isso cria um contexto em python # Vamos dizer nossas prioris! alpha = Normal ( 'alpha' , mu = 0 , sigma = 1 ) beta0 = Normal ( 'beta0' , mu = 12 , sigma = 1 ) beta1 = Normal ( 'beta1' , mu = 18 , sigma = 1 ) # Valor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Verossimilhan\u00e7a das observa\u00e7\u00f5es Y_obs = Normal ( 'Y_obs' , mu = mu , sigma = 1 , observed = Y ) # Amostras da distribui\u00e7\u00e3o trace = sample ( 1000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. A pr\u00f3xima fun\u00e7\u00e3o \u00e9 um plot de distribui\u00e7\u00e3o a priori amostrada (que \u00e9 basicamente um histograma) de cada par\u00e2metro. Veja que parece um chiado em torno da m\u00e9dia. traceplot ( trace ); Agora que n\u00f3s temos os dados gerados Y , vamos atualizar nosso conhecimento, nossa confian\u00e7a nos par\u00e2metros, atrav\u00e9s da distribui\u00e7\u00e3o a posteriori. Os dados devem ser independentes a ada intera\u00e7\u00e3o para que valha o que estudamos. Para isso, precisamos calcular a posteriori de cada par\u00e2metro. Nesse caso, vamos utilizar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o, utlizando aproxima\u00e7\u00e3o Kernel . N\u00e3o se preocupe com isso, \u00e9 s\u00f3 para mostrar que estamos calculado a posteriori def from_posterior ( param , samples ): smin , smax = np . min ( samples ), np . max ( samples ) width = smax - smin x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) # what was never sampled should have a small probability but not 0, # so we'll extend the domain and use linear approximation of density on it x = np . concatenate ([[ x [ 0 ] - 3 * width ], x , [ x [ - 1 ] + 3 * width ]]) y = np . concatenate ([[ 0 ], y , [ 0 ]]) return Interpolated ( param , x , y ) Agora, vamos gerar mais dados e usar a F\u00f3rmula de Bayes e usaremos uma forma sequencial das observa\u00e7\u00f5es, isto \u00e9, a posteriori da itera\u00e7\u00e3o n-1 ser\u00e1 a priori da itera\u00e7\u00e3o n . traces = [ trace ] # salva os tra\u00e7os para que plotamos depois. for _ in range ( 10 ): # _ indica uma vari\u00e1vel que n\u00e3o \u00e9 usada # Gerando mais e mais dados! X1 = np . random . randn ( size ) X2 = np . random . randn ( size ) * 0.2 e = np . random . randn ( size ) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e model = Model () with model : # As novas prioris s\u00e3o as posterioris alpha = from_posterior ( 'alpha' , trace [ 'alpha' ]) beta0 = from_posterior ( 'beta0' , trace [ 'beta0' ]) beta1 = from_posterior ( 'beta1' , trace [ 'beta1' ]) # EValor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Calculando a verossimilhan\u00e7a dos novos dados Y_obs = Normal ( 'Y_obs' , mu = mu , sigma = 1 , observed = Y ) # Amostrando da posteriori, porque n\u00e3o estamos calculando a forma fechada trace = sample ( 1000 ) traces . append ( trace ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The acceptance probability does not match the target. It is 0.8800702431829607, but should be close to 0.8. Try to increase the number of tuning steps. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. fig , ax = plt . subplots ( 1 , 3 , figsize = ( 20 , 5 )) # Definindo cores cmap = mpl . cm . autumn for index , param in enumerate ([ 'alpha' , 'beta0' , 'beta1' ]): for update_i , trace in enumerate ( traces ): samples = trace [ param ] smin , smax = np . min ( samples ), np . max ( samples ) x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) ax [ index ] . plot ( x , y , color = cmap ( 1 - update_i / len ( traces )), alpha = update_i / len ( traces )) ax [ index ] . axvline ({ 'alpha' : alpha_true , 'beta0' : beta0_true , 'beta1' : beta1_true }[ param ], c = 'k' ) ax [ index ] . set_ylabel ( 'Frequency' ) ax [ index ] . set_title ( param ) plt . tight_layout ();","title":"Priori e Posteriori"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#distribuicoes-a-priori-e-a-posteriori","text":"","title":"Distribui\u00e7\u00f5es a Priori e a Posteriori"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#priori","text":"Tratamos \\theta de um modelo como uma vari\u00e1vel aleat\u00f3ria e atribuimos uma distribui\u00e7\u00e3o para esse par\u00e2metro. O nome ser\u00e1 distribui\u00e7\u00e3o a priori. Ao fazer modelagens, ela \u00e9 em geral pr\u00e9-definida pelo modelador, que \u00e9 em geral aconselhado por um especialista.","title":"Priori"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#posteriori","text":"Sejam X_1, ..., X_n v.a. observadas e um par\u00e2metro \\theta desconhecido. A distribui\u00e7\u00e3o de \\theta condicionado nas vari\u00e1veis aleat\u00f3rias \u00e9 a distribui\u00e7\u00e3o a posteriori. Observe a rela\u00e7\u00e3o com o Teorema de Bayes.","title":"Posteriori"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#teorema","text":"Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o f(x|\\theta) . Suponha que o par\u00e2metro seja desconhecido e que a distribui\u00e7\u00e3o da priori seja \\xi(\\theta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori \u00e9: \\xi(\\theta|x) = \\frac{f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta)}{g_n(\\theta)}, \\theta \\in \\Omega Onde g_n \u00e9 a distribui\u00e7\u00e3o marginal conjunta de X_1,...,X_n Observe que, essencialmente \\xi(\\theta|x) \\propto f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta) , mas que sua integral seja 1 . Queremos que essa fun\u00e7\u00e3o seja integr\u00e1vel e a integral sobre o dom\u00ednio seja 1 .","title":"Teorema"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#funcao-de-verossimilhanca","text":"Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x)","title":"Fun\u00e7\u00e3o de Verossimilhan\u00e7a"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#observacoes-sequenciais-e-predicoes","text":"Nesse caso a ordem das vari\u00e1veis X_1, ..., X_n importam (como uma s\u00e9rie temporal, por exemplo). Nesse caso, podemos, iterativamente fazer: \\xi(\\theta|x_1) \\propto f(x_1|\\theta)\\xi(\\theta) \\xi(\\theta|x_1,...,x_{n+1}) \\propto f(x_n|\\theta)\\xi(\\theta|x_1,....,x_n) Isso acontece dada a independ\u00eancia das vari\u00e1veis aleat\u00f3rias. Notebook de Refer\u00eancia","title":"Observa\u00e7\u00f5es Sequenciais e Predi\u00e7\u00f5es"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#frequentistas","text":"Os dados observados s\u00e3o considerados aleat\u00f3rios, realidados de um processo aleat\u00f3rio. Os par\u00e2metros do modelo s\u00e3o fixos e desconhecidos Queremos derivas estimadores para os par\u00e2metros desconhecidos.","title":"Frequentistas"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#bayesianos","text":"Os dados s\u00e3o fixos, isto \u00e9, vieram de um processo aleat\u00f3rio, mas depois eles n\u00e3o se alteram. Os par\u00e2metros s\u00e3o usualmente representados por distribui\u00e7\u00f5es, s\u00e3o vari\u00e1veis aleat\u00f3rias. F\u00f3rmula de Bayes.","title":"Bayesianos"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#simples-exemplo-de-inferencia-bayesiana","text":"Hemofilia \u00e9 uma disordem gen\u00e9tica que prejudica a coagula\u00e7\u00e3o em resposta a rupturas em vasos sangu\u00edneos. \u00c9 recessiva ligada ao cromossomo X. Isso implica que homens com 1 gene s\u00e3o afetados, enquanto as mulheres n\u00e3o s\u00e3o afetadas, mas portadoras. Considere uma mulher cuja m\u00e3e \u00e9 portadora e tem um irm\u00e3o afetado. Ela se casa com um homem n\u00e3o afetado. A mulher tem dois filhos consecutivos que n\u00e3o s\u00e3o afetados. Ser\u00e1 que a m\u00e3e \u00e9 portadora? A pergunra \u00e9 simples. Vamos tentar usar um pouco do que sabemos. Seja W = 1 se a mulher \u00e9 portadora e W = 0 se ela n\u00e3o for portadora. Queremos saber P(W = 1|s_1 = 0, s_2 = 0) , isto \u00e9, os filhos n\u00e3o s\u00e3o afetados. Que informa\u00e7\u00e3o n\u00f3s temos ? A m\u00e3e dela \u00e9 portadora, portanto uma priori interessante \u00e9: P(W = 1) = 0.5 \\Rightarrow O(W = 1) = \\frac{P(W=1)}{P(W=0)} = 1 \\text{ chances (odds) a priori } Podemos calcular a fun\u00e7\u00e3o de verossimilhan\u00e7a: L(W = 1|s_1 = 0, s_2 = 0) = F(s_1 = 0, s_2 = 0 | W = 1) = (0.5)(0.5) = 0.25 L(W = 0|s_1 = 0, s_2 = 0) = (1)(1) = 1 import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm priori = 0.5 # P(W = 1) p = 0.5 # prob de um filho ser afetado Likelihood = lambda w , s : np . prod ([( 1 - i , p ** i * ( 1 - p ) ** ( 1 - i ))[ w ] for i in s ]) s = [ 0 , 0 ] posteriori = Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori )) print ( \"A probabilidade da m\u00e3e portar \u00e9 {} com dois filhos n\u00e3o portadores.\" . format ( posteriori )) A probabilidade da m\u00e3e portar \u00e9 0.2 com dois filhos n\u00e3o portadores. s = [ 0 ] # terceiro filho priori = posteriori posteriori = Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori )) print ( \"A probabilidade da m\u00e3e portar \u00e9 {:.3f} com tr\u00eas filhos n\u00e3o portadores.\" . format ( posteriori )) A probabilidade da m\u00e3e portar \u00e9 0.111 com tr\u00eas filhos n\u00e3o portadores. priori = 0.5 p = 0.5 s = [ 0 ] posteriori = [] for i in range ( 50 ): posteriori . append ( Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori ))) priori = posteriori [ - 1 ] priori = 0.5 posteriori2 = [] for i in range ( 50 ): posteriori2 . append ( Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori ))) priori = posteriori2 [ - 1 ] s = [ np . random . choice ([ 0 , 1 ], p = ( 0.9 , 0.1 ))] plt . plot ( range ( 50 ), posteriori , label = 'Situa\u00e7\u00e3o 1' ) plt . plot ( range ( 50 ), posteriori2 , label = 'Situa\u00e7\u00e3o 2' ) plt . legend () plt . title ( 'Probabilidade dado cada filho' ) plt . show ()","title":"Simples exemplo de infer\u00eancia Bayesiana"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#principio-de-verossimilhanca","text":"Afirma que para uma infer\u00eancia sobre um par\u00e2metro \\theta , toda evid\u00eancia de qualquer observa\u00e7\u00e3o de uma vari\u00e1vel aleat\u00f3ria X = x com distribui\u00e7\u00e3o X \\sim f(x|\\theta) se encontra na fun\u00e7\u00e3o de verossimilhan\u00e7a L(\\theta|x) . A interpreta\u00e7\u00e3o \u00e9 de que qualquer observa\u00e7\u00e3o de X pode construir conclus\u00f5es sobre \\theta . Al\u00e9m disso, se pud\u00e9ssemos obter informa\u00e7\u00e3o de \\theta sobre outra vari\u00e1vel aleat\u00f3ria Y com verossimilhan\u00e7a \\tilde{L} , teremos que L(\\theta|x) = c\\cdot \\tilde{L}(\\theta|y) . Isto \u00e9, as conclus\u00f5es sobre o par\u00e2metro n\u00e3o dependem da observa\u00e7\u00e3o feita. Qual o problema? Jeffreys : \"An hypothesis that may be true is rejected because it has failed to predict observable results that have not occurred. \" import pymc3 as pm from pymc3 import Model , Normal , Slice from pymc3 import sample from pymc3 import traceplot from pymc3.distributions import Interpolated from scipy import stats import matplotlib as mpl plt . style . use ( 'seaborn-darkgrid' )","title":"Princ\u00edpio de Verossimilhan\u00e7a"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#gerando-os-dados","text":"Y = \\alpha + \\beta_0\\cdot X_1 + \\beta_1\\cdot X_2 + \\text{erro} # True parameters alpha_true = 5 beta0_true = 7 beta1_true = 13 # Size of the dataset size = 100 # Random variables np . random . seed ( 1 ) X1 = np . random . randn ( size ) X2 = np . random . randn ( size ) * 0.2 e = np . random . randn ( size ) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e","title":"Gerando os dados"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#especificando-o-modelo","text":"Vamos fazer um modelo simples aqui, s\u00f3 para mostrar essa biblioteca nova. A ideia \u00e9 mostrar como funciona a ideia de priori e posteriori. Vamos dizer que Y \\sim N(\\mu, 1) , onde \\mu = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 model = Model () #cria um novo modelo, que em Python, \u00e9 um objeto with model : # Isso cria um contexto em python # Vamos dizer nossas prioris! alpha = Normal ( 'alpha' , mu = 0 , sigma = 1 ) beta0 = Normal ( 'beta0' , mu = 12 , sigma = 1 ) beta1 = Normal ( 'beta1' , mu = 18 , sigma = 1 ) # Valor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Verossimilhan\u00e7a das observa\u00e7\u00f5es Y_obs = Normal ( 'Y_obs' , mu = mu , sigma = 1 , observed = Y ) # Amostras da distribui\u00e7\u00e3o trace = sample ( 1000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. A pr\u00f3xima fun\u00e7\u00e3o \u00e9 um plot de distribui\u00e7\u00e3o a priori amostrada (que \u00e9 basicamente um histograma) de cada par\u00e2metro. Veja que parece um chiado em torno da m\u00e9dia. traceplot ( trace ); Agora que n\u00f3s temos os dados gerados Y , vamos atualizar nosso conhecimento, nossa confian\u00e7a nos par\u00e2metros, atrav\u00e9s da distribui\u00e7\u00e3o a posteriori. Os dados devem ser independentes a ada intera\u00e7\u00e3o para que valha o que estudamos. Para isso, precisamos calcular a posteriori de cada par\u00e2metro. Nesse caso, vamos utilizar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o, utlizando aproxima\u00e7\u00e3o Kernel . N\u00e3o se preocupe com isso, \u00e9 s\u00f3 para mostrar que estamos calculado a posteriori def from_posterior ( param , samples ): smin , smax = np . min ( samples ), np . max ( samples ) width = smax - smin x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) # what was never sampled should have a small probability but not 0, # so we'll extend the domain and use linear approximation of density on it x = np . concatenate ([[ x [ 0 ] - 3 * width ], x , [ x [ - 1 ] + 3 * width ]]) y = np . concatenate ([[ 0 ], y , [ 0 ]]) return Interpolated ( param , x , y ) Agora, vamos gerar mais dados e usar a F\u00f3rmula de Bayes e usaremos uma forma sequencial das observa\u00e7\u00f5es, isto \u00e9, a posteriori da itera\u00e7\u00e3o n-1 ser\u00e1 a priori da itera\u00e7\u00e3o n . traces = [ trace ] # salva os tra\u00e7os para que plotamos depois. for _ in range ( 10 ): # _ indica uma vari\u00e1vel que n\u00e3o \u00e9 usada # Gerando mais e mais dados! X1 = np . random . randn ( size ) X2 = np . random . randn ( size ) * 0.2 e = np . random . randn ( size ) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e model = Model () with model : # As novas prioris s\u00e3o as posterioris alpha = from_posterior ( 'alpha' , trace [ 'alpha' ]) beta0 = from_posterior ( 'beta0' , trace [ 'beta0' ]) beta1 = from_posterior ( 'beta1' , trace [ 'beta1' ]) # EValor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Calculando a verossimilhan\u00e7a dos novos dados Y_obs = Normal ( 'Y_obs' , mu = mu , sigma = 1 , observed = Y ) # Amostrando da posteriori, porque n\u00e3o estamos calculando a forma fechada trace = sample ( 1000 ) traces . append ( trace ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The acceptance probability does not match the target. It is 0.8800702431829607, but should be close to 0.8. Try to increase the number of tuning steps. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. fig , ax = plt . subplots ( 1 , 3 , figsize = ( 20 , 5 )) # Definindo cores cmap = mpl . cm . autumn for index , param in enumerate ([ 'alpha' , 'beta0' , 'beta1' ]): for update_i , trace in enumerate ( traces ): samples = trace [ param ] smin , smax = np . min ( samples ), np . max ( samples ) x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) ax [ index ] . plot ( x , y , color = cmap ( 1 - update_i / len ( traces )), alpha = update_i / len ( traces )) ax [ index ] . axvline ({ 'alpha' : alpha_true , 'beta0' : beta0_true , 'beta1' : beta1_true }[ param ], c = 'k' ) ax [ index ] . set_ylabel ( 'Frequency' ) ax [ index ] . set_title ( param ) plt . tight_layout ();","title":"Especificando o modelo"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/","text":"Distribui\u00e7\u00e3o Chi-Quadrado Para cada m positivo, a distribui\u00e7\u00e3o Gamma(m/2, 1/2) \u00e9 chamada de distribui\u00e7\u00e3o \\chi^2 . Ela foi primeiramente descrita por Helmert para computar a distribui\u00e7\u00e3o amostral de uma popula\u00e7\u00e3o normal. Vamos ver como a normal se relaciona mais a frente. f(x) = \\frac{1}{2^{m/2}\\Gamma(m/2)}x^{m/2 - 1}e^{-x/2} Propriedades Se X \\sim \\chi^2(m) , ent\u00e3o: E(X) = m Var(X) = 2m \\psi(t) = \\left(\\frac{1}{1-2t}\\right)^{m/2}, t < \\frac{1}{2} Soma de \\chi^2 Se X_1, ..., X_k s\u00e3o independentes e cada uma tem grau de liberdade m_i , ent\u00e3o X_1 + ... + X_n tem distribui\u00e7\u00e3o \\chi^2(m_1 + .... + m_k) Rela\u00e7\u00e3o com a Normal Se X tem distribui\u00e7\u00e3o normal padr\u00e3o, Y = X^2 \\sim \\chi^2(1) De fato, se juntarmos as \u00faltimos dois teoremas, veremos que a soma de quadrados de normais independentes e identicamente distribuidas ser\u00e1 \\chi^2(m) , onde m \u00e9 o n\u00famero de parcelas. Implementa\u00e7\u00e3o import numpy as np import matplotlib.pyplot as plt from scipy.stats import chi2 from matplotlib import animation , cm from IPython.display import HTML # Random Object ro = np . random . default_rng ( 1000 ) # Para assegurar reprodutibilidade degree_freedom = 10 mean , var , skew , kurt = chi2 . stats ( degree_freedom , moments = 'mvsk' ) print ( 'Propriedades' ) print ( 'M\u00e9dia: {}' . format ( mean )) print ( 'Var: {}' . format ( var )) print ( 'Assimetria: {}' . format ( skew )) print ( 'Curtose: {}' . format ( kurt )) Propriedades M\u00e9dia: 10.0 Var: 20.0 Assimetria: 0.8944271909999159 Curtose: 1.2 fig , ax = plt . subplots ( 1 , 1 ) x = np . linspace ( chi2 . ppf ( 0.01 , degree_freedom ), chi2 . ppf ( 0.99 , degree_freedom ), 100 ) ax . plot ( x , chi2 . pdf ( x , degree_freedom ), 'r-' , lw = 5 , alpha = 0.6 , label = 'chi2 pdf' ) r = chi2 . rvs ( degree_freedom , size = 10000 ) ax . hist ( r , density = True , alpha = 0.2 ) ax . legend () plt . show () fig , ax = plt . subplots () line , = ax . plot ( x , chi2 . pdf ( x , degree_freedom ), 'r-' , lw = 5 , alpha = 0.6 ) ax . set_xlim (( 0 , 150 )) ax . set_title ( 'Chi-Square' ) def animate ( i , degree_freedom ): x = np . linspace ( 0 , chi2 . ppf ( 0.99 , degree_freedom + i ), 100 ) line . set_data ( x , chi2 . pdf ( x , degree_freedom + i )) return line , anim = animation . FuncAnimation ( fig , animate , frames = 100 , interval = 50 , fargs = ( degree_freedom ,), repeat = False ) HTML ( anim . to_html5_video ()) Your browser does not support the video tag. Distribui\u00e7\u00e3o Conjunta da m\u00e9dia e vari\u00e2ncia amostrais X_1,...,X_n formam uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal e com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 desconhecidos. Estamos interessados na distribui\u00e7\u00e3o conjunta dos estimadores de m\u00e1xima verossimilhan\u00e7a para m\u00e9dia e vari\u00e2ncia da amostra. Teorema de Basu Sejam \\hat{\\mu} = \\bar{X}_n e \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 a m\u00e9dia e vari\u00e2ncia amostrais, respectivamente. Ent\u00e3o \\hat{\\mu} tem distribui\u00e7\u00e3o normal com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 /n , enquanto \\hat{\\sigma}^2 tem a distribui\u00e7\u00e3o \\chi^2(n-1) , isto \u00e9, com n-1 grau de liberdade. Al\u00e9m disso elas s\u00e3o independentes. Esse teorema \u00e9 um pouco mais complexo e, na verdade, essa seria uma esp\u00e9cie de aplica\u00e7\u00e3o do teorema, na verdade. O teorema de Basu diz que: Se T \u00e9 uma estat\u00edstica suficiente completa (Considere, nesse teorema, g uma fun\u00e7\u00e3o integr\u00e1vel limitada) para \\theta e A uma estat\u00edstica ancillary, ent\u00e3o T \u00e9 independente de A . Nesse caso \\hat{\\mu} \u00e9 completa suficiente e \\hat{\\sigma}^2 \u00e9 ancillary, por que n\u00e3o depende de \\mu . O mais interessante \u00e9 que essa propriedade \u00e9 s\u00f3 vista com a distribui\u00e7\u00e3o normal ! Olhem a p\u00e1gina 9. Demonstra\u00e7\u00e3o O livro tem uma abordagem um pouco mais voltado \u00e0 \u00c1lgebra Linear. Aqui vou mostrar uma ideia um pouco diferente, onde voc\u00eas podem demonstrar os passos, como exerc\u00edcio. Passo 1: \\sum_{i=1}^n X_i^2 = n\\hat{\\sigma}^2 + n\\hat{\\mu}^2 Dica: Escrever \\hat{\\sigma}^2 e abrir em tr\u00eas somat\u00f3rios. Passo 2: \\sum_{i=1}^n (X_i - \\mu)^2 = n\\hat{\\sigma}^2 + n(\\hat{\\mu} - \\mu)^2 Dica: O Passo 1 \u00e9 um caso especial do Passo 2. O processo \u00e9 o mesmo. Passo 3: \\hat{\\mu} \u00e9 independente de X_i - \\hat{\\mu}, i = 1,...,n . Dica: Montar a pdf conjunta de X_1, ..., X_n (j\u00e1 fizemos isso atrave\u015b da verossimilhan\u00e7a) e fazer uma mudan\u00e7a de vari\u00e1vel Y_1 = \\hat{\\mu}, Y_2 = X_2 - \\hat{\\mu}, ..., Y_n = X_n - \\hat{\\mu} . Com essa mudan\u00e7a, \u00e9 poss\u00edvel montar a pdf como fun\u00e7\u00e3o de y_1,...,y_n . Esse processo \u00e9 um pouco mais chato, mas \u00e9 bom lembrar como fazez mudan\u00e7a de vari\u00e1vel para pdfs. Aqui voc\u00ea pode conferir como . \u00c9 importante lembrar que \u00e9 uma fun\u00e7\u00e3o de y ap\u00f3s transformada e n\u00e3o de x . Dica 2: Fatorizar a pdf conjunta. Voc\u00ea vai ver como se destaca a independ\u00eancia aqui. Passo 4: Mostrar que \\hat{\\mu} e \\hat{\\sigma}^2 s\u00e3o independentes. Refer\u00eancias 1 2 Simples visualiza\u00e7\u00e3o Eu gostaria de comparar o que acontece com a m\u00e9dia e vari\u00e2ncia amostral da distribui\u00e7\u00e3o normal e da distribui\u00e7\u00e3o gamma. Para isso, geero amostras de tamanho n , calculo as estat\u00edsticas e salvo. Fa\u00e7o esse procedimento o n\u00famero de pontos que quiser. ite = 10000 n = 10000 # Par\u00e2metros da Normal mu = 5 sigma = 2 # Par\u00e2metros da Gamma alpha = 5 beta = 4 means = np . zeros (( ite , 2 )) variances = np . zeros (( ite , 2 )) for i in range ( ite ): X = ro . normal ( loc = mu , scale = sigma , size = n ) Y = ro . gamma ( shape = alpha , scale = 1 / beta , size = n ) means [ i , 0 ] = np . mean ( X ) means [ i , 1 ] = np . mean ( Y ) variances [ i , 0 ] = np . var ( X , ddof = 0 ) variances [ i , 1 ] = np . var ( Y , ddof = 0 ) coef_normal = np . polyfit ( x = means [:, 0 ], y = variances [:, 0 ], deg = 1 ) coef_gamma = np . polyfit ( x = means [:, 1 ], y = variances [:, 1 ], deg = 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) fig . suptitle ( 'Comparando m\u00e9dia e vari\u00e2ncia amostral' ) ax [ 0 ] . scatter ( means [:, 0 ], variances [:, 0 ]) ax [ 1 ] . scatter ( means [:, 1 ], variances [:, 1 ]) ax [ 0 ] . plot ( means [:, 0 ], coef_normal [ 0 ] * means [:, 0 ] + coef_normal [ 1 ], color = 'red' ) ax [ 1 ] . plot ( means [:, 1 ], coef_gamma [ 0 ] * means [:, 1 ] + coef_gamma [ 1 ], color = 'red' ) ax [ 0 ] . set_xlabel ( r '$\\bar{X}_n$' , fontsize = 18 ) ax [ 1 ] . set_xlabel ( r '$\\bar{X}_n$' , fontsize = 18 ) ax [ 0 ] . set_ylabel ( r '$\\sum (X_i - \\bar{X}_n)^2$' , fontsize = 18 ) ax [ 1 ] . set_ylabel ( r '$\\sum (X_i - \\bar{X}_n)^2$' , fontsize = 18 ) ax [ 0 ] . set_title ( 'Distribui\u00e7\u00e3o Normal' ) ax [ 1 ] . set_title ( 'Distribui\u00e7\u00e3o Gamma' ) ax [ 0 ] . grid ( alpha = 0.5 , linestyle = '--' ) ax [ 1 ] . grid ( alpha = 0.5 , linestyle = '--' ) plt . show () Obs: A n\u00e3o inclina\u00e7\u00e3o da reta n\u00e3o significa que existe independ\u00eancia, mas como s\u00e3o independentes, a gente espera que a inclina\u00e7\u00e3o seja pequena. Distribui\u00e7\u00f5es T Student Artigo original : Olhe a p\u00e1gina 9! Defini\u00e7\u00e3o Sejam Y \\sim \\chi^2(m) e Z \\sim N(0,1) independentes. Ent\u00e3o X = \\frac{Z}{\\left(\\frac{Y}{m}\\right)^{1/2}} \\sim t(m) onde t(m) \u00e9 a distribui\u00e7\u00e3o t-student com m graus de liberdade. Fun\u00e7\u00e3o densidade de probabilidade Para escrever essa fun\u00e7\u00e3o de probabilidade, defina X como acima e W = Y . J\u00e1 sabemos a distribui\u00e7\u00e3o conjunta de Y e Z , pois eles s\u00e3o independentes. Com essa mudan\u00e7a de vari\u00e1vel ( confira aqui se n\u00e3o lembra como \u00e9 feito ), voc\u00ea conseque escrever a distribui\u00e7\u00e3o conjunta de X e W . Depois, basta calcular a distribui\u00e7\u00e3o marginal de X , integrando em W . f(x) = \\frac{\\Gamma\\left(\\frac{m+1}{2}\\right)}{(m\\pi)^{1/2}\\Gamma\\left(\\frac{m}{2}\\right)}\\left(1 + \\frac{x^2}{m} \\right)^{-(m+1)/2}, x \\in \\mathbb{R}, onde \\Gamma \u00e9 a fun\u00e7\u00e3o Gamma , tal que, n \\in \\mathbb{N}, \\Gamma(n) = (n-1)! \\Gamma(z+1) = z\\Gamma(z) \\Gamma(1/2) = \\sqrt{\\pi} Quando m \\leq 1 , a m\u00e9dia \u00e9 divergente. Isso pode ser vizualizado pelo expoente que ser\u00e1 \\leq -1 , o que diverge (lembre de \\int 1/x ). Quando m > 1 , a m\u00e9dia existe e \u00e9 0 pela simetria da distribui\u00e7\u00e3o. Em particular, podemos mostrar que se k < m , E[|X^k|] < + \\infty e se k \\geq m , o momento diverge. Se X \\sim t(m), m > 2 , Var(X) = \\frac{m}{m-2} Teorema Seja X_1, ..., X_n \\overset{iid}{\\sim} N(\\mu,\\sigma^2) . Seja \\sigma ' = \\left[\\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}{n-1}\\right]^{1/2} Ent\u00e3o n^{1/2}(\\bar{X}_n - \\mu)/\\sigma ' \\sim t(n-1) Rela\u00e7\u00e3o com a Normal e Cauchy Da mesma forma que a distribui\u00e7\u00e3o normal e a distribui\u00e7\u00e3o Cauchy, a distribui\u00e7\u00e3o t \u00e9 centrada em 0 e tem sua moda nesse valor. Entretanto a cauda a distribui\u00e7\u00e3o t (quando x \\to -\\infty ou x \\to +\\infty ), \u00e9 mais pesada, no sentido de que tende para 0 em uma velocidade menor do que a normal. Outra coisa interessante \u00e9 que a ditrivui\u00e7\u00e3o t(1) \u00e9 a distribui\u00e7\u00e3o Cauchy . Al\u00e9m disso, quando n \\to \\infty , converge para a pdf da normal padr\u00e3o ( Normal(0,1) ). Ferramentas para demonstrar a converg\u00eancia Teorema de Slutsky : Considere o corol\u00e1rio com f(x,y) = \\frac{x}{y} Lei dos Grandes N\u00fameros : Escreva a qui-quadrado como soma de normais. from scipy.stats import t , norm , cauchy Implementa\u00e7\u00e3o Primeiro vamos ver a cara da distribui\u00e7\u00e3o t m = 10 X = t ( df = m ) w = np . arange ( - 3 , 3 , 0.1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax [ 0 ] . plot ( w , X . pdf ( w ), lw = 5 , color = 'orange' ) ax [ 1 ] . plot ( w , X . cdf ( w ), lw = 5 , color = 'orange' ) ax [ 0 ] . set_title ( 'PDF t-Student' ) ax [ 1 ] . set_title ( 'CDF t-Student' ) plt . show () Vamos ver o que acontece quando m \\leq 1 ? ite = 1000 n = 10000 m1 = 10 m2 = 0.5 means = np . zeros (( ite , 2 )) for i in range ( ite ): X = ro . standard_t ( df = m1 , size = n ) Y = ro . standard_t ( df = m2 , size = n ) means [ i , 0 ] = np . mean ( X ) means [ i , 1 ] = np . mean ( Y ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) ax [ 0 ] . hist ( means [:, 0 ], bins = 100 ) ax [ 1 ] . hist ( np . log ( means [:, 1 ]), bins = 10 ) ax [ 0 ] . set_xlabel ( 'E[X]' ) ax [ 1 ] . set_xlabel ( 'log E[X]' ) ax [ 0 ] . set_title ( 'm = 10' ) ax [ 1 ] . set_title ( 'm = 0.5' ) plt . show () No eixo x do segundo gr\u00e1fico plotei o logaritmo, dado que alguns resultados eram extremamente grandes! Isso indica visualmente que a m\u00e9dia diverge! Rela\u00e7\u00e3o com a Normal e com Cauchy C = cauchy () Z = norm ( loc = 0 , scale = 1 ) T = t ( df = 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) ax [ 0 ] . plot ( w , C . pdf ( w ), label = 'Cauchy' ) ax [ 0 ] . scatter ( w , T . pdf ( w ), c = 'red' , marker = \"*\" , label = 't-Student' ) ax [ 0 ] . legend () ax [ 0 ] . set_title ( 't-Student e Cauchy quando m = 1' ) ax [ 1 ] . plot ( w , Z . pdf ( w ), label = 'N(0,1)' ) ax [ 1 ] . set_title ( 'Converg\u00eancia da t para a normal' ) for i in np . logspace ( np . log10 ( 1 ), np . log10 ( 20 ), 5 ): T = t ( df = int ( i )) ax [ 1 ] . plot ( w , T . pdf ( w ), linestyle = '--' , alpha = i / 40 + 0.5 , color = 'grey' , label = 't({})' . format ( int ( i ))) ax [ 1 ] . legend ( loc = 'upper right' ) plt . show ()","title":"Distribui\u00e7\u00e3o Amostral"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#distribuicao-chi-quadrado","text":"Para cada m positivo, a distribui\u00e7\u00e3o Gamma(m/2, 1/2) \u00e9 chamada de distribui\u00e7\u00e3o \\chi^2 . Ela foi primeiramente descrita por Helmert para computar a distribui\u00e7\u00e3o amostral de uma popula\u00e7\u00e3o normal. Vamos ver como a normal se relaciona mais a frente. f(x) = \\frac{1}{2^{m/2}\\Gamma(m/2)}x^{m/2 - 1}e^{-x/2}","title":"Distribui\u00e7\u00e3o Chi-Quadrado"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#propriedades","text":"Se X \\sim \\chi^2(m) , ent\u00e3o: E(X) = m Var(X) = 2m \\psi(t) = \\left(\\frac{1}{1-2t}\\right)^{m/2}, t < \\frac{1}{2}","title":"Propriedades"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#soma-de-chi2","text":"Se X_1, ..., X_k s\u00e3o independentes e cada uma tem grau de liberdade m_i , ent\u00e3o X_1 + ... + X_n tem distribui\u00e7\u00e3o \\chi^2(m_1 + .... + m_k)","title":"Soma de \\chi^2"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#relacao-com-a-normal","text":"Se X tem distribui\u00e7\u00e3o normal padr\u00e3o, Y = X^2 \\sim \\chi^2(1) De fato, se juntarmos as \u00faltimos dois teoremas, veremos que a soma de quadrados de normais independentes e identicamente distribuidas ser\u00e1 \\chi^2(m) , onde m \u00e9 o n\u00famero de parcelas.","title":"Rela\u00e7\u00e3o com a Normal"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#implementacao","text":"import numpy as np import matplotlib.pyplot as plt from scipy.stats import chi2 from matplotlib import animation , cm from IPython.display import HTML # Random Object ro = np . random . default_rng ( 1000 ) # Para assegurar reprodutibilidade degree_freedom = 10 mean , var , skew , kurt = chi2 . stats ( degree_freedom , moments = 'mvsk' ) print ( 'Propriedades' ) print ( 'M\u00e9dia: {}' . format ( mean )) print ( 'Var: {}' . format ( var )) print ( 'Assimetria: {}' . format ( skew )) print ( 'Curtose: {}' . format ( kurt )) Propriedades M\u00e9dia: 10.0 Var: 20.0 Assimetria: 0.8944271909999159 Curtose: 1.2 fig , ax = plt . subplots ( 1 , 1 ) x = np . linspace ( chi2 . ppf ( 0.01 , degree_freedom ), chi2 . ppf ( 0.99 , degree_freedom ), 100 ) ax . plot ( x , chi2 . pdf ( x , degree_freedom ), 'r-' , lw = 5 , alpha = 0.6 , label = 'chi2 pdf' ) r = chi2 . rvs ( degree_freedom , size = 10000 ) ax . hist ( r , density = True , alpha = 0.2 ) ax . legend () plt . show () fig , ax = plt . subplots () line , = ax . plot ( x , chi2 . pdf ( x , degree_freedom ), 'r-' , lw = 5 , alpha = 0.6 ) ax . set_xlim (( 0 , 150 )) ax . set_title ( 'Chi-Square' ) def animate ( i , degree_freedom ): x = np . linspace ( 0 , chi2 . ppf ( 0.99 , degree_freedom + i ), 100 ) line . set_data ( x , chi2 . pdf ( x , degree_freedom + i )) return line , anim = animation . FuncAnimation ( fig , animate , frames = 100 , interval = 50 , fargs = ( degree_freedom ,), repeat = False ) HTML ( anim . to_html5_video ()) Your browser does not support the video tag.","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#distribuicao-conjunta-da-media-e-variancia-amostrais","text":"X_1,...,X_n formam uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal e com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 desconhecidos. Estamos interessados na distribui\u00e7\u00e3o conjunta dos estimadores de m\u00e1xima verossimilhan\u00e7a para m\u00e9dia e vari\u00e2ncia da amostra.","title":"Distribui\u00e7\u00e3o Conjunta da m\u00e9dia e vari\u00e2ncia amostrais"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#teorema-de-basu","text":"Sejam \\hat{\\mu} = \\bar{X}_n e \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 a m\u00e9dia e vari\u00e2ncia amostrais, respectivamente. Ent\u00e3o \\hat{\\mu} tem distribui\u00e7\u00e3o normal com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 /n , enquanto \\hat{\\sigma}^2 tem a distribui\u00e7\u00e3o \\chi^2(n-1) , isto \u00e9, com n-1 grau de liberdade. Al\u00e9m disso elas s\u00e3o independentes. Esse teorema \u00e9 um pouco mais complexo e, na verdade, essa seria uma esp\u00e9cie de aplica\u00e7\u00e3o do teorema, na verdade. O teorema de Basu diz que: Se T \u00e9 uma estat\u00edstica suficiente completa (Considere, nesse teorema, g uma fun\u00e7\u00e3o integr\u00e1vel limitada) para \\theta e A uma estat\u00edstica ancillary, ent\u00e3o T \u00e9 independente de A . Nesse caso \\hat{\\mu} \u00e9 completa suficiente e \\hat{\\sigma}^2 \u00e9 ancillary, por que n\u00e3o depende de \\mu . O mais interessante \u00e9 que essa propriedade \u00e9 s\u00f3 vista com a distribui\u00e7\u00e3o normal ! Olhem a p\u00e1gina 9.","title":"Teorema de Basu"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#demonstracao","text":"O livro tem uma abordagem um pouco mais voltado \u00e0 \u00c1lgebra Linear. Aqui vou mostrar uma ideia um pouco diferente, onde voc\u00eas podem demonstrar os passos, como exerc\u00edcio. Passo 1: \\sum_{i=1}^n X_i^2 = n\\hat{\\sigma}^2 + n\\hat{\\mu}^2 Dica: Escrever \\hat{\\sigma}^2 e abrir em tr\u00eas somat\u00f3rios. Passo 2: \\sum_{i=1}^n (X_i - \\mu)^2 = n\\hat{\\sigma}^2 + n(\\hat{\\mu} - \\mu)^2 Dica: O Passo 1 \u00e9 um caso especial do Passo 2. O processo \u00e9 o mesmo. Passo 3: \\hat{\\mu} \u00e9 independente de X_i - \\hat{\\mu}, i = 1,...,n . Dica: Montar a pdf conjunta de X_1, ..., X_n (j\u00e1 fizemos isso atrave\u015b da verossimilhan\u00e7a) e fazer uma mudan\u00e7a de vari\u00e1vel Y_1 = \\hat{\\mu}, Y_2 = X_2 - \\hat{\\mu}, ..., Y_n = X_n - \\hat{\\mu} . Com essa mudan\u00e7a, \u00e9 poss\u00edvel montar a pdf como fun\u00e7\u00e3o de y_1,...,y_n . Esse processo \u00e9 um pouco mais chato, mas \u00e9 bom lembrar como fazez mudan\u00e7a de vari\u00e1vel para pdfs. Aqui voc\u00ea pode conferir como . \u00c9 importante lembrar que \u00e9 uma fun\u00e7\u00e3o de y ap\u00f3s transformada e n\u00e3o de x . Dica 2: Fatorizar a pdf conjunta. Voc\u00ea vai ver como se destaca a independ\u00eancia aqui. Passo 4: Mostrar que \\hat{\\mu} e \\hat{\\sigma}^2 s\u00e3o independentes.","title":"Demonstra\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#referencias","text":"1 2","title":"Refer\u00eancias"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#simples-visualizacao","text":"Eu gostaria de comparar o que acontece com a m\u00e9dia e vari\u00e2ncia amostral da distribui\u00e7\u00e3o normal e da distribui\u00e7\u00e3o gamma. Para isso, geero amostras de tamanho n , calculo as estat\u00edsticas e salvo. Fa\u00e7o esse procedimento o n\u00famero de pontos que quiser. ite = 10000 n = 10000 # Par\u00e2metros da Normal mu = 5 sigma = 2 # Par\u00e2metros da Gamma alpha = 5 beta = 4 means = np . zeros (( ite , 2 )) variances = np . zeros (( ite , 2 )) for i in range ( ite ): X = ro . normal ( loc = mu , scale = sigma , size = n ) Y = ro . gamma ( shape = alpha , scale = 1 / beta , size = n ) means [ i , 0 ] = np . mean ( X ) means [ i , 1 ] = np . mean ( Y ) variances [ i , 0 ] = np . var ( X , ddof = 0 ) variances [ i , 1 ] = np . var ( Y , ddof = 0 ) coef_normal = np . polyfit ( x = means [:, 0 ], y = variances [:, 0 ], deg = 1 ) coef_gamma = np . polyfit ( x = means [:, 1 ], y = variances [:, 1 ], deg = 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) fig . suptitle ( 'Comparando m\u00e9dia e vari\u00e2ncia amostral' ) ax [ 0 ] . scatter ( means [:, 0 ], variances [:, 0 ]) ax [ 1 ] . scatter ( means [:, 1 ], variances [:, 1 ]) ax [ 0 ] . plot ( means [:, 0 ], coef_normal [ 0 ] * means [:, 0 ] + coef_normal [ 1 ], color = 'red' ) ax [ 1 ] . plot ( means [:, 1 ], coef_gamma [ 0 ] * means [:, 1 ] + coef_gamma [ 1 ], color = 'red' ) ax [ 0 ] . set_xlabel ( r '$\\bar{X}_n$' , fontsize = 18 ) ax [ 1 ] . set_xlabel ( r '$\\bar{X}_n$' , fontsize = 18 ) ax [ 0 ] . set_ylabel ( r '$\\sum (X_i - \\bar{X}_n)^2$' , fontsize = 18 ) ax [ 1 ] . set_ylabel ( r '$\\sum (X_i - \\bar{X}_n)^2$' , fontsize = 18 ) ax [ 0 ] . set_title ( 'Distribui\u00e7\u00e3o Normal' ) ax [ 1 ] . set_title ( 'Distribui\u00e7\u00e3o Gamma' ) ax [ 0 ] . grid ( alpha = 0.5 , linestyle = '--' ) ax [ 1 ] . grid ( alpha = 0.5 , linestyle = '--' ) plt . show () Obs: A n\u00e3o inclina\u00e7\u00e3o da reta n\u00e3o significa que existe independ\u00eancia, mas como s\u00e3o independentes, a gente espera que a inclina\u00e7\u00e3o seja pequena.","title":"Simples visualiza\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#distribuicoes-t-student","text":"Artigo original : Olhe a p\u00e1gina 9!","title":"Distribui\u00e7\u00f5es T Student"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#definicao","text":"Sejam Y \\sim \\chi^2(m) e Z \\sim N(0,1) independentes. Ent\u00e3o X = \\frac{Z}{\\left(\\frac{Y}{m}\\right)^{1/2}} \\sim t(m) onde t(m) \u00e9 a distribui\u00e7\u00e3o t-student com m graus de liberdade.","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#funcao-densidade-de-probabilidade","text":"Para escrever essa fun\u00e7\u00e3o de probabilidade, defina X como acima e W = Y . J\u00e1 sabemos a distribui\u00e7\u00e3o conjunta de Y e Z , pois eles s\u00e3o independentes. Com essa mudan\u00e7a de vari\u00e1vel ( confira aqui se n\u00e3o lembra como \u00e9 feito ), voc\u00ea conseque escrever a distribui\u00e7\u00e3o conjunta de X e W . Depois, basta calcular a distribui\u00e7\u00e3o marginal de X , integrando em W . f(x) = \\frac{\\Gamma\\left(\\frac{m+1}{2}\\right)}{(m\\pi)^{1/2}\\Gamma\\left(\\frac{m}{2}\\right)}\\left(1 + \\frac{x^2}{m} \\right)^{-(m+1)/2}, x \\in \\mathbb{R}, onde \\Gamma \u00e9 a fun\u00e7\u00e3o Gamma , tal que, n \\in \\mathbb{N}, \\Gamma(n) = (n-1)! \\Gamma(z+1) = z\\Gamma(z) \\Gamma(1/2) = \\sqrt{\\pi} Quando m \\leq 1 , a m\u00e9dia \u00e9 divergente. Isso pode ser vizualizado pelo expoente que ser\u00e1 \\leq -1 , o que diverge (lembre de \\int 1/x ). Quando m > 1 , a m\u00e9dia existe e \u00e9 0 pela simetria da distribui\u00e7\u00e3o. Em particular, podemos mostrar que se k < m , E[|X^k|] < + \\infty e se k \\geq m , o momento diverge. Se X \\sim t(m), m > 2 , Var(X) = \\frac{m}{m-2}","title":"Fun\u00e7\u00e3o densidade de probabilidade"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#teorema","text":"Seja X_1, ..., X_n \\overset{iid}{\\sim} N(\\mu,\\sigma^2) . Seja \\sigma ' = \\left[\\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}{n-1}\\right]^{1/2} Ent\u00e3o n^{1/2}(\\bar{X}_n - \\mu)/\\sigma ' \\sim t(n-1)","title":"Teorema"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#relacao-com-a-normal-e-cauchy","text":"Da mesma forma que a distribui\u00e7\u00e3o normal e a distribui\u00e7\u00e3o Cauchy, a distribui\u00e7\u00e3o t \u00e9 centrada em 0 e tem sua moda nesse valor. Entretanto a cauda a distribui\u00e7\u00e3o t (quando x \\to -\\infty ou x \\to +\\infty ), \u00e9 mais pesada, no sentido de que tende para 0 em uma velocidade menor do que a normal. Outra coisa interessante \u00e9 que a ditrivui\u00e7\u00e3o t(1) \u00e9 a distribui\u00e7\u00e3o Cauchy . Al\u00e9m disso, quando n \\to \\infty , converge para a pdf da normal padr\u00e3o ( Normal(0,1) ).","title":"Rela\u00e7\u00e3o com a Normal e Cauchy"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#ferramentas-para-demonstrar-a-convergencia","text":"Teorema de Slutsky : Considere o corol\u00e1rio com f(x,y) = \\frac{x}{y} Lei dos Grandes N\u00fameros : Escreva a qui-quadrado como soma de normais. from scipy.stats import t , norm , cauchy","title":"Ferramentas para demonstrar a converg\u00eancia"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#implementacao_1","text":"Primeiro vamos ver a cara da distribui\u00e7\u00e3o t m = 10 X = t ( df = m ) w = np . arange ( - 3 , 3 , 0.1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax [ 0 ] . plot ( w , X . pdf ( w ), lw = 5 , color = 'orange' ) ax [ 1 ] . plot ( w , X . cdf ( w ), lw = 5 , color = 'orange' ) ax [ 0 ] . set_title ( 'PDF t-Student' ) ax [ 1 ] . set_title ( 'CDF t-Student' ) plt . show () Vamos ver o que acontece quando m \\leq 1 ? ite = 1000 n = 10000 m1 = 10 m2 = 0.5 means = np . zeros (( ite , 2 )) for i in range ( ite ): X = ro . standard_t ( df = m1 , size = n ) Y = ro . standard_t ( df = m2 , size = n ) means [ i , 0 ] = np . mean ( X ) means [ i , 1 ] = np . mean ( Y ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) ax [ 0 ] . hist ( means [:, 0 ], bins = 100 ) ax [ 1 ] . hist ( np . log ( means [:, 1 ]), bins = 10 ) ax [ 0 ] . set_xlabel ( 'E[X]' ) ax [ 1 ] . set_xlabel ( 'log E[X]' ) ax [ 0 ] . set_title ( 'm = 10' ) ax [ 1 ] . set_title ( 'm = 0.5' ) plt . show () No eixo x do segundo gr\u00e1fico plotei o logaritmo, dado que alguns resultados eram extremamente grandes! Isso indica visualmente que a m\u00e9dia diverge!","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#relacao-com-a-normal-e-com-cauchy","text":"C = cauchy () Z = norm ( loc = 0 , scale = 1 ) T = t ( df = 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) ax [ 0 ] . plot ( w , C . pdf ( w ), label = 'Cauchy' ) ax [ 0 ] . scatter ( w , T . pdf ( w ), c = 'red' , marker = \"*\" , label = 't-Student' ) ax [ 0 ] . legend () ax [ 0 ] . set_title ( 't-Student e Cauchy quando m = 1' ) ax [ 1 ] . plot ( w , Z . pdf ( w ), label = 'N(0,1)' ) ax [ 1 ] . set_title ( 'Converg\u00eancia da t para a normal' ) for i in np . logspace ( np . log10 ( 1 ), np . log10 ( 20 ), 5 ): T = t ( df = int ( i )) ax [ 1 ] . plot ( w , T . pdf ( w ), linestyle = '--' , alpha = i / 40 + 0.5 , color = 'grey' , label = 't({})' . format ( int ( i ))) ax [ 1 ] . legend ( loc = 'upper right' ) plt . show ()","title":"Rela\u00e7\u00e3o com a Normal e com Cauchy"},{"location":"infestatistica/StatisticalInference/StatisticalInference/","text":"Infer\u00eancia Estat\u00edstica Procedimento que objetiva produzir uma proposi\u00e7\u00e3o probabil\u00edsitca sobre um modelo estat\u00edstico. Modelo Estat\u00edstico Identificar vari\u00e1veis aleat\u00f3rias de interesse, especificar uma distribui\u00e7\u00e3o conjunta (ou fam\u00edlia), par\u00e2metros relevantes e uma especifica\u00e7\u00e3o para uma distribui\u00e7\u00e3o para os par\u00e2metros desconhecidos (baysianos adoram essa parte, p \\sim N(0,1) ) Espa\u00e7o dos Par\u00e2metros Uma caracter\u00edstica ou uma combina\u00e7\u00e3o de caracter\u00edsticas para determinar uma distribui\u00e7\u00e3o conjunta para as vari\u00e1veis aleat\u00f3rias forma o par\u00e2metro, que pertence a um espa\u00e7o denominado \\Omega . Estat\u00edstica Fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias observ\u00e1veis Problemas estudados Predi\u00e7\u00e3o: Baseado na \u00e9poca do ano que estamos, fatores climatol\u00f3gicos dos \u00faltimos dias, entre outros fatores, qual a probabilidade de chuva amanh\u00e3? Problemas de decis\u00e3o estat\u00edstica: \u00c9 relacionado ao risco e teste de hip\u00f3teses. Resposta consider\u00e1vel Desenho de experimentos: um psic\u00f3logo quer inferir qu\u00e3o avesso ao risco \u00e9 uma determinada popula\u00e7\u00e3o. Ele pode determinar, desenhar o experimento para isso. Infer\u00eancia Estat\u00edstica com Python import numpy as np import pandas as pd from scipy.stats import poisson import matplotlib.pyplot as plt import seaborn as sns sns . set () Importando os Dados Este banco de dados inclui um registro para cada vazamento ou derramamento de oleoduto relatado \u00e0 Administra\u00e7\u00e3o de Seguran\u00e7a de Dutos e Materiais Perigosos desde 2010. Esses registros incluem a data e hora do incidente, operador e oleoduto, causa do incidente, tipo de l\u00edquido perigoso e quantidade perdida, ferimentos e fatalidades e custos associados. oil_accident_df = pd . read_csv ( '../data/oil_pipeline.csv' ) oil_accident_df . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Report Number Supplemental Number Accident Year Accident Date/Time Operator ID Operator Name Pipeline/Facility Name Pipeline Location Pipeline Type Liquid Type ... Other Fatalities Public Fatalities All Fatalities Property Damage Costs Lost Commodity Costs Public/Private Property Damage Costs Emergency Response Costs Environmental Remediation Costs Other Costs All Costs 871 20120202 17135 2012 6/15/2012 3:50 PM 31476 ROSE ROCK MIDSTREAM L.P. BURKETT DISCHARGE ONSHORE UNDERGROUND CRUDE OIL ... NaN NaN NaN 6020.0 200.0 2500.0 10500.0 8500.0 16000.0 43720 1 rows \u00d7 48 columns cols_of_interest = [ 'Accident Date/Time' , 'Accident State' , 'Pipeline Location' , 'Liquid Type' , 'Net Loss (Barrels)' , 'All Costs' ] data = oil_accident_df [ cols_of_interest ] data [ 'All Costs' ] = data [ 'All Costs' ] / 1000000 # unidade em milh\u00e3o. data . sample () /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Accident Date/Time Accident State Pipeline Location Liquid Type Net Loss (Barrels) All Costs 263 10/11/2010 4:10 PM NJ ONSHORE REFINED AND/OR PETROLEUM PRODUCT (NON-HVL), LI... 0.0 0.0 Vamos entender um pouco como esta informa\u00e7\u00e3o esta organizada. data . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Net Loss (Barrels) All Costs count 2795.000000 2795.000000 mean 132.194050 0.834033 std 1185.019252 16.578298 min 0.000000 0.000000 25% 0.000000 0.005040 50% 0.000000 0.023129 75% 2.000000 0.117232 max 30565.000000 840.526118 Vamos analisar os dados utilizando leis da probabilidade para aprender sobre a popula\u00e7\u00e3o. Veja que n\u00e3o temos a informa\u00e7\u00e3o completa, apenas a partir de 2010. fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 4 )) sns . boxplot ( data [ 'All Costs' ], data = data , ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Custos dos Acidentes por Milh\u00e3o US$' ) sns . boxplot ( data [ 'Net Loss (Barrels)' ], data = data , ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Preju\u00edzo L\u00edquido (Barris)' ) plt . show () Mas esse n\u00e3o era para ser um boxplot? Cade a caixa? Isso indica que valores grandes nos dois dados s\u00e3o muito maiores relativamente aos outros dados. Poder\u00edamos prever o custo de um acidente usando a mediana dos valores? \u00c9 de fato um modelo, mas nesse caso, parece ser ruim dado os valores grandes. O que s\u00e3o esses valores grandes, afinal? Em alguns casos, podem realmente apresentar erros, mas nesse caso fica dif\u00edcil de afirmar. Bom. Podemos, dados esses problemas, trabalhar com outra vari\u00e1vel dispon\u00edvel: o tempo do acidente. Conhecemos uma fam\u00edlia de distribui\u00e7\u00f5es de probabilidade que modela frequ\u00eancia de acidentes em um intervalo de tempo? Distribui\u00e7\u00e3o de Poisson: probabilidade de uma s\u00e9rie de eventos ocorrer num certo per\u00edodo de tempo se estes eventos ocorrem independentemente de quando ocorreu o \u00faltimo evento. De forma geral, podemos dizer que isso \u00e9 verdade para acidentes de \u00f3leo. Assim, temos uma vari\u00e1vel aleat\u00f3ria de interesser X , que indica o n\u00famero de acidentes, j\u00e1 temos uma distribui\u00e7\u00e3o para essa vari\u00e1vel (Poisson) e j\u00e1 temos o par\u00e2metro \\lambda desconhecido. data [ 'Accident Date/Time' ] = pd . to_datetime ( data [ 'Accident Date/Time' ]) totaltimespan = np . max ( data [ 'Accident Date/Time' ]) - np . min ( data [ 'Accident Date/Time' ]) totaltime_hour = ( totaltimespan . days * 24 + totaltimespan . seconds / ( 3600 )) totaltime_month = ( totaltimespan . days + totaltimespan . seconds / ( 3600 * 24 )) * 12 / 365 lmda_h = len ( data ) / totaltime_hour lmda_m = len ( data ) / totaltime_month print ( 'N\u00famero estimado de acidentes por hora: {}' . format ( lmda_h )) print ( 'N\u00famero estimado de acidentes por m\u00eas {}' . format ( lmda_m )) N\u00famero estimado de acidentes por hora: 0.04540255169379675 N\u00famero estimado de acidentes por m\u00eas 33.14386273647162 /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy \"\"\"Entry point for launching an IPython kernel. Poder\u00edamos ter procedimentos para estimar \\lambda , mas por hora, vamos tomar ele como a m\u00e9dia das observa\u00e7\u00f5es. Pela Lei dos Grandes N\u00fameros, sabemos que a m\u00e9dia da Poisson \u00e9 \\lambda e a m\u00e9dia amostral tende para ela. lamda = 33 X = poisson ( lamda ) I = np . arange ( 0 , 60 , 1 ) #intervalo(0,60), passo = 1 samples_poisson = np . sort ( np . random . poisson ( lamda , 10000 )) Y = X . cdf ( samples_poisson ) #fun\u00e7\u00e3o de densidade acumulada fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 8 )) ax [ 0 ] . scatter ( I , X . pmf ( I ) , color = 'purple' ) ax [ 0 ] . set_xlabel ( 'N\u00famero de Acidentes por m\u00eas (n)' ) ax [ 0 ] . set_ylabel ( 'P(X <= n)' ) ax [ 0 ] . set_title ( 'Fun\u00e7\u00e3o de Massa de Probabilidade' ) ax [ 1 ] . scatter ( samples_poisson , Y , color = 'purple' ) ax [ 1 ] . hlines ( 0.5 , xmin = min ( samples_poisson ), xmax = max ( samples_poisson ), linestyle = '--' , color = 'black' ) ax [ 1 ] . set_xlabel ( 'N\u00famero de acidentes por m\u00eas (n)' ) ax [ 1 ] . set_ylabel ( 'P(X <= n)' ) ax [ 1 ] . set_title ( 'Fun\u00e7\u00e3o de Distribui\u00e7\u00e3o Acumulada' ) plt . show () A partir de nosso modelo, j\u00e1 podemos fazer acerta\u00e7\u00f5es probabil\u00edstica! real_data = np . array ( data [ 'Accident Date/Time' ] . apply ( lambda x : ( x . year , x . month ))) accidents_count = { 2010 + i : { m : 0 for m in range ( 1 , 13 )} for i in range ( 8 )} for info in real_data : accidents_count [ info [ 0 ]][ info [ 1 ]] += 1 distribution = [ accidents_count [ y ][ m ] for y in accidents_count . keys () for m in accidents_count [ y ] . keys ()] distribution = distribution [: - 12 ] #Tirando 2 observa\u00e7\u00f5es de 2017 fig , ax = plt . subplots () sns . distplot ( distribution , bins = 15 , ax = ax , label = 'Original data' , kde = False , norm_hist = True ) ax . scatter ( I , X . pmf ( I ) , color = 'purple' , label = 'Nosso modelo' ) ax . legend () ax . set_title ( 'Comparando modelo com dados reais' ) plt . show ()","title":"Introdu\u00e7\u00e3o"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#inferencia-estatistica","text":"Procedimento que objetiva produzir uma proposi\u00e7\u00e3o probabil\u00edsitca sobre um modelo estat\u00edstico.","title":"Infer\u00eancia Estat\u00edstica"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#modelo-estatistico","text":"Identificar vari\u00e1veis aleat\u00f3rias de interesse, especificar uma distribui\u00e7\u00e3o conjunta (ou fam\u00edlia), par\u00e2metros relevantes e uma especifica\u00e7\u00e3o para uma distribui\u00e7\u00e3o para os par\u00e2metros desconhecidos (baysianos adoram essa parte, p \\sim N(0,1) )","title":"Modelo Estat\u00edstico"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#espaco-dos-parametros","text":"Uma caracter\u00edstica ou uma combina\u00e7\u00e3o de caracter\u00edsticas para determinar uma distribui\u00e7\u00e3o conjunta para as vari\u00e1veis aleat\u00f3rias forma o par\u00e2metro, que pertence a um espa\u00e7o denominado \\Omega .","title":"Espa\u00e7o dos Par\u00e2metros"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#estatistica","text":"Fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias observ\u00e1veis","title":"Estat\u00edstica"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#problemas-estudados","text":"Predi\u00e7\u00e3o: Baseado na \u00e9poca do ano que estamos, fatores climatol\u00f3gicos dos \u00faltimos dias, entre outros fatores, qual a probabilidade de chuva amanh\u00e3? Problemas de decis\u00e3o estat\u00edstica: \u00c9 relacionado ao risco e teste de hip\u00f3teses. Resposta consider\u00e1vel Desenho de experimentos: um psic\u00f3logo quer inferir qu\u00e3o avesso ao risco \u00e9 uma determinada popula\u00e7\u00e3o. Ele pode determinar, desenhar o experimento para isso. Infer\u00eancia Estat\u00edstica com Python import numpy as np import pandas as pd from scipy.stats import poisson import matplotlib.pyplot as plt import seaborn as sns sns . set ()","title":"Problemas estudados"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#importando-os-dados","text":"Este banco de dados inclui um registro para cada vazamento ou derramamento de oleoduto relatado \u00e0 Administra\u00e7\u00e3o de Seguran\u00e7a de Dutos e Materiais Perigosos desde 2010. Esses registros incluem a data e hora do incidente, operador e oleoduto, causa do incidente, tipo de l\u00edquido perigoso e quantidade perdida, ferimentos e fatalidades e custos associados. oil_accident_df = pd . read_csv ( '../data/oil_pipeline.csv' ) oil_accident_df . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Report Number Supplemental Number Accident Year Accident Date/Time Operator ID Operator Name Pipeline/Facility Name Pipeline Location Pipeline Type Liquid Type ... Other Fatalities Public Fatalities All Fatalities Property Damage Costs Lost Commodity Costs Public/Private Property Damage Costs Emergency Response Costs Environmental Remediation Costs Other Costs All Costs 871 20120202 17135 2012 6/15/2012 3:50 PM 31476 ROSE ROCK MIDSTREAM L.P. BURKETT DISCHARGE ONSHORE UNDERGROUND CRUDE OIL ... NaN NaN NaN 6020.0 200.0 2500.0 10500.0 8500.0 16000.0 43720 1 rows \u00d7 48 columns cols_of_interest = [ 'Accident Date/Time' , 'Accident State' , 'Pipeline Location' , 'Liquid Type' , 'Net Loss (Barrels)' , 'All Costs' ] data = oil_accident_df [ cols_of_interest ] data [ 'All Costs' ] = data [ 'All Costs' ] / 1000000 # unidade em milh\u00e3o. data . sample () /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Accident Date/Time Accident State Pipeline Location Liquid Type Net Loss (Barrels) All Costs 263 10/11/2010 4:10 PM NJ ONSHORE REFINED AND/OR PETROLEUM PRODUCT (NON-HVL), LI... 0.0 0.0 Vamos entender um pouco como esta informa\u00e7\u00e3o esta organizada. data . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Net Loss (Barrels) All Costs count 2795.000000 2795.000000 mean 132.194050 0.834033 std 1185.019252 16.578298 min 0.000000 0.000000 25% 0.000000 0.005040 50% 0.000000 0.023129 75% 2.000000 0.117232 max 30565.000000 840.526118 Vamos analisar os dados utilizando leis da probabilidade para aprender sobre a popula\u00e7\u00e3o. Veja que n\u00e3o temos a informa\u00e7\u00e3o completa, apenas a partir de 2010. fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 4 )) sns . boxplot ( data [ 'All Costs' ], data = data , ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Custos dos Acidentes por Milh\u00e3o US$' ) sns . boxplot ( data [ 'Net Loss (Barrels)' ], data = data , ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Preju\u00edzo L\u00edquido (Barris)' ) plt . show () Mas esse n\u00e3o era para ser um boxplot? Cade a caixa? Isso indica que valores grandes nos dois dados s\u00e3o muito maiores relativamente aos outros dados. Poder\u00edamos prever o custo de um acidente usando a mediana dos valores? \u00c9 de fato um modelo, mas nesse caso, parece ser ruim dado os valores grandes. O que s\u00e3o esses valores grandes, afinal? Em alguns casos, podem realmente apresentar erros, mas nesse caso fica dif\u00edcil de afirmar. Bom. Podemos, dados esses problemas, trabalhar com outra vari\u00e1vel dispon\u00edvel: o tempo do acidente. Conhecemos uma fam\u00edlia de distribui\u00e7\u00f5es de probabilidade que modela frequ\u00eancia de acidentes em um intervalo de tempo? Distribui\u00e7\u00e3o de Poisson: probabilidade de uma s\u00e9rie de eventos ocorrer num certo per\u00edodo de tempo se estes eventos ocorrem independentemente de quando ocorreu o \u00faltimo evento. De forma geral, podemos dizer que isso \u00e9 verdade para acidentes de \u00f3leo. Assim, temos uma vari\u00e1vel aleat\u00f3ria de interesser X , que indica o n\u00famero de acidentes, j\u00e1 temos uma distribui\u00e7\u00e3o para essa vari\u00e1vel (Poisson) e j\u00e1 temos o par\u00e2metro \\lambda desconhecido. data [ 'Accident Date/Time' ] = pd . to_datetime ( data [ 'Accident Date/Time' ]) totaltimespan = np . max ( data [ 'Accident Date/Time' ]) - np . min ( data [ 'Accident Date/Time' ]) totaltime_hour = ( totaltimespan . days * 24 + totaltimespan . seconds / ( 3600 )) totaltime_month = ( totaltimespan . days + totaltimespan . seconds / ( 3600 * 24 )) * 12 / 365 lmda_h = len ( data ) / totaltime_hour lmda_m = len ( data ) / totaltime_month print ( 'N\u00famero estimado de acidentes por hora: {}' . format ( lmda_h )) print ( 'N\u00famero estimado de acidentes por m\u00eas {}' . format ( lmda_m )) N\u00famero estimado de acidentes por hora: 0.04540255169379675 N\u00famero estimado de acidentes por m\u00eas 33.14386273647162 /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy \"\"\"Entry point for launching an IPython kernel. Poder\u00edamos ter procedimentos para estimar \\lambda , mas por hora, vamos tomar ele como a m\u00e9dia das observa\u00e7\u00f5es. Pela Lei dos Grandes N\u00fameros, sabemos que a m\u00e9dia da Poisson \u00e9 \\lambda e a m\u00e9dia amostral tende para ela. lamda = 33 X = poisson ( lamda ) I = np . arange ( 0 , 60 , 1 ) #intervalo(0,60), passo = 1 samples_poisson = np . sort ( np . random . poisson ( lamda , 10000 )) Y = X . cdf ( samples_poisson ) #fun\u00e7\u00e3o de densidade acumulada fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 8 )) ax [ 0 ] . scatter ( I , X . pmf ( I ) , color = 'purple' ) ax [ 0 ] . set_xlabel ( 'N\u00famero de Acidentes por m\u00eas (n)' ) ax [ 0 ] . set_ylabel ( 'P(X <= n)' ) ax [ 0 ] . set_title ( 'Fun\u00e7\u00e3o de Massa de Probabilidade' ) ax [ 1 ] . scatter ( samples_poisson , Y , color = 'purple' ) ax [ 1 ] . hlines ( 0.5 , xmin = min ( samples_poisson ), xmax = max ( samples_poisson ), linestyle = '--' , color = 'black' ) ax [ 1 ] . set_xlabel ( 'N\u00famero de acidentes por m\u00eas (n)' ) ax [ 1 ] . set_ylabel ( 'P(X <= n)' ) ax [ 1 ] . set_title ( 'Fun\u00e7\u00e3o de Distribui\u00e7\u00e3o Acumulada' ) plt . show () A partir de nosso modelo, j\u00e1 podemos fazer acerta\u00e7\u00f5es probabil\u00edstica! real_data = np . array ( data [ 'Accident Date/Time' ] . apply ( lambda x : ( x . year , x . month ))) accidents_count = { 2010 + i : { m : 0 for m in range ( 1 , 13 )} for i in range ( 8 )} for info in real_data : accidents_count [ info [ 0 ]][ info [ 1 ]] += 1 distribution = [ accidents_count [ y ][ m ] for y in accidents_count . keys () for m in accidents_count [ y ] . keys ()] distribution = distribution [: - 12 ] #Tirando 2 observa\u00e7\u00f5es de 2017 fig , ax = plt . subplots () sns . distplot ( distribution , bins = 15 , ax = ax , label = 'Original data' , kde = False , norm_hist = True ) ax . scatter ( I , X . pmf ( I ) , color = 'purple' , label = 'Nosso modelo' ) ax . legend () ax . set_title ( 'Comparando modelo com dados reais' ) plt . show ()","title":"Importando os Dados"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/","text":"Estimadores n\u00e3o enviesados Defini\u00e7\u00e3o Um estimador \\delta(X) \u00e9 dito n\u00e3o enviesado para g(\\theta) se E_{\\theta}[\\delta(X)] = g(\\theta) para todo valor de \\theta . O vi\u00e9s do estimador \u00e9 definido por E_{\\theta}[\\delta(X)] - g(\\theta) . Se \\delta \u00e9 um estimador com vari\u00e2ncia finita, ent\u00e3o: R(\\theta, \\delta) = Var(\\delta) + Vi\u00e9s(\\delta)^2 Estimador n\u00e3o enviesado para vari\u00e2ncia s^2 = \\hat{\\sigma}_1^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 # Importando bibliotecas import numpy as np import pandas as pd from IPython.display import display , Math # Display latex import matplotlib.pyplot as plt % matplotlib inline np . random . seed ( 1000 ) # Garantindo reprodutibilidade Nota: Por que garantir reprodutibilidade? Reprodutibilidade \u00e9 a ideia de tornar o processo que foi feito por voc\u00ea reprodut\u00edvel por qualquer outra pessoa, para que ela possa obter os mesmos resultados seguindo os mesmos passos que voc\u00ea. Quando escolhemos um n\u00famero aleat\u00f3rio ( pseudoaleat\u00f3rio na verdade), ele muda de tempos em tempos. Mas isso vai tirar a ideia de \"garantir os mesmos resultados\". O resultado pode ser parecido, mas n\u00e3o exatamente igual. Isso \u00e9 muito importante no meio cient\u00edfico. Exemplo Vamos ver como se comporta esse estimador n\u00e3o viesado em uma popula\u00e7\u00e3o que representa o Brasil todo! Veja que eu n\u00e3o peguei dados online, porque quero TODA a popula\u00e7\u00e3o. Por isso vamos fazer uma simula\u00e7\u00e3o. A m\u00e9dia verdadeira da distribui\u00e7\u00e3o \u00e9 161,1cm e o desvio padr\u00e3o \u00e9 10cm. # Tamanho da popula\u00e7\u00e3o N = int ( 200e5 ) # Popula\u00e7\u00e3o gerada por simula\u00e7\u00e3o, usando a distribui\u00e7\u00e3o normal. population_height = pd . Series ( np . random . normal ( loc = 161.1 , scale = 10 , size = N )) Podemos ver a m\u00e9dia dessa popula\u00e7\u00e3o. population_height . mean () 161.10097546939974 O que a fun\u00e7\u00e3o var do pandas faz? Vamos comparar com o estimador trivial. ddof = 1 # Se ddof = 0, teremos a divis\u00e3o por N population_height . var ( ddof = ddof ) 100.02715920849081 Divindindo por N sigma_square_hat = (( population_height - population_height . mean ()) ** 2 ) . sum () / N sigma1_square_hat = (( population_height - population_height . mean ()) ** 2 ) . sum () / ( N - 1 ) display ( Math ( r '\\hat\\sigma^2 = {}' . format ( sigma_square_hat ))) display ( Math ( r '\\hat\\sigma_1^2 = {}' . format ( sigma1_square_hat ))) \\displaystyle \\hat\\sigma^2 = 100.02715420713488 \\displaystyle \\hat\\sigma_1^2 = 100.02715920849283 Estima\u00e7\u00e3o dos Par\u00e2metros Vamos supor que n\u00e3o conhecemos os par\u00e2metros da nossa popula\u00e7\u00e3o e podemos conhecer apenas uma amostra aleat\u00f3ria dela. Vamos fazer 500 dessas simula\u00e7\u00f5es number_simulations = 500 sample_size = 30 sample = pd . DataFrame ( population_height . sample ( n = number_simulations * sample_size , replace = True , random_state = 100 ), columns = [ 'height' ]) reshape = sample . to_numpy () . reshape (( - 1 , 30 )) samples = pd . DataFrame ( reshape , columns = range ( 0 , 30 )) Vamos estimar a m\u00e9dia com a m\u00e9dia amostral que \u00e9 n\u00e3o viesada tamb\u00e9m! Al\u00e9m disso ela \u00e9 o MLE. Estamos estimado para cada amostra a m\u00e9dia! Se fizermos uma m\u00e9dia das m\u00e9dias, veremos que ela chegar\u00e1 pr\u00f3ximo a m\u00e9dia verdadeira. estimated_mean = samples . mean ( axis = 1 ) #Axis = 1 faz a m\u00e9dia por linha. estimated_mean_of_means = estimated_mean . expanding () . mean () fig , ax = plt . subplots ( figsize = ( 8 , 5 )) ax . plot ( estimated_mean_of_means , label = 'Valor estimado' ) ax . hlines ( population_height . mean (), xmin = 0 , xmax = number_simulations , color = 'grey' , linestyle = '--' , alpha = 0.8 , label = 'Valor verdadeiro' ) ax . grid ( alpha = 0.4 ) ax . set_title ( 'Estimado a m\u00e9dia verdadeira' ) plt . show () Vamos comparar os estimadores para a vari\u00e2ncia, o viesado e o n\u00e3o viesado. df = pd . DataFrame ({ 'enviesado (dividido por n)' : samples . var ( ddof = 0 , axis = 1 ) . expanding () . mean (), 'nao_viesado (dividido por n - 1)' : samples . var ( ddof = 1 , axis = 1 ) . expanding () . mean (), 'verdadeiro' : pd . Series ( population_height . var ( ddof = 0 ), index = samples . index )}) ax = df . plot () ax . set_title ( 'Estimadores para a vari\u00e2ncia' ) ax . grid ( alpha = 0.4 ) plt . show () # Comparar com Conscist\u00eancia","title":"Estimadores n\u00e3o enviesados"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#estimadores-nao-enviesados","text":"","title":"Estimadores n\u00e3o enviesados"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#definicao","text":"Um estimador \\delta(X) \u00e9 dito n\u00e3o enviesado para g(\\theta) se E_{\\theta}[\\delta(X)] = g(\\theta) para todo valor de \\theta . O vi\u00e9s do estimador \u00e9 definido por E_{\\theta}[\\delta(X)] - g(\\theta) . Se \\delta \u00e9 um estimador com vari\u00e2ncia finita, ent\u00e3o: R(\\theta, \\delta) = Var(\\delta) + Vi\u00e9s(\\delta)^2","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#estimador-nao-enviesado-para-variancia","text":"s^2 = \\hat{\\sigma}_1^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 # Importando bibliotecas import numpy as np import pandas as pd from IPython.display import display , Math # Display latex import matplotlib.pyplot as plt % matplotlib inline np . random . seed ( 1000 ) # Garantindo reprodutibilidade","title":"Estimador n\u00e3o enviesado para vari\u00e2ncia"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#nota-por-que-garantir-reprodutibilidade","text":"Reprodutibilidade \u00e9 a ideia de tornar o processo que foi feito por voc\u00ea reprodut\u00edvel por qualquer outra pessoa, para que ela possa obter os mesmos resultados seguindo os mesmos passos que voc\u00ea. Quando escolhemos um n\u00famero aleat\u00f3rio ( pseudoaleat\u00f3rio na verdade), ele muda de tempos em tempos. Mas isso vai tirar a ideia de \"garantir os mesmos resultados\". O resultado pode ser parecido, mas n\u00e3o exatamente igual. Isso \u00e9 muito importante no meio cient\u00edfico.","title":"Nota: Por que garantir reprodutibilidade?"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#exemplo","text":"Vamos ver como se comporta esse estimador n\u00e3o viesado em uma popula\u00e7\u00e3o que representa o Brasil todo! Veja que eu n\u00e3o peguei dados online, porque quero TODA a popula\u00e7\u00e3o. Por isso vamos fazer uma simula\u00e7\u00e3o. A m\u00e9dia verdadeira da distribui\u00e7\u00e3o \u00e9 161,1cm e o desvio padr\u00e3o \u00e9 10cm. # Tamanho da popula\u00e7\u00e3o N = int ( 200e5 ) # Popula\u00e7\u00e3o gerada por simula\u00e7\u00e3o, usando a distribui\u00e7\u00e3o normal. population_height = pd . Series ( np . random . normal ( loc = 161.1 , scale = 10 , size = N )) Podemos ver a m\u00e9dia dessa popula\u00e7\u00e3o. population_height . mean () 161.10097546939974 O que a fun\u00e7\u00e3o var do pandas faz? Vamos comparar com o estimador trivial. ddof = 1 # Se ddof = 0, teremos a divis\u00e3o por N population_height . var ( ddof = ddof ) 100.02715920849081 Divindindo por N sigma_square_hat = (( population_height - population_height . mean ()) ** 2 ) . sum () / N sigma1_square_hat = (( population_height - population_height . mean ()) ** 2 ) . sum () / ( N - 1 ) display ( Math ( r '\\hat\\sigma^2 = {}' . format ( sigma_square_hat ))) display ( Math ( r '\\hat\\sigma_1^2 = {}' . format ( sigma1_square_hat ))) \\displaystyle \\hat\\sigma^2 = 100.02715420713488 \\displaystyle \\hat\\sigma_1^2 = 100.02715920849283","title":"Exemplo"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#estimacao-dos-parametros","text":"Vamos supor que n\u00e3o conhecemos os par\u00e2metros da nossa popula\u00e7\u00e3o e podemos conhecer apenas uma amostra aleat\u00f3ria dela. Vamos fazer 500 dessas simula\u00e7\u00f5es number_simulations = 500 sample_size = 30 sample = pd . DataFrame ( population_height . sample ( n = number_simulations * sample_size , replace = True , random_state = 100 ), columns = [ 'height' ]) reshape = sample . to_numpy () . reshape (( - 1 , 30 )) samples = pd . DataFrame ( reshape , columns = range ( 0 , 30 )) Vamos estimar a m\u00e9dia com a m\u00e9dia amostral que \u00e9 n\u00e3o viesada tamb\u00e9m! Al\u00e9m disso ela \u00e9 o MLE. Estamos estimado para cada amostra a m\u00e9dia! Se fizermos uma m\u00e9dia das m\u00e9dias, veremos que ela chegar\u00e1 pr\u00f3ximo a m\u00e9dia verdadeira. estimated_mean = samples . mean ( axis = 1 ) #Axis = 1 faz a m\u00e9dia por linha. estimated_mean_of_means = estimated_mean . expanding () . mean () fig , ax = plt . subplots ( figsize = ( 8 , 5 )) ax . plot ( estimated_mean_of_means , label = 'Valor estimado' ) ax . hlines ( population_height . mean (), xmin = 0 , xmax = number_simulations , color = 'grey' , linestyle = '--' , alpha = 0.8 , label = 'Valor verdadeiro' ) ax . grid ( alpha = 0.4 ) ax . set_title ( 'Estimado a m\u00e9dia verdadeira' ) plt . show () Vamos comparar os estimadores para a vari\u00e2ncia, o viesado e o n\u00e3o viesado. df = pd . DataFrame ({ 'enviesado (dividido por n)' : samples . var ( ddof = 0 , axis = 1 ) . expanding () . mean (), 'nao_viesado (dividido por n - 1)' : samples . var ( ddof = 1 , axis = 1 ) . expanding () . mean (), 'verdadeiro' : pd . Series ( population_height . var ( ddof = 0 ), index = samples . index )}) ax = df . plot () ax . set_title ( 'Estimadores para a vari\u00e2ncia' ) ax . grid ( alpha = 0.4 ) plt . show () # Comparar com Conscist\u00eancia","title":"Estima\u00e7\u00e3o dos Par\u00e2metros"}]}